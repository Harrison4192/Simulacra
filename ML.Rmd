---
title: "ML"
author: "Harrison Tietze"
output:
  html_document:
    toc: yes
    css: faded.css
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
library(ISLR)
library(tidyverse)
library(glmnet)
```

#Regression Models

##glmnet

In this analysis we will work with the "College"
dataset, which contains 777 observations with 18 variables. The goals is to predict how many applications a college will recieve "Apps" based off other predictors. 

First examine the structure of the dataset:

```{r}
str(College)
```

The variables are numeric, except for the factor: Private. Let's build a model matrix that automatically creates dummy variables, which is important because glmnet only accepts numeric variables.

```{r}
X = model.matrix(Apps ~ ., data = College)
Y = College$Apps
```

Since we will be testing the performance of multiple models, we need to split the data into a test and training set.

```{r}
train <- sample(1:nrow(X), replace = FALSE, round(nrow(X) * .25))
X.train <- X[train, ]
Y.train <- Y[train]
```


Let's fit a least-squares regression, which can be done using glmnet and no further arguments:

```{r}
OLS.mod <- glmnet(X.train, Y.train)
```

The 17 coefficients of the least-squares regression model are fit by minimizing the RSS, the sum of squared residuals. 

##Ridge Regression

Ridge regression fits a linear model by minimizing the quanitity $RSS + \lambda \sum_{j=1}^p\beta_j^2$, where $\lambda \sum_{j=1}^p\beta_j^2$ also written $\lambda||\beta||^2$ is called a *shrinkage penalty* and $\lambda \geq 0$ is called a *tuning parameter*. At the extremes, $\lambda = 0$ returns the least-squares estimate, and $\lambda \rightarrow \infty$ returns the *null model*, where all the predictors are forced to $0$. Note, that shrinkage is not applied to the intercept. Instead we first center the inputs and then estmiate $\beta_0$ by $\overline{y}$. Centering is also necessary because unlike in OLS, the scale of the predictors can influence the estimate. Centering predictors is done by dividing by their standard deviation: $\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^N(x_{ij}-\overline{x_j})^2}}$. The $\tilde{x_{ij}}$ are called *standardized predictors* and are used to compute *standardized coefficients*. 

Unlike the least squares estimate which produces only one set of coefficients, ridge regression computes a *path* of coefficients $\beta^R_{\lambda}$ which is a $p-$dimensional sequence indexed by $\lambda$. Cross validation must be used the to select the $\lambda$ such that $\beta^R_{\lambda}$ minimizes the out-of-sample error. 

Let's fit a ridge regression. Also, we can manually specify a grid of values for $\lambda$:

```{r}
grid <- 10^ seq (10,-2, length =100)
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = grid)  

# alpha = 0 specifies that we are using a Ridge Regression.
# glmnet automatically does centering first; Standardize = TRUE by default
```

The glmnet object has several useful accessor methods that return information about the model. `coef` returns a matrix of coefficients. There are 19 rows, one for each coefficient, and 100 columns corresponding to different lambda values. 

```{r}
dim(coef(ridge.mod))
```
The `lambda` attribute returns the lambda values we generated. Let's looks at the coefficients calculated for the 10th, 50th, and 90th lambda.

```{r}



el2norm <- function(x){sqrt(sum(x^2))}

el2norm.r <- function(col){el2norm(coef(ridge.mod)[, col])}

coef(ridge.mod)[,c(10,50,90)] %>% rbind( map_dbl(c(10,50,90), el2norm.r) ) %>% rbind(ridge.mod$lambda[c(10,50,90)])



```
The last two rows are the *l-2-norm* and the $\lambda$ value. We can see that for $\lambda = .16$ we have the highest shrinkage of the coefficients

We can also use the `predict` method to estimate coefficients at a new value of $\lambda$, say $\lambda = 80$

```{r}
predict(ridge.mod, s=80, type="coefficients")
```

Let's also look a the `%Dev` column when we print our model directly.

```{r}
# print(ridge.mod)   output is too long
```

`%Dev` is the *Null Deviance* of the model. If the *Null Deviance* is very small, it signifies that the proposed model does not perform better than the null model. We can see that when lambda is very high, the `%Dev` is almost 0, but increases as lambda decreases, and the penalty on the coefficients is relaxed. 


Rather than supplying a pre-defined grid,  we can use cross-validation to select the best lambda. The function `cv.glmnet` not only fits 100 models, but it tells us which one is the best.  

```{r}
set.seed(10)

ridge.cv <- cv.glmnet(X.train, Y.train, alpha = 0)

pred <- predict(ridge.cv, s="lambda.min", newx = X[-train, ]) 

MSE <- function(x, y){sqrt(sum((x-y)^2))/length(x)}

ridge.MSE <- MSE(pred, Y[-train])

bestlam <- ridge.cv$lambda.min
```
Here `cv.glmnet` uses 10-fold cross validation (by default) and returns a `cv.glmnet` object containing a sequence of models. The model that miminimized the MSE can be found using `lambda.min`. Here the best lambda is `r round(bestlam, 3)` and its corresponding estimate of the test error is `r round(ridge.MSE, 3)`. We could access the coefficients of this model by calling `coef(ridge.cv, s = "lambda.min")` and calculate their *l-2-norm* `r round(el2norm(coef(ridge.cv, s = "lambda.min")), 2)`.

## The Lasso

The Lasso is similar to ridge regression, except that the penalty is applied to the *l-1-norm* norm of the coeffiecients: $||\beta||_1$. That is, the lasso selects the best model by minimizing the quantity $RSS + \lambda\sum_{j=1}^p|\beta_j|$. Unlike Ridge regression, however, the lasso shrinks some of the coefficients to zero, and thus performs *variable selection*. The advantage of the lasso is that it yields *sparse* models with less coefficients are easier to interperet. 

```{r}
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = grid)

lasso.cv <- cv.glmnet(X.train, Y.train, alpha = 1)

bestlam <- lasso.cv$lambda.min

coef(lasso.cv, s = "lambda.min")


```

We can see that some of the variables have been dropped. 
