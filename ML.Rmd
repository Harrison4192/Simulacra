---
title: "Statistical Learning"
author: "Harrison Tietze"
date: "Fall 2017"
output:
  html_document:
    toc: yes
    css: faded.css
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = T)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(ISLR)
library(tidyverse)
library(glmnet)
library(magrittr)
library(microbenchmark)
```

reference: [Introduction to Statistical learning](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_1?ie=UTF8&qid=1510705551&sr=8-1&keywords=introduction+to+statistical+learning) (2013) *Tibshirani, Hastie*

#Regression Models


##glmnet

In this analysis we will work with the "College"
dataset, which contains 777 observations with 18 variables. The goals is to predict how many applications a college will recieve "Apps" based off other predictors. 

First examine the structure of the dataset:

```{r}
str(College)
```

<img src="collegedata.png">

The variables are numeric, except for the factor: Private. Let's build a model matrix that automatically creates dummy variables, which is important because glmnet only accepts numeric variables.

```{r}
X = model.matrix(Apps ~ . - 1, data = College)[, -1]
Y = model.matrix(~ Apps + 0, data = College)

data_df <- as.data.frame(cbind(X, Y))

```


Since we will be testing the performance of multiple models, we need to split the data into a test and training set.

```{r}
train <- sample(1:nrow(X), replace = FALSE, round(nrow(X) * .25))
X.train <- X[train, ]
Y.train <- Y[train]

X.test <- X[-train, ]
Y.test <- Y[-train]
```


Let's fit a least-squares regression, which can be done using the lm function. The 17 coefficients of the least-squares regression model are fit by minimizing the RSS, the sum of squared residuals. 

```{r}
LM.mod <- lm(Apps ~ . , data = data_df)

LM.pred <- predict(LM.mod, newx = X.test)

RMSE <- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

Percent.Error <- function(x){RMSE(x)/mean(Y) * 100}

LM.RMSE <- RMSE(LM.pred)
OLS.Percent.Err <- Percent.Error(LM.pred)

my_results <- tibble(model = "OLS",
                  RMSE = LM.RMSE,
                  )

knitr::kable(my_results, digits = 3)
```

The RMSE can be interpreted as an estimate for the standard deviation of predicted values. In the context of our College data, given a new set of observations, we could estimate the amount of applications a college will receive, give or take 7.6 apps. The percent error, given by the RMSE over the average value of Y, gives an estimate of percent average deviation of our prediction. This helps interpret the error, given the average college receives `r round(mean(Y))` and our percent error is `r OLS.Percent.Err`%, we can infer our error is quite high. 



##Ridge Regression

Ridge regression fits a linear model by minimizing the quanitity $RSS + \lambda \sum_{j=1}^p\beta_j^2$, where $\lambda \sum_{j=1}^p\beta_j^2$ also written $\lambda||\beta||^2$ is called a *shrinkage penalty* and $\lambda \geq 0$ is called a *tuning parameter*. At the extremes, $\lambda = 0$ returns the least-squares estimate, and $\lambda \rightarrow \infty$ returns the *null model*, where all the predictors are forced to $0$. Note, that shrinkage is not applied to the intercept. Instead we first center the inputs and then estmiate $\beta_0$ by $\overline{y}$. Centering is also necessary because unlike in OLS, the scale of the predictors can influence the estimate. Centering predictors is done by dividing by their standard deviation: $\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^N(x_{ij}-\overline{x_j})^2}}$. The $\tilde{x_{ij}}$ are called *standardized predictors* and are used to compute *standardized coefficients*. 

Unlike the least squares estimate which produces only one set of coefficients, ridge regression computes a *path* of coefficients $\beta^R_{\lambda}$ which is a $p-$dimensional sequence indexed by $\lambda$. Cross validation must be used the to select the $\lambda$ such that $\beta^R_{\lambda}$ minimizes the out-of-sample error. 

Let's fit a ridge regression. Also, we can manually specify a grid of values for $\lambda$:

```{r}
grid <- 10^ seq (10,-2, length =100)
ridge.mod <- glmnet(X.train, Y.train, alpha = 0, lambda = grid)  

# alpha = 0 specifies that we are using a Ridge Regression.
# glmnet automatically does centering first; Standardize = TRUE by default
```

The glmnet object has several useful accessor methods that return information about the model. `coef` returns a matrix of coefficients. There are 19 rows, one for each coefficient, and 100 columns corresponding to different lambda values. 

```{r}
dim(coef(ridge.mod))
```
The `lambda` attribute returns the lambda values we generated. Let's looks at the coefficients calculated for the 10th, 50th, and 90th lambda.

```{r}



el2norm <- function(x){sqrt(sum(x^2))}

el2norm.r <- function(col){el2norm(coef(ridge.mod)[, col])}

coef(ridge.mod)[,c(10,50,90)] %>% rbind( map_dbl(c(10,50,90), el2norm.r) ) %>% rbind(ridge.mod$lambda[c(10,50,90)])



```
The last two rows are the *l-2-norm* and the $\lambda$ value. We can see that for $\lambda = .16$ we have the highest shrinkage of the coefficients

We can also use the `predict` method to estimate coefficients at a new value of $\lambda$, say $\lambda = 80$

```{r}
predict(ridge.mod, s=80, type="coefficients")
```

Let's also look a the `%Dev` column when we print our model directly.

```{r}
# print(ridge.mod)   output is too long
```

`%Dev` is the *Null Deviance* of the model. If the *Null Deviance* is very small, it signifies that the proposed model does not perform better than the null model. We can see that when lambda is very high, the `%Dev` is almost 0, but increases as lambda decreases, and the penalty on the coefficients is relaxed. 


Rather than supplying a pre-defined grid,  we can use cross-validation to select the best lambda. The function `cv.glmnet` not only fits 100 models, but it tells us which one is the best.  

```{r}
set.seed(10)

ridge.cv <- cv.glmnet(X.train, Y.train, alpha = 0)

ridge.pred <- predict(ridge.cv, s="lambda.min", newx = X.test) 


ridge.RMSE <- RMSE(ridge.pred)


bestlam <- ridge.cv$lambda.min
```
Here `cv.glmnet` uses 10-fold cross validation (by default) and returns a `cv.glmnet` object containing a sequence of models. The model that miminimized the MSE can be found using `lambda.min`. Here the best lambda is `r round(bestlam, 3)` and its corresponding estimate of the test error is `r round(ridge.RMSE, 3)`. We could access the coefficients of this model by calling `coef(ridge.cv, s = "lambda.min")` and calculate their *l-2-norm* `r round(el2norm(coef(ridge.cv, s = "lambda.min")), 2)`.

## The Lasso

The Lasso is similar to ridge regression, except that the penalty is applied to the *l-1-norm* norm of the coeffiecients: $||\beta||_1$. That is, the lasso selects the best model by minimizing the quantity $RSS + \lambda\sum_{j=1}^p|\beta_j|$. Unlike Ridge regression, however, the lasso shrinks some of the coefficients to zero, and thus performs *variable selection*. The advantage of the lasso is that it yields *sparse* models with less coefficients are easier to interperet. 

```{r}
set.seed(1)

lasso.cv <- cv.glmnet(X.train, Y.train, alpha = 1)

lasso.lam <- lasso.cv$lambda.min

coef(lasso.cv, s = "lambda.min")

lasso.mod <- glmnet(X.train, Y.train, alpha = 1, lambda = grid)

lasso.pred <- predict(lasso.mod, s=lasso.lam, newx = X.test)


```

We can see that some of the variables have been dropped. 

```{r}
lasso.RMSE <- RMSE(lasso.pred)

my_results <- rbind(my_results, list("Ridge", ridge.RMSE), list("Lasso", lasso.RMSE))

knitr::kable(my_results, digits = 3)

```

For some reason, the ridge and lasso perform substantially worse than the ordinary least squares model.


##Principle Components Regression

```{r}
library(pls)
set.seed(2)
PCR.mod <- pcr(Apps ~ ., subset = train, data = data_df,  validation = "CV")

validationplot(PCR.mod, val.type = "MSEP")
```

The validation plot compares the performance of PCR based off the number of components that are chosen. The MSEP axis is actually the RSME used to measure the training error. The error stabilizes around 5 or 6 components, and overall decreases monotonically as we include more components. Using all components would defeat the purpose of PCR and actually return the OLS solutions, where each variable is its own component.

```{r}
summary(PCR.mod)
```

The summary which reveals the training error and variance explained per number of components included, further shows that including more than 6 components gives no benefit. However it may be best to use even less components to create a better model. Three components should work, since it explains 93% of the variance.  

```{r}
PCR.pred <- predict(PCR.mod, X.test, ncomp = 3)
PCR.RMSE <- RMSE(PCR.pred)
my_results <- rbind(my_results,  list("PCR", PCR.RMSE))

knitr::kable(my_results, digits = 3)

```

##Partial Least Squares

Partial least sqaures is a supervised alternative to principle components regression. PLS attempts to choose components that explain variance in both the predictors and response. 

```{r}
PLS.mod <- plsr(Apps ~ ., data = data_df, subset = train, validation = "CV")
summary(PLS.mod)
validationplot(PLS.mod)

```

We can see that 3 components explains about 93% of the variance and gives a low training RMSE. It's better to keep the amount of components lowe to decrease the variance of the model, even if more components give slightly lower training RMSE. 

```{r}
PLS.pred <- predict(PLS.mod, X.test, ncomp = 3)
PLS.RMSE <- RMSE(PLS.pred)
my_results <- rbind(my_results, list("PLS", PLS.RMSE))
knitr::kable(my_results, digits = 3)
```

Since PLS supervises the choice of components, it can be more effective than PCR for a regression problem when using less components. Notice how the RMSE at 1 component is alsmost 3 times that for PCR than PLS, but it becomes about equal for the inclusion of more components.  

#Simplifying with Caret

In this section, I write a function using caret to train all the models at once. Each model's parameters is optimized by cross validation automatically, as specified in the trainControl function. 

```{r message = FALSE}
library(caret)

caret_RMSE <- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

trainControl <- trainControl(method="cv", number=5)


  my_caret <- function(method_name){ 
    #this function accepts the name of the method and returns its RMSE from testing it on our specified College dataset
  
  method_fit <- train(Apps~., data=data_df, method=method_name, metric="RMSE", preProc=c("center","scale"), trControl=trainControl)
  
  method_predictions <- predict(method_fit, X.test)
  
  method_RMSE <- caret_RMSE(method_predictions)
  
  list(method_name, method_RMSE)
  }
  
caret_names <- list("lm", "lasso", "ridge", "glmnet", "pcr", "pls", "lars")
```

Now I write a short pipeline to run the `my_caret` function on the list of method names and display the my_results in a nice table.  

```{r}
map(caret_names, my_caret) %>%
  transpose() %>% 
  map(unlist) %>% 
  set_names(c("models","RMSE")) %>% 
  as_tibble() %>%
  arrange(RMSE)-> 
  caret_table

knitr::kable(caret_table, digits = 3, booktabs = TRUE, caption = "Result produced by Caret. Models arranged by RMSE.")
```

The my_results are quite similar to running each model individually. The ridge and lasso are slightly better, probably because the caret algorithm picked a better lambda. However there is a noticable discrepancy between the original least squares call and this least-squares call, which is comparable to other regression methods. 

# Subset selection

##Best Subset Selection

Subset selection is a technique for selecting a subset of your original varaible to be predictors in your model. For example, let's say we want to fit a least squares model to the training data, but we suspect that it will generalize better to testing data if we keep fewer variables. Perhaps some are reduntant or lack predictive power. In the *Best Subset Selection* procedure, we consider all possible models. Since we are working with the College data which has 17 predictor variables, this will require fitting $$\binom{17}{0} + \binom{17}{1} + \binom{17}{2} + \dots + \binom{17}{17} = \sum_{k = 0}^{17}\binom{17}{k} = 2^{17}$$ models. 
A quick aside regarding the combinatoric identity: $\sum_{k = 0}^{p}\binom{p}{k} = 2^{p}$. It arises from the question, what is the cardinality of the power set of a set containing $n$ objects. That is, how many distinct subsets can be created from this set. We can apprach this two ways: iterate over each element and decide whether or not to include it in your subset: this gives $2^p$ choices. Or compute individually the amount of subsets of size $k$ from $0\leq k \leq n$ by taking combinations, and then summing the counts. These two approaches are identical, hence the formula. Let's apply:

```{r warning=FALSE, message=FALSE}
library(leaps)
BEST.mod_9 = regsubsets(Apps ~ ., data_df)
summary(BEST.mod_9)
```

Th asterix indicated whether the variable is included in the model. The row indices indicate a model of that many variables, so each row is a separate model, each one containing one more variable than in the previous row. The algorithm calculates only up to 9-variable models by default. Lets try setting `nvmax = 17` to include all models, and then extract some statistics.

```{r message = FALSE}
BEST.mod_17 <- regsubsets(Apps ~ ., data_df, nvmax = 17)
bmod.summary <- summary(BEST.mod_17)

```



```{r}

show_metrics <- function(my_summary){

metrics <- c("adjr2", "rsq", "bic", "cp")

best_df <- as.data.frame(`[`(my_summary, metrics)) 

best_df_melt <- best_df %>% gather(key = "metric", value = "value") %>% mutate(model = rep(1:17, 4))



(ggplot(data = best_df_melt, aes(x = model, y = value, color = metric)) +
         geom_line() +
        facet_grid(metric ~ ., scales = "free_y")) %>% print()

c(map_dbl(best_df[c("adjr2", "rsq")], which.max), map_dbl(best_df[c("bic", "cp")], which.min))

}
```



```{r}
show_metrics(bmod.summary)
```

The graphs show how the error metric changes as we introduce more variables into the model. They seem to drop off rapidly and then hit a max or min, as displayed in the table.


## Estimates of Test error
These statistics are intended to estimate the test error of the model by making an adjustment to the training error to account for the bias due to overfitting in the training process. 

Let's interperet each of these statistics (exact formulas can be looked up): 

- $R^2_{adj}$  modifies the denominator of $RSS$ in $R^2$ to  $RSS/(N - p - 1)$, where $p$ is the number of predictor variables in our model, thus inflating the error as we add more predictors.  Unlike $R^2$ which will increase monotincally with additionaly variables,$R^2_{adj}$ penalizes the addition of noise variables. Both these statistics measure the goodness-of-fit of the model on a scale of $0$ to $1$, so we want to choose the model that maximizes them. 

- $C_p$ adds a penalty of $p *\hat{\sigma}$ to the RSS, where sigma estimates the variance in the response. Clearly $C_p$ increases with more predictors, so we want to minimize it.

- $AIC$ is proportional to $C_p$ so it should be minimized

- $BIC$ adds a term of $log(N) * p * \hat{\sigma}^2$ to the $RSS$ where $N$ is the number of obserations, so it penalizes models with many variables and observations. Thus, we want to mimize it. The more severe penalty given by the $BIC$ also explains why it chose the sparsest model. 

##Forward Selection

Clearly Best Subset selection is very computationally intensive, so shorter alternatives have been developed. *Forward Selection* works by adding in the best variable at each stage. That is, it chooses to include the variable that minimizes the model's RSS, and then keeps that variable while choosing a new one from the remaining variables, until all are used. This my_results in having to test $1 + \sum_{k=0}^{p-1}p - k  = 1 + \frac{p(p+1)}{2}$ models. In our College dataset, $p = 17$, so the total number of models we fit is $$1 + 17 + 16 + \dots + 2 + 1 = 1 + \frac{(17)(18)}{2} = 154$$ This is substantially less computation. Note that the additional one is from fitting the null model. At each stage the best variable is chosen by comparing $R^2$ amongst the different possible models. At the end, we get $p+1$ models: $M_0, \dots, M_{17}$. Since these models do not have the same number of variables, we cannot compare them directly using $R^2$ (it will increase monotionically). Instead we can use one of the alternative metrics listed earlier. 

```{r}
FWD.mod <- regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = "forward")

sum.FWD <- summary(FWD.mod)


show_metrics(sum.FWD)


```

The metrics are the same for both methods.


Let's compare the speed of these algorithms. 


```{r include = FALSE}
FWD_time <- microbenchmark(regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = "forward")) 
BEST_time <- microbenchmark(regsubsets(Apps ~ ., data = data_df, nvmax = 17))
```
```{r}
BEST_time
FWD_time
```

We can see that forward selection is on average a few miliseconds faster than the full model selection. 

Let's compare the models they selected as determined by the BIC. Since the BIC was minimized by the model with 10 variables, we can see if they are the same variables for the Best subset and Forward selection. 

```{r}
sum.FWD$which[10, ] == bmod.summary$which[10, ]
```

It looks like both methods give the same my_results. Therefore we can opt for the faster, forward selection method. However, this was a small data set. my_results may vary for large high-dimensional data. 

#Decision Trees

## Fitting Classification Trees

In this setion I we will fit a classification tree to the College data set to predict whether or not a college is private. In fact, we use the original dataset, not the `data_df` that encodes the private factor as a dummy variable. 

```{r}
library(tree)
tree.College <- tree(Private ~ ., data = College)
summary(tree.College)
```


The misclassification error rate, which is the training error rate, is about $5%$, which is quite good. The deviance reported is given by $$-2\sum_m\sum_kn_{nm}\log\hat{p}_{mk}$$ Where $n_{mk}$ is the number of observations in the $m$th terminal node that belong to the $k$th class. 
Where $m$ is the number of terminal nodes of the tree, 10 in this case, and $k$ can take $2$ values: one for each class label (public or private). $\hat{p}_{mk}$ is the class propotion for a specific node, so in this case it is $\frac{n_{mk}}{n_{m1} + n_{m2}}$ for $k = 0$ or $k=1$. We can see that this formula for deviance looks like the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy). It follows that we can interpret it as a cumulative measure of node purity. To elaborate let's discuss how the decision trees work. They divide up the predictor space into $m$ regions which can be visualized as a cut-up rectangular box where each box is the region, or an upside-down tree where the terminal leaves are the regions.
<img src="treeimage.png">

For a classification tree, the class label that is a assigned to a data point in a certain region of the predictor space is the *majority vote* of class labels in that region. Since we only have 2 class labels, it would be whichever label is dominant. Therefore, misclassification error occurs when there are training points in a region where the majority vote has a different label. That's why we want to measure the region's or node's purity: the percentage of training points that belong to a single label. By maximizing the purity, we minimize the training error. An easily interpretable measure of node purity is the Gini index: $$G = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})$$ which gives the purity for the $m$th node. If all the training labels belong to almost a single class $k$, we can see that $G$ is almost 0 and the node is very pure. The *cross-entropy* and similarly the deviance have a similar interpretaion to the Gini index, and thus we want to minimize them to get the best classfier. 

The *residual mean deviance* statistic reported is simply the deviance divided by $n - |T|$ where $n = 777$ is the number of observations, and $|T| = 10$ is the number of terminal nodes. A small RMD indicates that the tree provides a good fit to the data. 

We can print out a visual representation of the tree. 
```{r}
plot(tree.College)
text(tree.College)
```

We can see the most important determining variable is the number of full time undergraduates. From the leftmost branch we can see that a second split of the `F.Undergrad` predictor was significant: smaller schools tend to be private. The out-of-state tuition is also an important fact: schools with smaller enrollments but higher tuition tend to be private, and if the tuition is just really high, it is probably private. Private schools also have a larger percentage of alumni who donate back.

To get an estimate of the test error we need to split the data into a training and validation set.


```{r}
College.test <- College[-train, ]
Private.test <- College$Private[-train]
tree.college <- tree(Private ~ ., data = College, subset = train)
tree.pred <- predict(tree.college, College.test, type = "class")
tree.table <- table(tree.pred, Private.test)
```

The table function gives a confusion matrix to explain the performance of the classifier:

```{r}
tree.table
```

By dividing the counts in the off-diagonal by the total observations, we get the misclassification error:

```{r}

error_rate = function(myTable)  {(myTable[1,2] + myTable[2, 1]) / (sum(myTable))}

error_rate(tree.table)


```

## Cost-complexity pruning

Fitting a deep classification tree may produce good predictions on the training set but is likely to overfit the data, leading to poor performance on the test set. It may be better to build a less copmlex tree that has more bias but lower variance when we test it against new data. To accomplish this we can use a formula that penalizes the complexity, or number of terminal nodes of the tree: $$\sum_{m = 1}^{|T|}\sum_{i:x_i\in R_m}(y_i - \hat{y_{R_m}})^2+\alpha |T|$$ Alpha is the tuning parameter that adds a penalty for more leaves. When alpha is 0, we get the original tree. As it increases, we get a sequence of subtrees, with less or equal node to the previous tree. Cross validation can be used to select the optimal value for alpha. This process is done on the training data set. Once we choose an alpha, we can test that tree's performance on the test set. 

```{r}
set.seed(1)
cv.college = cv.tree(tree.college, FUN = prune.misclass)
cv.college

best_size <- function(x){x$dev %>% which.min() %>% `[`(x$size, .)}
best_size_1 <- best_size(cv.college)

```

Explanation of the output: `size` refers to the number of terminal nodes in the tree, `dev` is the CV error, `k` is the tuning parameter $\alpha$.
We set `FUN = prune.misclass`to indicate that classification error is what guides the tuning process, as opposed to the default metric: deviance. 

```{r}
 
  
plot_cv <- function(cv_object){  as.data.frame(cv_object[c('size', 'dev', 'k')]) %>% gather(key = "metric", value = "value") %>% mutate(index = rep(1:length(cv_object$size), 3)) %>%
ggplot(data = ., aes(x = index, y = value, col = metric)) +
  geom_line()}

plot_cv(cv.college)
```

It looks like the CV error is minimized for a tree of size `r best_size_1`. 

Now we can apply the `prune.misclass` function to obtain this tree:

```{r}
set.seed(1)
prune.college <- prune.misclass(tree.college, best = best_size_1)
plot(prune.college)
text(prune.college, pretty = 0)
```

We can see it incorporates the 2 most important variables previously identified, out-of-state tuition and number of full-time undergraduates. 

```{r}
prune.pred = predict(prune.college, College.test, type = "class")
prune.table <- table(prune.pred, Private.test)
```

```{r}
prune.table
error_rate(prune.table)
```

Interestingly we have the same exact test error! If you compare both tables, you can see that the numbers are different but the sums are the same, yielding the same fraction. Now we have a much more interpretable tree with the same exact performance. 

## Fitting Regression Trees

Regression trees work much like classification trees, except that we estimate points by taking the average of the training responses of the region that it falls into, whereas in classification we took the majority vote. Let's see how a regression tree performs on our college dataset. 

```{r}
set.seed(1)
rtree.college <- tree(Apps ~ ., subset = train, data = College)
summary(rtree.college)
```


```{r}
plot(rtree.college)
text(rtree.college, pretty = 0)
```

Notice that only two predictors were used in the construction of this tree, but they are split 6 times. The *residual mean deviance* is just the mean squared error for the tree. `Accept` refers to the number of applications accepted. It makes perfect sense that this quantity would be correlated with `Apps` which is the number of applications received. I did not expect that graduateion rate would be the other important variable; however, it is only used in one split. 

```{r}
rtree.yhat <- predict(rtree.college, College[-train, ])
TREE.RMSE <- RMSE(rtree.yhat)
TREE.RMSE
```



```{r}
set.seed(1)
rcv.college = cv.tree(rtree.college)
plot_cv(rcv.college)
rcv.college
best_size_2 <- ifelse(best_size(rcv.college) != 1, best_size(rcv.college), 5)
```
It looks like the best tree is of size `r best_size_2`. Note that the `predict` function throws an error for a single-node tree.

```{r}
set.seed(1)
rprune.college <- prune.tree(rtree.college, best = best_size_2)

plot(rprune.college)
text(rprune.college, pretty = 0)

rcollege.pred = predict(rprune.college, newdata = College[-train, ])

RMSE(rcollege.pred)
```

Noice that this performs exactly the same as the unpruned tree even though it uses `Terminal`: percent of faculty with terminal degrees, as the secondary predictor.

```{r}
my_results <- rbind(my_results, list("Tree", TREE.RMSE)) %>% arrange(RMSE)
knitr::kable(my_results, digits = 3)
```

We can see that the regression tree performs worse than the optimized linear regression models. Perhaps we can infer that a linear decision boundary is a better fit for this data set. The tree will perform better when the predictor space can be divided up into rectangular regions.

<img src="treefit.png">

## Bagging

Bagging is a technique to reduce the variance of a statistical learning procedure. If we fit a deep tree to the data, we may have overfitting, so the model will not generalize well. However, if we fit many trees and average the results we will have lower variance when generalizing. Bagging stands for *bootstrap aggregation* and works as follows: Take $B$ bootstrap samples and fit a full tree to each sample. Our prediction for an observation $x$ is $$\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)$$

```{r}
library(randomForest)
set.seed(1)
bag.college <- randomForest(Apps ~ ., data = College, subset = train, mtry = 17, importance = TRUE)
bag.college
varImpPlot(bag.college)
```

To discuss these error metrics, we need to discuss $OOB$: out-of-bag observations. For each observation, it will be left out of about $B/3$ bootstrap sample, and is said to be an out-of-bag observation. TWe can estimate the error of a single observation by finding the residual each time it was out of bag, and averaging them. The root mean square of these residuals estimates the test error. The `%IncMSE` reports the mean decrease of accuracy in predictions on the out-of-bag samples when a given variable is excluded from the model, where the $OOB$ sample is the observations left out when we take a bootstrap sample. The `IncNodePurity` is a measure of the total decrease in node impurity that results from splits on that variable, averaged over all trees. In case of regression trees, node impurity is measured using RSS

Note that bagging is a special case of random forests where all the predictors are used, so we can call the `randomForest` function and specify `mtry = 17`. 

```{r}
yhat.bag <- predict(bag.college, newdata = College[-train, ])
RMSE(yhat.bag)
```

This is an improvement over the plain regression tree. 

## Random Forests

Random forest provide an improvement over bagging by *decorrelating* the trees. With a random foresst, you are only allowedd to consider a subset of predictors during each split. For example, in our bagging model, for each split consider which of the 17 predictors will yield a greatest decrease in residual error and we split that predictor. With random forests we choose a smaller number, usually $\sqrt{p}$ for classification and $p/3$ for regressoin, so in this case $6$ predictors. During each split we choose another random sample of $6$ predictors from our original 17 and only consider the best split amongst those predictors. This decorrelates the trees especially when you have a couple of very strong predictors driving the splits. Then if you fit 500 trees, they'll have similar specifications even if the bootstrapped samples are different. Random forests ensure you are considering the contributions of each predictor in making the best decision. 

```{r}
set.seed(1)
rf.college <- randomForest(Apps ~ ., data = College, subset = train, mtry = 6, importance = TRUE)
yhat.rf = predict(rf.college, newdata = College[-train, ])
RF.RMSE <- RMSE(yhat.rf)
RF.RMSE

```

The random forest did not improve over bagging. 

```{r}
importance(rf.college)
varImpPlot(rf.college)
```

The three clear most important variables are the number of accepted applicants, number of new students enrolled, and number of fulltime undergraduates. Even though the performance on this test set was not much better than bagging, I believe this model would generalize much better. The bagging model was dominated by the `Accept` predictor and did not properly weight the contributions of `Enroll` and `F.Undergrad`, even though they were still identified as the most important. Instead the mysterious `Top25perc` found its way to the top, just like in the original tree. 

## Boosting

Boosting is a "slow learning" method creates an additive model after training trees on bootstrapped samples.. The algorithm for boosting begins by assuming that your prediction for yhat is 0, so your residual is yhat - 0 = yhat. It fits a tree on each bootstrapped sample, each time adding a small increment to yhat, and decreasing the residual. Each prediction is actually the sum of small predictions made on each bootstrapped set, where each new prediction is made in order to decrease the residual slightly more. The output is the boosted additive model. Here is the algorithm as shown in the ISLR book: 

<img src="boosting.png">

Lambda is the shrinkage parameter applied to each boostrap prediction. The parameter $d$, called the ineraction depth,  can be quite small, and results in fitting small trees. By fitting small trees to the residual, we slowly improve f.hat in areas where it does not perform well. Since each tree takes into account trees that have already been grown, small trees are sufficient, which makes this model much different from the bagging approach. 

```{r}
library(gbm)
set.seed(1)
boost.college <- gbm(Apps ~ ., data = College[-train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.college)
```

Note that the distribution assumed on the response is gaussian, since this is a regression problem (bernoulli for classification), and the shrinkage parameter is by default set to $.001$. The relative influence plot confirms that accept and enroll are by far the most important variables. 

```{r}
yhat.boost <- predict(boost.college, newdata = College[-train, ], n.trees = 5000)
BOOST.RMSE <- RMSE(yhat.boost)
BOOST.RMSE
```

Boosting well outperforms the random forest approach and is on par with ridge regression. 

```{r}
my_results <- rbind(my_results, list("Boost", BOOST.RMSE), list("RF", RF.RMSE)) %>% arrange(RMSE)
knitr::kable(my_results, digits = 3)
```

