---
title: "ML"
author: "Harrison Tietze"
output:
  html_document:
    toc: yes
    css: faded.css
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(ISLR)
library(tidyverse)
library(glmnet)
library(magrittr)
library(microbenchmark)
```

#Regression Models

reference: *Introduction to Statistical learning* (2015) *Tibshirani, Hastie*

##glmnet

In this analysis we will work with the "College"
dataset, which contains 777 observations with 18 variables. The goals is to predict how many applications a college will recieve "Apps" based off other predictors. 

First examine the structure of the dataset:

```{r}
str(College)
```

The variables are numeric, except for the factor: Private. Let's build a model matrix that automatically creates dummy variables, which is important because glmnet only accepts numeric variables.

```{r}
X = model.matrix(Apps ~ . - 1, data = College)[, -1]
Y = model.matrix(~ Apps + 0, data = College)

data_df <- as.data.frame(cbind(X, Y))

```


Since we will be testing the performance of multiple models, we need to split the data into a test and training set.

```{r}
train <- sample(1:nrow(X), replace = FALSE, round(nrow(X) * .25))
X.train <- X[train, ]
Y.train <- Y[train]

X.test <- X[-train, ]
Y.test <- Y[-train]
```


Let's fit a least-squares regression, which can be done using glmnet and no further arguments. The 17 coefficients of the least-squares regression model are fit by minimizing the RSS, the sum of squared residuals. 

```{r}
OLS.mod <- glmnet(X.train, Y.train)

OLS.pred <- predict(OLS.mod, newx = X.test)

RMSE <- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

Percent.Error <- function(x){RMSE(x)/mean(Y) * 100}

OLS.RMSE <- RMSE(OLS.pred)
OLS.Percent.Err <- Percent.Error(OLS.pred)

results <- tibble(model = "OLS",
                  RMSE = OLS.RMSE,
                  )

knitr::kable(results, digits = 3)
```


The RMSE can be interpreted as an estimate for the standard deviation of predicted values. In the context of our College data, given a new set of observations, we could estimate the amount of applications a college will receive, give or take 7.6 apps. The percent error, given by the RMSE over the average value of Y, gives an estimate of percent average deviation of our prediction. This helps interpret the error, given the average college receives `r round(mean(Y))` and our percent error is `r OLS.Percent.Err`%, we can infer our error is overall relatively small. 


##Ridge Regression

Ridge regression fits a linear model by minimizing the quanitity $RSS + \lambda \sum_{j=1}^p\beta_j^2$, where $\lambda \sum_{j=1}^p\beta_j^2$ also written $\lambda||\beta||^2$ is called a *shrinkage penalty* and $\lambda \geq 0$ is called a *tuning parameter*. At the extremes, $\lambda = 0$ returns the least-squares estimate, and $\lambda \rightarrow \infty$ returns the *null model*, where all the predictors are forced to $0$. Note, that shrinkage is not applied to the intercept. Instead we first center the inputs and then estmiate $\beta_0$ by $\overline{y}$. Centering is also necessary because unlike in OLS, the scale of the predictors can influence the estimate. Centering predictors is done by dividing by their standard deviation: $\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^N(x_{ij}-\overline{x_j})^2}}$. The $\tilde{x_{ij}}$ are called *standardized predictors* and are used to compute *standardized coefficients*. 

Unlike the least squares estimate which produces only one set of coefficients, ridge regression computes a *path* of coefficients $\beta^R_{\lambda}$ which is a $p-$dimensional sequence indexed by $\lambda$. Cross validation must be used the to select the $\lambda$ such that $\beta^R_{\lambda}$ minimizes the out-of-sample error. 

Let's fit a ridge regression. Also, we can manually specify a grid of values for $\lambda$:

```{r}
grid <- 10^ seq (10,-2, length =100)
ridge.mod <- glmnet(X.test, Y.test, alpha = 0, lambda = grid)  

# alpha = 0 specifies that we are using a Ridge Regression.
# glmnet automatically does centering first; Standardize = TRUE by default
```

The glmnet object has several useful accessor methods that return information about the model. `coef` returns a matrix of coefficients. There are 19 rows, one for each coefficient, and 100 columns corresponding to different lambda values. 

```{r}
dim(coef(ridge.mod))
```
The `lambda` attribute returns the lambda values we generated. Let's looks at the coefficients calculated for the 10th, 50th, and 90th lambda.

```{r}



el2norm <- function(x){sqrt(sum(x^2))}

el2norm.r <- function(col){el2norm(coef(ridge.mod)[, col])}

coef(ridge.mod)[,c(10,50,90)] %>% rbind( map_dbl(c(10,50,90), el2norm.r) ) %>% rbind(ridge.mod$lambda[c(10,50,90)])



```
The last two rows are the *l-2-norm* and the $\lambda$ value. We can see that for $\lambda = .16$ we have the highest shrinkage of the coefficients

We can also use the `predict` method to estimate coefficients at a new value of $\lambda$, say $\lambda = 80$

```{r}
predict(ridge.mod, s=80, type="coefficients")
```

Let's also look a the `%Dev` column when we print our model directly.

```{r}
# print(ridge.mod)   output is too long
```

`%Dev` is the *Null Deviance* of the model. If the *Null Deviance* is very small, it signifies that the proposed model does not perform better than the null model. We can see that when lambda is very high, the `%Dev` is almost 0, but increases as lambda decreases, and the penalty on the coefficients is relaxed. 


Rather than supplying a pre-defined grid,  we can use cross-validation to select the best lambda. The function `cv.glmnet` not only fits 100 models, but it tells us which one is the best.  

```{r}
set.seed(10)

ridge.cv <- cv.glmnet(X.train, Y.train, alpha = 0)

ridge.pred <- predict(ridge.cv, s="lambda.min", newx = X.test) 


ridge.RMSE <- RMSE(ridge.pred)


bestlam <- ridge.cv$lambda.min
```
Here `cv.glmnet` uses 10-fold cross validation (by default) and returns a `cv.glmnet` object containing a sequence of models. The model that miminimized the MSE can be found using `lambda.min`. Here the best lambda is `r round(bestlam, 3)` and its corresponding estimate of the test error is `r round(ridge.RMSE, 3)`. We could access the coefficients of this model by calling `coef(ridge.cv, s = "lambda.min")` and calculate their *l-2-norm* `r round(el2norm(coef(ridge.cv, s = "lambda.min")), 2)`.

## The Lasso

The Lasso is similar to ridge regression, except that the penalty is applied to the *l-1-norm* norm of the coeffiecients: $||\beta||_1$. That is, the lasso selects the best model by minimizing the quantity $RSS + \lambda\sum_{j=1}^p|\beta_j|$. Unlike Ridge regression, however, the lasso shrinks some of the coefficients to zero, and thus performs *variable selection*. The advantage of the lasso is that it yields *sparse* models with less coefficients are easier to interperet. 

```{r}
set.seed(1)

lasso.cv <- cv.glmnet(X.train, Y.train, alpha = 1)

lasso.lam <- lasso.cv$lambda.min

coef(lasso.cv, s = "lambda.min")

lasso.mod <- glmnet(X.train, Y.train, alpha = 1, lambda = grid)

lasso.pred <- predict(lasso.mod, s=lasso.lam, newx = X.test)


```

We can see that some of the variables have been dropped. 

```{r}
lasso.RMSE <- RMSE(lasso.pred)

results <- rbind(results, list("Ridge", ridge.RMSE), list("Lasso", lasso.RMSE))

knitr::kable(results, digits = 3)

```

For some reason, the ridge and lasso perform substantially worse than the ordinary least squares model.


##Principle Components Regression

```{r}
library(pls)
set.seed(2)
PCR.mod <- pcr(Apps ~ ., subset = train, data = data_df,  validation = "CV")

validationplot(PCR.mod, val.type = "MSEP")
```

The validation plot compares the performance of PCR based off the number of components that are chosen. The MSEP axis is actually the RSME used to measure the training error. The error stabilizes around 5 or 6 components, and overall decreases monotonically as we include more components. Using all components would defeat the purpose of PCR and actually return the OLS solutions, where each variable is its own component.

```{r}
summary(PCR.mod)
```

The summary which reveals the training error and variance explained per number of components included, further shows that including more than 6 components gives no benefit. However it may be best to use even less components to create a better model. Three components should work, since it explains 93% of the variance.  

```{r}
PCR.pred <- predict(PCR.mod, X.test, ncomp = 3)
PCR.RMSE <- RMSE(PCR.pred)
results <- rbind(results,  list("PCR", PCR.RMSE))

knitr::kable(results, digits = 3)

```

##Partial Least Squares

Partial least sqaures is a supervised alternative to principle components regression. PLS attempts to choose components that explain variance in both the predictors and response. 

```{r}
PLS.mod <- plsr(Apps ~ ., data = data_df, subset = train, validation = "CV")
summary(PLS.mod)
validationplot(PLS.mod)

```

We can see that 3 components explains about 93% of the variance and gives a low training RMSE. It's better to keep the amount of components lowe to decrease the variance of the model, even if more components give slightly lower training RMSE. 

```{r}
PLS.pred <- predict(PLS.mod, X.test, ncomp = 3)
PLS.RMSE <- RMSE(PLS.pred)
results <- rbind(results, list("PLS", PLS.RMSE))
knitr::kable(results, digits = 3)
```

Since PLS supervises the choice of components, it can be more effective than PCR for a regression problem when using less components. Notice how the RMSE at 1 component is alsmost 3 times that for PCR than PLS, but it becomes about equal for the inclusion of more components.  

#Simplifying with Caret

In this section, I write a function using caret to train all the models at once. Each model's parameters is optimized by cross validation automatically, as specified in the trainControl function. 

```{r message = FALSE}
library(caret)

caret_RMSE <- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

trainControl <- trainControl(method="cv", number=5)


  my_caret <- function(method_name){ 
    #this function accepts the name of the method and returns its RMSE from testing it on our specified College dataset
  
  method_fit <- train(Apps~., data=data_df, method=method_name, metric="RMSE", preProc=c("center","scale"), trControl=trainControl)
  
  method_predictions <- predict(method_fit, X.test)
  
  method_RMSE <- caret_RMSE(method_predictions)
  
  list(method_name, method_RMSE)
  }
  
caret_names <- list("lm", "lasso", "ridge", "glmnet", "pcr", "pls", "lars")
```

Now I write a short pipeline to run the `my_caret` function on the list of method names and display the results in a nice table.  

```{r}
map(caret_names, my_caret) %>%
  transpose() %>% 
  map(unlist) %>% 
  set_names(c("models","RMSE")) %>% 
  as_tibble() %>%
  arrange(RMSE)-> 
  caret_table

knitr::kable(caret_table, digits = 3, booktabs = TRUE, caption = "Result produced by Caret. Models arranged by RMSE.")
```

The results are quite similar to running each model individually. The ridge and lasso are slightly better, probably because the caret algorithm picked a better lambda. However there is a noticable discrepancy between the original least squares call, which is noticably better than all other methods, and this least-squares call, which is comparable to other regression methods. 

# Subset selection

##Best Subset Selection

Subset selection is a technique for selecting a subset of your original varaible to be predictors in your model. For example, let's say we want to fit a least squares model to the training data, but we suspect that it will generalize better to testing data if we keep fewer variables. Perhaps some are reduntant or lack predictive power. In the *Best Subset Selection* procedure, we consider all possible models. Since we are working with the College data which has 17 predictor variables, this will require fitting $$\binom{17}{0} + \binom{17}{1} + \binom{17}{2} + \dots + \binom{17}{17} = \sum_{k = 0}^{17}\binom{17}{k} = 2^{17}$$ models. 
A quick aside regarding the combinatoric identity: $\sum_{k = 0}^{p}\binom{p}{k} = 2^{p}$. It arises from the question, what is the cardinality of the power set of a set containing $n$ objects. That is, how many distinct subsets can be created from this set. We can apprach this two ways: iterate over each element and decide whether or not to include it in your subset: this gives $2^p$ choices. Or compute individually the amount of subsets of size $k$ from $0\leq k \leq n$ by taking combinations, and then summing the counts. These two approaches are identical, hence the formula. Let's apply:

```{r warning=FALSE, message=FALSE}
library(leaps)
BEST.mod_9 = regsubsets(Apps ~ ., data_df)
summary(BEST.mod_9)
```

Th asterix indicated whether the variable is included in the model. The row indices indicate a model of that many variables, so each row is a separate model, each one containing one more variable than in the previous row. The algorithm calculates only up to 9-variable models by default. Lets try setting `nvmax = 17` to include all models, and then extract some statistics.

```{r message = FALSE}
BEST.mod_17 <- regsubsets(Apps ~ ., data_df, nvmax = 17)
bmod.summary <- summary(BEST.mod_17)

```



```{r}

show_metrics <- function(my_summary){

metrics <- c("adjr2", "rsq", "bic", "cp")

best_df <- as.data.frame(`[`(my_summary, metrics)) 

best_df_melt <- best_df %>% gather(key = "metric", value = "value") %>% mutate(model = rep(1:17, 4))



(ggplot(data = best_df_melt, aes(x = model, y = value, color = metric)) +
         geom_line() +
        facet_grid(metric ~ ., scales = "free_y")) %>% print()

c(map_dbl(best_df[c("adjr2", "rsq")], which.max), map_dbl(best_df[c("bic", "cp")], which.min))

}
```



```{r}
show_metrics(bmod.summary)
```

The graphs show how the error metric changes as we introduce more variables into the model. They seem to drop off rapidly and then hit a max or min, as displayed in the table.


## Estimates of Test error
These statistics are intended to estimate the test error of the model by making an adjustment to the training error to account for the bias due to overfitting in the training process. 

Let's interperet each of these statistics (exact formulas can be looked up): 

- $R^2_{adj}$  modifies the denominator of $RSS$ in $R^2$ to  $RSS/(N - p - 1)$, where $p$ is the number of predictor variables in our model, thus inflating the error as we add more predictors.  Unlike $R^2$ which will increase monotincally with additionaly variables,$R^2_{adj}$ penalizes the addition of noise variables. Both these statistics measure the goodness-of-fit of the model on a scale of $0$ to $1$, so we want to choose the model that maximizes them. 

- $C_p$ adds a penalty of $p *\hat{\sigma}$ to the RSS, where sigma estimates the variance in the response. Clearly $C_p$ increases with more predictors, so we want to minimize it.

- $AIC$ is proportional to $C_p$ so it should be minimized

- $BIC$ adds a term of $log(N) * p * \hat{\sigma}^2$ to the $RSS$ where $N$ is the number of obserations, so it penalizes models with many variables and observations. Thus, we want to mimize it. The more severe penalty given by the $BIC$ also explains why it chose the sparsest model. 

##Forward Selection

Clearly Best Subset selection is very computationally intensive, so shorter alternatives have been developed. *Forward Selection* works by adding in the best variable at each stage. That is, it chooses to include the variable that minimizes the model's RSS, and then keeps that variable while choosing a new one from the remaining variables, until all are used. This results in having to test $1 + \sum_{k=0}^{p-1}p - k  = 1 + \frac{p(p+1)}{2}$ models. In our College dataset, $p = 17$, so the total number of models we fit is $$1 + 17 + 16 + \dots + 2 + 1 = 1 + \frac{(17)(18)}{2} = 154$$ This is substantially less computation. Note that the additional one is from fitting the null model. At each stage the best variable is chosen by comparing $R^2$ amongst the different possible models. At the end, we get $p+1$ models: $M_0, \dots, M_{17}$. Since these models do not have the same number of variables, we cannot compare them directly using $R^2$ (it will increase monotionically). Instead we can use one of the alternative metrics listed earlier. 

```{r}
FWD.mod <- regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = "forward")

sum.FWD <- summary(FWD.mod)


show_metrics(sum.FWD)


```

The metrics are the same for both methods.


Let's compare the speed of these algorithms. 


```{r include = FALSE}
FWD_time <- microbenchmark(regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = "forward")) 
BEST_time <- microbenchmark(regsubsets(Apps ~ ., data = data_df, nvmax = 17))
```
```{r}
BEST_time
FWD_time
```

We can see that forward selection is on average a few miliseconds faster than the full model selection. 

Let's compare the models they selected as determined by the BIC. Since the BIC was minimized by the model with 10 variables, we can see if they are the same variables for the Best subset and Forward selection. 

```{r}
sum.FWD$which[10, ] == bmod.summary$which[10, ]
```

It looks like both methods give the same results. Therefore we can opt for the faster, forward selection method. However, this was a small data set. Results may vary for large high-dimensional data. 
