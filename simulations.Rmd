---
title: "Simulation"
author: "Harrison Tietze"
output:
  html_document:
    toc: yes
    css: faded.css
    number_sections: true
---

```{r echo=FALSE, message=FALSE}
knitr::opts_chunk$set(cache=TRUE)
library(tidyverse)
library(stringr)
```


#Methods of Generating Random Variables

##Inverse Transform Method

Let's simulate a random sample from a distribution with density  $f(x) = \frac{3}{32}x^5 \ \ \  0<x<2$. We find the distribution to be $F_X(x) = \frac{x^6}{64}$ and its inverse $x= F^{-1}_X(u) = 2u^{\frac{1}{6}}$ where $U \sim \text{Uniform}[0,1]$ by the probability integral transform. We can now sample values of x: 

```{r}
n <- 1000
u <- runif(n)
x <- 2*u^(1/6)
hist(x, probability = TRUE, main = expression(f(x)==textstyle(frac(3,32))*x^5))
y <- seq(0, 2, .01)
lines(y, 3/32 * y^5)
```

##Understanding the Inverse Transform Method

How did we get this uniform random variable? There is a method to the madness.

Let's take a look at our CDF:

```{r, echo=FALSE}
F <- function(x){x^6 / 64}
plot.function(F, from = 0, to = 2, ylab = "u", main = expression(F(x)==textstyle(frac(x^6,64))), sub = "Figure 2")
```

Geometrically, we have a smooth mapping that is contracting the the interval $[0,2]$ to the interval $[0,1]$. However, this is not being done evenly. Let's bin the x-axis into intervals of width $.25$ and compute the direct image of each bin.

```{r, echo = FALSE}
init_points <- seq(0, 1.75, .25)
end_points <- seq(.25, 2, .25)
img_init_points <- round(F(init_points),3)
img_end_points <- round(F(end_points),3)
img_length <- round(img_end_points - img_init_points,3)

as_interval <- function(a, b){
  str_c("[", a, " , ", b, "]")
}

img_table <- tibble(
  bin = as_interval(init_points, end_points),
  image = as_interval(img_init_points, img_end_points),
  img_length = img_length

)

knitr::kable(img_table, caption = "Note infinitesimals truncated to 0")

img_plot <- ggplot(data = img_table, mapping = aes(x = bin, y = img_length)) +
  geom_bar(stat = "identity")
  

```

We partition the domain into bins of equal length, which under the image of the CDF, paritions the interval $[0,1]$ into bins of *very* unequal length. If we then sample uniformly from the interval $[0,1]$ and return each point to its original bin (i.e take the inverse), we will end up with our original distribution

```{r echo = FALSE}
img_plot
```

The image length is just the proportion of points sampled uniformly from [0,1] whose inverse falls into the specified bin.

##Acceptance-Rejection Method

Suppose we are told that a coin is flipped 50 times, 10 of which land heads. We want to simulate a sample of possibles values for the bias of the coin. We can model our belief about the bias as a beta distribution with parameters `a = 10` and `b = 40` with density $$f(x) = \frac{49!}{9!39!}x^9(1-x)^{39}$$
```{r include= TRUE, echo = FALSE}
a <- 10
b <- 40
beta_density <- function(x) {
  (1/beta(a,b)) * x^(a-1) * (1-x)^(b-1)
}

plot.function(beta_density, from = 0, to = 1)
```

We will use the uniform distribution as our `g(x)` since it has the same domain and is easy to sample from. Now we solve for the threshold parameter `c` which satisfies: $$ \forall x \in [0,1]: \hspace{2mm}  \frac{f(x)}{g(x)} = \frac{49!}{9!39!}x^9(1-x)^{39} < c$$.
To solve for `c`, our graph of the density reveals we should choose the mode of the distribution for our choice of `x`. which has a convenient formula: $$mode = \frac{(a-1)}{(a+b-2)}$$
```{r include = TRUE, echo = FALSE}
beta_mode <- function(a,b){
  (a-1)/(a+b-2)
}
my_x <- beta_mode(a,b)
my_c <- beta_density(my_x) 

```

The mode of the beta distribution is `r my_x` and we calculate that c = `r round(my_c, 3)`. It follows that a random `x` from `g(x)` is accepted if $$\frac{f(x)}{cg(x)} = \frac{\frac{49!}{9!39!}x^9(1-x)^{39}}{7.159} > u$$

for some random uniform `u`. 
```{r}
n <- 1000  #target sample size
k <- 0     #counter for accepted
j <- 0     #iterations
y <- numeric(n)
acceptance_ratio <- function(x){beta_density(x)/my_c}

while(k < n){
  acceptance_threshold <- runif(1)
  j <- j + 1
  x <- runif(1)    #random variate from g
  if(acceptance_ratio(x) > acceptance_threshold){
    #accept x
    k <- k + 1
    y[k] <-  x
  }
}
```

In this simulation, j = `r j` iterations were required to generate a sample of n = `r n` variates, compared to the expected $cn =$ `r round(my_c * n)` iterations. Comparing the empirical and theoretical deciles confirms that our sample fits the beta distribution.

```{r echo = FALSE}
library(stringr)
#compare empirical and theoretical percentiles
p <- seq(.1, .9, .1)
Qsim <- quantile(y, p) #quantiles of sample
Q <- qbeta(p, a, b)    #theoretical quantiles
percentiles <- str_c(as.character(seq(10,90,10)),"%")
my_table <- rbind(Qsim, Q)
names(my_table) <- percentiles
knitr::kable(my_table, caption = "Pariwise comparison of empirical and theorectical quantile")

```

## Convolutions

Let $X_1 \dots X_n$ be an i.i.d list of random variables such that $X_j \sim X$. The distribution of their sum $S = X_1 + \dots + X_n$ is called the n-fold convolution of $X$ and has distribution $F^{*(n)}_X$. For exapmle, the chi-squared distribution with degree of freedom $\nu$ is the $\nu$-fold convolution of squared standard normals. Let's use this fact to simulate a random sample of size $n$ from a $\chi^2_{\nu}$ distribution.

```{r}
library(tidyverse)
n <- 1000
nu <- 6
X <- matrix(rnorm(n*nu), n, nu)^2
y <- rowSums(X)

comp <- data.frame(
theoretical = c(nu, 2*nu),
empirical = c(mean(y), mean(y^2) - (mean(y))^2),
row.names = c("mean", "variance")
)

knitr::kable(comp)


```

We have accurately generated a chi-squared random sample. 

## A Mixture of Gammas

A discrete mixture of random variables has a distribution $$F_X = \sum_{j=1}^n\theta_j F_{X_j}$$ where our weights $\theta_j$ sum to 1 and $X_1 \dots X_n$ is any sequence of random variables. 

Here I will investigate the behavior of a specific mixture of gamma random variables given by $$X_j \sim \text{Gamma}(r,\  \lambda_j = \frac{1}{j})$$ $$\theta_j = \frac{2j}{n(n+1)}$$ $$1\leq j \leq n$$. Our mixed random variable is $Y_n$ with distribution $$F_{Y_n} = \sum_{j=1}^n\theta_j F_{X_j}$$

I will simulate from mixtures of different sizes. Notice that in our mixture, the contribution of variables appearing later in the sequence is much higher than that of the previous ones. I hyopthesize that as $n$ increases, $F_{Y_n} \rightarrow F_{X_n}$.

```{r message=FALSE}
library(tidyverse)

N <- 1000 # sample size
n <- 50
r <- 3
gamma_mixture <- function(n = 50, r = 3, N = 1000){
  # n is the number of variables we are mixing
  # N is the sample size we a generating 
  index <- 1:n
  weights <- index/((n*(n+1))/2)
  index_sample <- sample(index, size = N, replace = TRUE, prob = weights)
  lambda <- 1/index_sample
  mixture_sample <- rgamma(N, shape = r, rate = lambda)
  mixture_sample
}

sizes <- c(50, 200, 1000)

sizes %>%
  map(function(x) gamma_mixture(n = x)) ->
  mixture_list

sizes %>%
  map(function(x) rgamma(N, shape = r, rate = 1/x)) -> 
  gamma_list

df <- tibble(
  data = c(mixture_list, gamma_list),
  n_size = rep(sizes, times = 2),
  distribution = factor(rep(c("mixture", "gamma"), each = 3), levels = c("mixture", "gamma"))
)

library(magrittr)
library(scales)
df %<>% unnest()

ggplot(data = df, aes(x = data)) +
  geom_density(position = "stack", aes(color = distribution)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 3)) +
  facet_grid(. ~n_size, scales = "free",  labeller = label_bquote(cols = n == .(n_size)))

```

The graphs supports our hypothesis that the similarity between the distributions of $Y_n$ and $X_n$  gets stronger as $n$ increases. You can see the sharp peak of the mixture distribution rapidly flattens out to match the gamma distribution. Note the variance of a gamma random variable $X_n$ with rate parameter $\frac{1}{n}$ and shape parameter $r$ is $$ Var[X_n] = n^2r$$

so the variance increases exponentially with our choice of $n$, flattening out the graph. In contrast, our gamma mixture takes contributions from all $X_{1\leq j\leq n}$ so it will have less variance and thus a sharper peak. 

# Monte-Carlo Integration

## Simple Monte Carlo Integration

We can use random number generation to estimate the integral of a function that would be difficult to solve analytically. In this case the quantity we want to estimate is really the value of an integral $\theta = \int_{\chi}g(x)dx$. The connection between generic integrals and random variables is given by the definition of expectation of a transformed random variable.  Recall that if $X$ is a random variable with density $f(x)$, then the mathematical expectation of the transformed random variable $g(X)$ is $$ \mathbb{E}_f[g(X)] = \int_{\chi}g(x)f(x)dx$$ where $\chi$ is the support of the density $f$. A simple case occurs when integrating a function $g(x)$ over $[0,1]$ because we can choose $X \sim \text{Uniform}[0,1]$. $$\theta = E[g(X)] = \int_0^1g(x)(1)dx = \int_0^1g(x)dx$$ We can then generate a random sample of uniforms $X_1 \dots X_m$ to get the monte-carlo estimate of $\theta$ using the sample mean: $$\widehat{\theta}_{MC} = \widehat{\mathbb{E}_f[g(X)]} =  \overline{g_m(X)} = \frac{1}{m}\sum_{i=1}^mg(X_i)$$
By the law of large numbers, the sample mean converges to true expected value. $$P\bigg(\lim_{m\rightarrow \infty}\left|\overline{g_m(X)} - E[g(X)]\right|<\epsilon \bigg) = 1$$ So we can expect that our estimate will approximate $\theta$ given a large enough sample. Let's use this to estimate $$\theta = \int_0^1 e^{-x}dx$$ and compare the estimate with the exact value.

```{r}
m <- 10000
x <- runif(m)
theta.hat <- round(mean(exp(-x)),6)
soln <- round(1-exp(-1),6)
st_err <- format((1/m)*(sum(exp(-x)-theta.hat)^2)^(1/2), digits = 3)

```

The estimate is $\widehat{\theta}=$ `r theta.hat` and $\theta = 1 -e^{-1}$ = `r soln`. 

##Estimating Standard Error

We can quantify the accuracy of our estimate. Note that in statistical parlance $\theta$ is a parameter we want to estimate, and our estimator is a sample statistic $$\widehat{\theta}_m = T(g(X_1), \dots g(X_m))$$ The population distribution of our sample statistic is called the sampling distribution. We want to find the standard deviaton of the sampling distribution called the standard error, denoted $SE(\widehat{\theta}_m)$. Since our estimator is a sample mean from the distribution of $g(X)$, we can estimate the standard error with $$SE(\widehat{\theta}_m) = \frac{\widehat{\sigma}}{\sqrt{m}} = \frac{1}{m}\left\{\sum_{i=1}^m|g(x_i)-\overline{g(x)}]^2\right\}^{\frac{1}{2}}$$

Using this formula we calculate $SE(\widehat{\theta}_{10^5})$ = `r st_err` for the previous example. 

## General Monte-Carlo Integration

We need not limit ourselves to uniform random variables for our monte-carlo estimator. Suppose we want to estimate the quantity $\theta = \int_{\chi}g(x)dx$. We can look for a random variable $X$ with pdf $f$ that is supported on $\chi$ and easy to sample from. Then we define the transformation $h(X) = \frac{g(X)}{f(X)}$. Taking the expectation gives us$$
\mathbb{E}_f[h(X)] = \int_{\chi}\frac{g(x)f(x)}{f(x)}dx = \int_{\chi}g(x)dx$$ We can get a monte-carlo estimate of $\theta$ using the sample mean of random variables $X_1, \dots, X_m$ generated from $f$: $$\widehat{\theta}_{MC} = \widehat{\mathbb{E}_f[h(X)]}  = \frac{1}{m}\sum_{i=1}^m\frac{g(X_i)}{f(X_i)}$$
Note that the example in (2.1) is special case where $f(X_i) = 1$ because we are using uniform random variables. In this example we will consider a function defined on an infinite domain, so we cannot choose any uniform pdf for our $f$. Consider $$\int_0^{\infty}\sin(x^2)e^{-x^2}dx$$ If we choose $X \sim \exp(\lambda = 1)$ then $f(x) = e^{-x}$ on $x\geq 0$ and $$h(x) = \frac{\sin(x^2)e^{-x^2}}{e^{-x}} = \sin(x^2)e^{-x^2 + x}$$
 
 
```{r}
m <- 10^5
sample <- rexp(m, rate = 1)
h <- function(x){
  sin(x^2)*exp(-x^2 + x)
}
estimate <- sum(h(sample))/m


```
Our Monte Carlo estimate $\widehat{\theta}_{MC}$ = `r estimate` approximates the true value determined by Mathematica:


$$\pmb{\text{}}\\
\pmb{\text{Integrate}\left[\text{Sin}[x{}^{\wedge}2]*e^{-x^2}, \{x, 0 , \text{Infinity}\}\right]} = \frac{\sqrt{\pi } \ \text{Sin}\left[\frac{\pi }{8}\right]}{2\ 2^{1/4}} = 0.285185$$ 

## Importance Sampling

Given a density $g$ that is strictly positive on the supported domain of $h\times f$ , we can write $$\mathbb{E}_f[h(X)] = \int_{\chi}h(x)\frac{f(x)}{g(x)}g(x)dx = \mathbb{E}_g\left[\frac{h(X)f(X)}{g(X)}\right]$$ 

Instead of sampling from a pdf $f$ we will can sample from a pdf $g$. This *importance sampling fundamental identity* justifies the use of the estimator $$\widehat{\mathbb{E}_f[h(X)]} = \frac{1}{m}\sum_{j=1}^m\frac{h(X_j)f(X_j)}{g(X_j)}$$ based on a sample generated from $g$. 

# Numerical Algorithms

##EM algorithm: Faking data with Rick and Morty

Rick and Morty have just returned from an off-world adventure on alien exoplanet DW7449 where they were retreiving slime samples from Type III Lazy Slug, a possible candidate for a new flavor of Sechuan Sticky Sauce. Two important flavor variables they need to measure are the slime's schleeb and its glubis which are known to be normally distributed. While taking measurements in Rick's workshop, Morty slipped on a slime patch crashing into the samples, corrupting the data before they could finish recording the glubis. Fortunately, Rick had already predicted that a typical Morty-related disaster would occur, and had preprogrammed an EM algorithm to rescue the lost data. Let's investigate what he did.  


---


```{r message=FALSE}
library(ggthemes)
library(MASS)

set.seed(3)
n <- 100     #sample size
d <- 2       #dimension
mu <- c(1,14) #vector of means
SIGMA <- matrix(c(12,3,3,6), ncol = 2, byrow = TRUE)

slime_data <- data.frame(mvrnorm(n, mu, Sigma = SIGMA))

names(slime_data) = c("schleeb", "glubis")

lost_data <- sample(1:n, size = n/2)

slime_data[lost_data, 2] <- 0 #Set the lost data values to 0


slime_plot <- ggplot(slime_data, aes(schleeb, glubis)) + 
  geom_point(color = "yellow") + 
  theme_solarized(light = FALSE) + 
  theme(text = element_text(size=20, color = "green")) +
  stat_smooth(method = "lm", color = "red", se = FALSE) + 
  coord_cartesian(ylim = c(-1,20))


```

This first step in the EM algorithm is create an artificially complete data set by arbitrarily setting the missing glubis values to 0. We call this dataset $slime^{(0)}$ it is plotted below. We don't expect the regression line to be accurate. 

```{r}
slime_plot
```



We know the slime data comes from a bivariate normal distribution with parameters $\theta = (\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$. To get maximum likelihod estimates for $\mu_2, \sigma_2$, and $\rho$ we need to properly fill in the missing data for the glubis.  

  To get one interation of the EM algorithm we can use maximum likelihood estimators to produce an estimate for $\widehat{\theta}^{(0)} = (\mu_1^{(0)}, \mu_2^{(0)}, \sigma_1^{(0)}, \sigma_2^{(0)}, \rho^{(0)})$ This is the M("Maximization Step")

```{r}
#construct a function that computes MLE's
theta_hat_estimator <- function(schleeb = slime_data$schleeb, glubis = slime_data$glubis){
mu_1 <- mean(schleeb)
mu_2 <- mean(glubis)
sigma_1 <- sd(schleeb)
sigma_2 <- sd(glubis)
rho <- cor(schleeb,glubis)
return(c(mu_1, mu_2, sigma_1, sigma_2, rho))
}
```

We can now impute the missing values using the formula for conditional expectation of a bivariate normal variable where $\theta = \widehat{\theta}^{(0)}$. This is the E"Expectation" step :   $$\mathbb{E}[x_{2i}|\widehat{\theta}^{(0)}] = \mu_2^{(0)} + \rho^{(0)}\frac{\sigma_2^{(0)}}{\sigma_1^{(0)}}(x_{1i} - \mu_1^{(0)}) $$ 
```{r}
#construct a function to update the slime data from slime_j -> slime_j+1
impute_glubis <- function(x_1, mu_1, mu_2, sigma_1, sigma_2, rho){
  mu_2 + rho*(sigma_2/sigma_1) * (x_1 - mu_1)
}

```

The E and M steps are repeated giving a new dataset $slime^{(j)}$ at the $j^{th}$ stage and updating the parameter estimate $\widehat{\theta}^{(j)}$ until reaching some convergence threshold $||\widehat{\theta}^{(j+1)} - \widehat{\theta}^{(j)}|| < \epsilon$.

```{r}
#Run the EM algorithm
epsilon <- .000005 # convergence threshold

theta_hat_0 <- rep(0,5)  #initialize temp variable
theta_hat_1 <- theta_hat_estimator(slime_data$schleeb, slime_data$glubis)
var_path <- c(0,theta_hat_1)
step = 1

condition <- function(){
  #convergence condition
  sqrt(sum((theta_hat_1 - theta_hat_0)^2)) > epsilon
}

while(condition()) {
  #EM algorithm
 
    slime_data[lost_data, 2] <- impute_glubis(slime_data[lost_data, 1], theta_hat_1[1], theta_hat_1[2], theta_hat_1[3], theta_hat_1[4], theta_hat_1[5])
  
  theta_hat_0 = theta_hat_1
  
  theta_hat_1 = theta_hat_estimator(slime_data$schleeb, slime_data$glubis)
  
  var_path = rbind(var_path, c(step, theta_hat_1))
  
  step = step + 1
}

knitr::kable(var_path, booktabs = F, align = c("c"), 
             col.names =  c("step", "$\\mu_1$", "$\\mu_2$", "$\\sigma_1$", "$\\sigma_2$", "$\\rho$")
             )
```

The last line in our table corresponds to the final estimates of our parameters. 

```{r}
slime_plot %+% slime_data
```

This is a plot of Rick's reconstructed data: $slime^{(22)}$. Notice how the imputed data points lie along the regression line- since they were computed using conditional expectation, this is to by design. 