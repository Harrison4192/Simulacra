<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Harrison Tietze" />


<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Harrison's website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About Me</a>
</li>
<li>
  <a href="simulations.html">Sims</a>
</li>
<li>
  <a href="zodiac.html">CLV</a>
</li>
<li>
  <a href="ML.html">ML</a>
</li>
<li>
  <a href="GLM.html">GLM</a>
</li>
<li>
  <a href="certificates.html">certificates</a>
</li>
<li>
  <a href="topology.html">Baire Space</a>
</li>
<li>
  <a href="KDE.html">KDE</a>
</li>
<li>
  <a href="https://docs.google.com/document/d/e/2PACX-1vTnpiuuxd0-ia3SMAk1SGusAyLyvwPN-wsl_dQCIC6Nglj0WgyTne0nYS1JISZZI6H-ym631b9B0kr0/pub">Novella</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore"><p style="color:purple;">
General Linear Models
</p></h1>
<h4 class="author"><em>Harrison Tietze</em></h4>
<h4 class="date"><em>Fall 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#correlation-coefficients"><span class="toc-section-number">1</span> Correlation Coefficients</a><ul>
<li><a href="#pearsons-r-spearmans-r_s-kendalls-tau"><span class="toc-section-number">1.1</span> Pearson’s <span class="math inline">\(r\)</span>, Spearman’s <span class="math inline">\(r_s\)</span>, Kendall’s <span class="math inline">\(\tau\)</span></a></li>
<li><a href="#comparing-r-r_s-and-tau"><span class="toc-section-number">1.2</span> Comparing <span class="math inline">\(r\)</span>, <span class="math inline">\(r_s\)</span>, and <span class="math inline">\(\tau\)</span></a></li>
<li><a href="#r-is-positive-but-r_s-and-tau-are-negative"><span class="toc-section-number">1.3</span> <span class="math inline">\(r\)</span> is positive but <span class="math inline">\(r_s\)</span> and <span class="math inline">\(\tau\)</span> are negative</a></li>
<li><a href="#construct-a-distribution-where-r_s-is-negative-but-tau-is-positive"><span class="toc-section-number">1.4</span> Construct a distribution where <span class="math inline">\(r_s\)</span> is negative but <span class="math inline">\(\tau\)</span> is positive</a></li>
</ul></li>
<li><a href="#implementing-glms"><span class="toc-section-number">2</span> Implementing GLM’s</a><ul>
<li><a href="#specifying-the-glm-in-r"><span class="toc-section-number">2.1</span> Specifying the GLM in R</a></li>
<li><a href="#specifying-the-link-function"><span class="toc-section-number">2.2</span> Specifying the Link Function</a></li>
<li><a href="#specifying-your-own-link-function"><span class="toc-section-number">2.3</span> Specifying your own link function</a></li>
</ul></li>
<li><a href="#quasi-likelihood-and-the-generalized-linear-model"><span class="toc-section-number">3</span> Quasi-likelihood and the Generalized Linear Model</a><ul>
<li><a href="#implementing-quasi-likelihood-in-r"><span class="toc-section-number">3.1</span> Implementing quasi-likelihood in R</a></li>
<li><a href="#comparing-different-models"><span class="toc-section-number">3.2</span> Comparing different models</a></li>
<li><a href="#wedderburns-quasi-likelihood-and-nelders-extension"><span class="toc-section-number">3.3</span> Wedderburn’s Quasi likelihood and Nelder’s extension</a></li>
</ul></li>
<li><a href="#applications-of-glms"><span class="toc-section-number">4</span> Applications of GLMs</a><ul>
<li><a href="#predicting-redshift-with-gamma-glm"><span class="toc-section-number">4.1</span> Predicting Redshift with gamma GLM</a></li>
<li><a href="#all-time-world-ranking-data"><span class="toc-section-number">4.2</span> All time world ranking data</a></li>
<li><a href="#weighted-items"><span class="toc-section-number">4.3</span> Weighted items</a></li>
</ul></li>
</ul>
</div>

<div id="correlation-coefficients" class="section level1">
<h1><span class="header-section-number">1</span> Correlation Coefficients</h1>
<div id="pearsons-r-spearmans-r_s-kendalls-tau" class="section level2">
<h2><span class="header-section-number">1.1</span> Pearson’s <span class="math inline">\(r\)</span>, Spearman’s <span class="math inline">\(r_s\)</span>, Kendall’s <span class="math inline">\(\tau\)</span></h2>
<p>-<strong>Pearson’s correlation coefficient</strong> is a measure of linear correlation between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. The formula when applied to a population is given by <span class="math display">\[\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X\sigma_Y}\]</span> The sample Pearson’s correlation coeffiecient <span class="math inline">\(r\)</span> can be calculated by plugging in estimates for the population parameters. <span class="math display">\[r = \frac{\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum_{i=1}^n(x_i-\overline{x})^2}\sqrt{\sum_{i=1}^n(y_i-\overline{y})^2}}\]</span> This formula aids the interpretation that the correlation is positive when both the <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> values lie above their their respective mean and negative when they lie on opposite side of their respective mean. The stronger this tendency is, the larger the magnitude of the correlation coefficient.</p>
<p>-<strong>Spearman’s rank correlation coefficient</strong> is defined as the Pearson correlation coefficient between the ranked variables and denoted <span class="math inline">\(r_s\)</span>. To compute it on a sample of size <span class="math inline">\(n\)</span>, first the raw scores <span class="math inline">\(X_i,Y_i\)</span> are transformed into their corresponding ranks in ascending order denoted <span class="math inline">\(rgX_i,rgY_i\)</span>. In a bivariate sample, each variable is ranked and the correlation is computed: <span class="math display">\[r_s = \rho_{rg_xrg_Y}\frac{cov(rg_X,rg_Y)}{\sigma_{rg_x}\sigma_{rg_Y}}\]</span> The advantage of <span class="math inline">\(r_s\)</span> is that it can assess non-linear monotonic relationships between two variables. It will have a value of 1 is one variable is a strictly monotonic function of the other. For example, the dataset <span class="math inline">\(\{(1,1),(2,4),(3,8),(4,16)\}\)</span> is exponentially related and has the property <span class="math inline">\(rgX_i = rgY_i\)</span>. Identical values are usually each assigned fractional ranks equal to the average of their positions in the ascending order of the values. However, if all the ranks are distinct integers, there is the formula <span class="math display">\[r_s =1-\frac{6\sum d_i^2}{n(n^2-1)}\]</span> where <span class="math inline">\(d_i = rg(X_i) - rg(Y_i)\)</span> is the difference in ranks of an observation. Note that an important advantage of the Spearman Correlation is that it is less sensitive to outliers because Spearman’s rho limits the outlier to the value of its rank.</p>
<p>-<strong>Kendall’s tau coefficient</strong></p>
<p>Let <span class="math inline">\((x_i,y_i)\)</span> and <span class="math inline">\((x_j,y_j)\)</span> be two distinct observations in a bivariate sample. Then the observations are <em>concordant</em> if the ranks for both elements agree: <span class="math inline">\(y_i&lt;y_j\)</span> or if <span class="math inline">\(x_i&gt;x_j\)</span> and <span class="math inline">\(y_i&gt;y_j\)</span>. The observations are <em>discordant</em> if the ranks disagree: <span class="math inline">\(x_i&lt;x_j\)</span> and <span class="math inline">\(y_i&gt;y_j\)</span> or if <span class="math inline">\(x_i&gt;x_j\)</span> and <span class="math inline">\(y_i&lt;y_j\)</span>. The pair of observations is neither if the ranks are equal. The Kendall <span class="math inline">\(\tau\)</span> coefficient is defined as <span class="math display">\[\tau = \frac{\text{(number of concordant pairs) - (number of discordant pairs)}}{n(n-1)/2}\]</span></p>
<p>Where the denominator is <span class="math inline">\(\binom{n}{2}\)</span>: the number of pairs of observations in the sample, so <span class="math inline">\(-1\leq \tau \leq 1\)</span>. An advantage of Kendall’s tau is that its interpretation in terms of the probabilities of observing the agreeable (concordant) and non-agreeable (discordant) pairs is very direct.Though in most of the situations, the interpretations of Kendall’s tau and Spearman’s rank correlation coefficient are very similar and thus invariably lead to the same inferences. Spearman’s rank correlation coefficient is the more widely used rank correlation coefficient. Kendall’s Tau may be less sensitive to error and give smaller values than Spearman’s rho correlation, but the required number of computations for its calcualation can make implementation inefficient.</p>
<p>References:</p>
<p><a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" class="uri">https://en.wikipedia.org/wiki/Pearson_correlation_coefficient</a> <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient</a> <a href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php" class="uri">https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php</a> <a href="https://en.wikipedia.org/wiki/Ranking#Ranking_in_statistics" class="uri">https://en.wikipedia.org/wiki/Ranking#Ranking_in_statistics</a> <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" class="uri">https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient</a> <a href="http://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/" class="uri">http://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/</a></p>
<hr />
</div>
<div id="comparing-r-r_s-and-tau" class="section level2">
<h2><span class="header-section-number">1.2</span> Comparing <span class="math inline">\(r\)</span>, <span class="math inline">\(r_s\)</span>, and <span class="math inline">\(\tau\)</span></h2>
<p><img src="hw1img.png"></p>
<pre class="r"><code>#organize observations into data frame
df &lt;- data.frame(
  x_1 = rep(c(0,1,10), times = 2),
  x_2 = rep(c(0,1), each = 3),
  p = c(.1, .4, .2, .1, .1, .1)
)


n = 10^5
df.rows = nrow(df)

#sample from the row indices of the data frame
samples &lt;- sample(1:df.rows, size = n, replace = TRUE, prob = df$p)

#create your sample by subsetting the data frame
my_sample &lt;- df[samples,1:2]

knitr::kable(table(my_sample), caption = &quot;Frequency table for sample from discrete bivariate&quot;)</code></pre>
<table>
<caption>Frequency table for sample from discrete bivariate</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">0</th>
<th align="right">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">9926</td>
<td align="right">9905</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">39950</td>
<td align="right">10086</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="right">20127</td>
<td align="right">10006</td>
</tr>
</tbody>
</table>
<pre class="r"><code>get_cor &lt;- function(my_method){
#function to get the 3 types of correlation coefficients
cor(my_sample$x_2,my_sample$x_1, method = my_method)
}

corrs &lt;- c(pearson = get_cor(&quot;pearson&quot;), spearman = get_cor(&quot;spearman&quot;), kendall = get_cor(&quot;kendall&quot;))

knitr::kable(data.frame(corrs))</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">corrs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pearson</td>
<td align="right">0.0242226</td>
</tr>
<tr class="even">
<td>spearman</td>
<td align="right">-0.0820445</td>
</tr>
<tr class="odd">
<td>kendall</td>
<td align="right">-0.0779843</td>
</tr>
</tbody>
</table>
<pre class="r"><code>sample_mean &lt;- function(x){
  weighted.mean(x, w = df$p)
}

df$rank_x1 &lt;- rep(c(1,2,3),2)
df$rank_x2 &lt;-  rep(c(1,2),each = 3)

sample_list &lt;- list(x1_mean = df$x_1, x2_mean = df$x_2, rank_x1_mean = rep(c(1,2,3),2), rank_x2_mean = rep(c(1,2),each = 3))

means &lt;- lapply(sample_list, sample_mean)

rank_df &lt;- df %&gt;% mutate(
  x1_star = x_1 - means[[1]],
  x2_star = x_2 - means[[2]],
  r1_star = rank_x1 - means[[3]],
  r2_star = rank_x2 - means[[4]],
  x_cov = x1_star * x2_star,
  r_cov = r1_star * r2_star
) %&gt;%
 select(x_1, x_2, x_cov, r_cov, p)</code></pre>
<hr />
</div>
<div id="r-is-positive-but-r_s-and-tau-are-negative" class="section level2">
<h2><span class="header-section-number">1.3</span> <span class="math inline">\(r\)</span> is positive but <span class="math inline">\(r_s\)</span> and <span class="math inline">\(\tau\)</span> are negative</h2>
<p><strong>Pearson’s correlation is positive because the outlier inflates the positive contribution to the covariance.</strong></p>
<p>To invstigate the covariance of the raw and the ranked data, observe this dataframe with the following transformed variables: x_cov = <span class="math inline">\((x_{i1}-\overline{x_1})(x_{i2}-\overline{x_2})\)</span> and r_cov = <span class="math inline">\((rgx_{i1}-\overline{rgx_1})(rgx_{i2}-\overline{rgx_2})\)</span></p>
<table>
<thead>
<tr class="header">
<th align="right">x_1</th>
<th align="right">x_2</th>
<th align="right">x_cov</th>
<th align="right">r_cov</th>
<th align="right">p</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1.05</td>
<td align="right">0.33</td>
<td align="right">0.1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0.75</td>
<td align="right">0.03</td>
<td align="right">0.4</td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">0</td>
<td align="right">-1.95</td>
<td align="right">-0.27</td>
<td align="right">0.2</td>
</tr>
<tr class="even">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-2.45</td>
<td align="right">-0.77</td>
<td align="right">0.1</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">-1.75</td>
<td align="right">-0.07</td>
<td align="right">0.1</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">1</td>
<td align="right">4.55</td>
<td align="right">0.63</td>
<td align="right">0.1</td>
</tr>
</tbody>
</table>
<p>Because the distribution of x_1 is skewed to the right, its mean is rather large. We can observe that the mode of the distribution x = (1,0), contributes positively to the covariance of the two variables. In the raw data, this contribution is rather high: an additional .75 for each (1,0) in our sample. Similarly the outlier (10,1) contributes 4.55 to the covariance, a large amount. However, the ranking the variables lowers the mean and reduces the effect of the outlier on the covariance. The mode (1,0) now has a negligble contribution of .03, and the contribution of largest magnitude isn’t (10,1), but instead (0,1) with a negative contribution of -.77. This helps explain the negative value of Spearman’s correlation. It is easy to see Kendall’s correlation is negative because of the overwhelming number of discordant pairs of observations.</p>
<hr />
</div>
<div id="construct-a-distribution-where-r_s-is-negative-but-tau-is-positive" class="section level2">
<h2><span class="header-section-number">1.4</span> Construct a distribution where <span class="math inline">\(r_s\)</span> is negative but <span class="math inline">\(\tau\)</span> is positive</h2>
<p>Notice a key difference between Kendall’s <span class="math inline">\(\tau\)</span> and Spearman’s <span class="math inline">\(\rho\)</span> is that the former is not sensitive to the distance between rank swaps, just their frequency. This can be illustrated using a discrete distribution of distinct integers that allows us to use the shortcut formula for <span class="math inline">\(r_s\)</span>.</p>
<pre class="r"><code>x1 &lt;- 1:80
x2 &lt;- c(80:70,11:69,10:1)
cor(x1, x2, method = &quot;spearman&quot;)</code></pre>
<pre><code>## [1] -0.1978434</code></pre>
<pre class="r"><code>cor(x1, x2, method = &quot;kendall&quot;)</code></pre>
<pre><code>## [1] 0.08291139</code></pre>
<pre class="r"><code>plot(x1,x2)</code></pre>
<p><img src="GLM_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We can see that Spearman’s correlation is negative because of the weight of <span class="math inline">\(\sum d_i^2 = (80-1)^2 + (79-2)^2 + ...\)</span> is very large. Kendall’s tau is not sensitive to this fact, and shows a positive correlation because of the many concordant pairs in the middle of the distribution.</p>
</div>
</div>
<div id="implementing-glms" class="section level1">
<h1><span class="header-section-number">2</span> Implementing GLM’s</h1>
<div id="specifying-the-glm-in-r" class="section level2">
<h2><span class="header-section-number">2.1</span> Specifying the GLM in R</h2>
<p>Models in <code>R</code> can be specified using a compact formula syntax <code>y ~ model</code> where <code>y</code> is the response variable we want to measure and <code>model</code> specifies the formula we fit to the data. Consider a regression model where we want to predict <code>y</code> from <span class="math inline">\(p\)</span> predictors: <span class="math inline">\(x_1, ..., x_p\)</span>. Different operators will specify how we include these predictors as terms in the model:</p>
<ul>
<li><code>+</code> <code>x_1 + x_2</code> indicates a linear combination of the variables</li>
<li><code>:</code> <code>x_1:x_2</code> includes the interaction term <span class="math inline">\(x_1x_2\)</span> in the model</li>
<li><code>*</code> <code>x_1 * x_2</code> denotes a factor crossing: <span class="math inline">\(x_1 + x_2 +x_1x_2\)</span></li>
<li><code>^</code> <code>(x_1 + x_2 + x_3)^2</code> expands to a formula including the main effects of <span class="math inline">\(x_1, x_2, x_3\)</span> and their second order interation terms</li>
<li><code>-</code> <code>-1</code> removes the intercept term</li>
<li><code>I</code> <code>I(x_1^2)</code> inhibits the conversion to <code>x_1*x_1</code> which reduces to <code>x_1</code> since formulas remove reduntant terms. Using <code>I</code> properly introduces the term <span class="math inline">\(x_1^2\)</span> to the formula</li>
</ul>
<p>To fit a general linear model in <code>R</code>, we use the <code>glm</code> function and supply the model formula to the <code>formula</code> argument. The family name such, as binomial, gaussian, or poisson, can be supplied to the family argument as a family object. The default link is used unless an alternative is specified within the family object. For example, <code>glm(formula, family=binomial(link=probit))</code> uses the probit link function.</p>
</div>
<div id="specifying-the-link-function" class="section level2">
<h2><span class="header-section-number">2.2</span> Specifying the Link Function</h2>
<p>Consider a dataset which includes two observations:</p>
<p><img src="table2.png"></p>
<pre class="r"><code>df &lt;- data.frame(Y = c(1,4),
                 X = 1:2) </code></pre>
<p>Let’s assume the linear model is <span class="math inline">\(\eta=\beta c\)</span> and the family is gaussian. We can estimate the coefficients <span class="math inline">\(\beta\)</span> using different link functions. Here we write a function that generates models from this data using different links. A table of the coefficients of each model is provided below.</p>
<pre class="r"><code>glm.mod &lt;- function(.link){
  glm(formula = Y ~ ., family = gaussian(link = .link), data = df)
}

links &lt;- c(&quot;identity&quot;,&quot;log&quot;,&quot;inverse&quot;)

model.list &lt;- map(links, glm.mod)

coef.list &lt;- map(model.list, coef) %&gt;% as.data.frame()

names(coef.list) &lt;- links

row.names(coef.list) &lt;- c(&quot;Intercept&quot;, &#39;X&#39;)


knitr::kable(coef.list, row.names = TRUE, caption = &quot;Coeffecients for each of 3 fitted models&quot;)</code></pre>
<table>
<caption>Coeffecients for each of 3 fitted models</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">identity</th>
<th align="right">log</th>
<th align="right">inverse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intercept</td>
<td align="right">-2</td>
<td align="right">-1.386294</td>
<td align="right">1.75</td>
</tr>
<tr class="even">
<td>X</td>
<td align="right">3</td>
<td align="right">1.386294</td>
<td align="right">-0.75</td>
</tr>
</tbody>
</table>
</div>
<div id="specifying-your-own-link-function" class="section level2">
<h2><span class="header-section-number">2.3</span> Specifying your own link function</h2>
<pre class="r"><code>explink &lt;- function()
{
    linkfun &lt;- function(mu) exp(mu)
    linkinv &lt;- function(eta) log(eta)
    mu.eta &lt;- function(eta) 1/eta
    valideta &lt;- function(eta) TRUE
    link &lt;- &quot;exponential&quot;
    structure(list(linkfun = linkfun, linkinv = linkinv,
                   mu.eta = mu.eta, valideta = valideta, name = link),
              class = &quot;link-glm&quot;)
}

explink.mod &lt;- glm(Y ~ X - 1, family=gaussian(link=explink()), data = df)
explink.mod</code></pre>
<pre><code>## 
## Call:  glm(formula = Y ~ X - 1, family = gaussian(link = explink()), 
##     data = df)
## 
## Coefficients:
##     X  
## 8.614  
## 
## Degrees of Freedom: 2 Total (i.e. Null);  1 Residual
## Null Deviance:       Inf 
## Residual Deviance: 2.661     AIC: 10.25</code></pre>
<pre class="r"><code>beta.mod &lt;- coef(explink.mod)</code></pre>
<p>Using this link our estimate of <span class="math inline">\(\beta\)</span> is 8.6143241.</p>
</div>
</div>
<div id="quasi-likelihood-and-the-generalized-linear-model" class="section level1">
<h1><span class="header-section-number">3</span> Quasi-likelihood and the Generalized Linear Model</h1>
<div id="implementing-quasi-likelihood-in-r" class="section level2">
<h2><span class="header-section-number">3.1</span> Implementing quasi-likelihood in R</h2>
<ul>
<li><code>quasi</code> is a function that generates a family object containing a list of functions and expressions that can be used by <code>glm</code> and <code>gam</code>. It accepts two main arguments <code>link</code> and <code>variance</code>, which default to “identity” and “constant” if not specified.</li>
</ul>
<ol style="list-style-type: decimal">
<li>The <code>quasi</code> family accepts the following links: <code>logit</code>, <code>probit</code>, <code>cloglog</code>, <code>identity</code>, <code>inverse</code>, <code>log</code>, <code>1/mu^2</code>, and <code>sqrt</code></li>
</ol>
<ul>
<li>The <code>power</code> function creates a link object based on the link function <span class="math inline">\(\eta = \mu^{\lambda}\)</span>, where the argument for <span class="math inline">\(\lambda\)</span> is a real number. If <span class="math inline">\(\lambda &lt; 0\)</span> it is taken to be <span class="math inline">\(0\)</span>, and the log link is obtained. The default <code>lambda = 1</code> gives the identity link.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>The variance argument accepts a character string that names the variance function to use with the <code>quasi</code> function. You can specify one of the following: <code>mu(1-mu)</code>, <code>mu</code>, <code>mu^2</code>, <code>mu^3</code>. Furthermore, the variance function is specific to the <code>quasi</code> function; other family functions have a fixed variance function and do not accept this parameter.</li>
</ol>
</div>
<div id="comparing-different-models" class="section level2">
<h2><span class="header-section-number">3.2</span> Comparing different models</h2>
<p><img src="hw3pic.png"></p>
<pre class="r"><code>my_data &lt;- data.frame(
  x = c(1, 2, 3, 5, 8),
  y = c(5, 12, 14, 35, 42)
)


model_constructor &lt;- function(family_object){
  glm(y ~ x, family = family_object, data = my_data)
}

family_objects &lt;- list(gaussian,
                       gaussian(link = &#39;log&#39;),
                       poisson,
                       poisson(link = &#39;log&#39;),
                       quasi,
                       quasi(link = &#39;log&#39;),
                       quasi(variance = &#39;mu&#39;),
                       quasi(link = &#39;log&#39;, variance = &#39;mu&#39;)
                       )

my_8_models &lt;- map(family_objects, model_constructor)

names(my_8_models) &lt;-  c(&quot;g&quot;, &#39;gl&#39;, &#39;p&#39;, &#39;pl&#39;, &#39;q&#39;, &#39;ql&#39;, &#39;qmu&#39;, &#39;qlmu&#39;)

summaries &lt;- map(my_8_models, summary)</code></pre>
<p>Let’s examine the output of the gaussian model using the <code>summary</code> function</p>
<pre class="r"><code>summaries[[1]]</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = family_object, data = my_data)
## 
## Deviance Residuals: 
##       1        2        3        4        5  
## -1.0000   0.4286  -3.1429   6.7143  -3.0000  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   0.4286     3.8109   0.112  0.91756   
## x             5.5714     0.8396   6.635  0.00697 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 21.71429)
## 
##     Null deviance: 1021.200  on 4  degrees of freedom
## Residual deviance:   65.143  on 3  degrees of freedom
## AIC: 33.025
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>The goodness-of-fit of a glm model is described by the Null deviance, Residual deviance, and AIC metrics, where low values indicate a better fit. Let’s compare the fit of our models.</p>
<pre class="r"><code>model_fits &lt;- map(my_8_models, extract, c(&quot;deviance&quot;, &quot;null.deviance&quot;, &quot;aic&quot;)) %&gt;% transpose() %&gt;% map(unlist) %&gt;% as_tibble() %&gt;% cbind(names(my_8_models)) %&gt;% rename(names =&quot;names(my_8_models)&quot;)
knitr::kable(model_fits)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">deviance</th>
<th align="right">null.deviance</th>
<th align="right">aic</th>
<th align="left">names</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">65.142857</td>
<td align="right">1021.2000</td>
<td align="right">33.02511</td>
<td align="left">g</td>
</tr>
<tr class="even">
<td align="right">161.186851</td>
<td align="right">1021.2000</td>
<td align="right">37.55502</td>
<td align="left">gl</td>
</tr>
<tr class="odd">
<td align="right">7.870696</td>
<td align="right">48.7626</td>
<td align="right">35.15431</td>
<td align="left">p</td>
</tr>
<tr class="even">
<td align="right">7.870696</td>
<td align="right">48.7626</td>
<td align="right">35.15431</td>
<td align="left">pl</td>
</tr>
<tr class="odd">
<td align="right">65.142857</td>
<td align="right">1021.2000</td>
<td align="right">NA</td>
<td align="left">q</td>
</tr>
<tr class="even">
<td align="right">161.186851</td>
<td align="right">1021.2000</td>
<td align="right">NA</td>
<td align="left">ql</td>
</tr>
<tr class="odd">
<td align="right">2.336588</td>
<td align="right">48.7626</td>
<td align="right">NA</td>
<td align="left">qmu</td>
</tr>
<tr class="even">
<td align="right">7.870696</td>
<td align="right">48.7626</td>
<td align="right">NA</td>
<td align="left">qlmu</td>
</tr>
</tbody>
</table>
<p>Let’s see which models performed best and worst under each metric:</p>
<pre class="r"><code>knitr::kable(model_fits %&gt;% select(deviance, names) %&gt;% arrange(deviance))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">deviance</th>
<th align="left">names</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2.336588</td>
<td align="left">qmu</td>
</tr>
<tr class="even">
<td align="right">7.870696</td>
<td align="left">p</td>
</tr>
<tr class="odd">
<td align="right">7.870696</td>
<td align="left">pl</td>
</tr>
<tr class="even">
<td align="right">7.870696</td>
<td align="left">qlmu</td>
</tr>
<tr class="odd">
<td align="right">65.142857</td>
<td align="left">g</td>
</tr>
<tr class="even">
<td align="right">65.142857</td>
<td align="left">q</td>
</tr>
<tr class="odd">
<td align="right">161.186851</td>
<td align="left">gl</td>
</tr>
<tr class="even">
<td align="right">161.186851</td>
<td align="left">ql</td>
</tr>
</tbody>
</table>
<p>The quasi-likelihood with variance proportional to mean has the lowest deviance, whereas the quasi-likelihood with log link has the highest.</p>
<pre class="r"><code>knitr::kable(model_fits %&gt;% select(null.deviance, names) %&gt;% arrange(null.deviance))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">null.deviance</th>
<th align="left">names</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">48.7626</td>
<td align="left">p</td>
</tr>
<tr class="even">
<td align="right">48.7626</td>
<td align="left">pl</td>
</tr>
<tr class="odd">
<td align="right">48.7626</td>
<td align="left">qmu</td>
</tr>
<tr class="even">
<td align="right">48.7626</td>
<td align="left">qlmu</td>
</tr>
<tr class="odd">
<td align="right">1021.2000</td>
<td align="left">g</td>
</tr>
<tr class="even">
<td align="right">1021.2000</td>
<td align="left">gl</td>
</tr>
<tr class="odd">
<td align="right">1021.2000</td>
<td align="left">q</td>
</tr>
<tr class="even">
<td align="right">1021.2000</td>
<td align="left">ql</td>
</tr>
</tbody>
</table>
<p>The null deviance is an even split, half the models performed well and the other half did not.</p>
<pre class="r"><code>knitr::kable(model_fits %&gt;% select(aic, names) %&gt;% arrange(aic))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">aic</th>
<th align="left">names</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">33.02511</td>
<td align="left">g</td>
</tr>
<tr class="even">
<td align="right">35.15431</td>
<td align="left">p</td>
</tr>
<tr class="odd">
<td align="right">35.15431</td>
<td align="left">pl</td>
</tr>
<tr class="even">
<td align="right">37.55502</td>
<td align="left">gl</td>
</tr>
<tr class="odd">
<td align="right">NA</td>
<td align="left">q</td>
</tr>
<tr class="even">
<td align="right">NA</td>
<td align="left">ql</td>
</tr>
<tr class="odd">
<td align="right">NA</td>
<td align="left">qmu</td>
</tr>
<tr class="even">
<td align="right">NA</td>
<td align="left">qlmu</td>
</tr>
</tbody>
</table>
<p>It looks like the standard gaussian with identity link has the lowest aic, a metric which is not avaialable for quasi likelihoods.</p>
</div>
<div id="wedderburns-quasi-likelihood-and-nelders-extension" class="section level2">
<h2><span class="header-section-number">3.3</span> Wedderburn’s Quasi likelihood and Nelder’s extension</h2>
<p>Quasi-likelihood estimation is one way of allowing for overdispersion, that is, greater variability in the data than would be expected from the statistical model used. It is most often used with models for count data or grouped binary data, i.e. data that would otherwise be modelled using the Poisson or binomial distribution. Instead of specifying a probability distribution for the data, only a relationship between the mean and the variance is specified in the form of a variance function giving the variance as a function of the mean.Wedderburn relaxes the assumption of a known variance function of <span class="math inline">\(y\)</span> by allowing an unknown constant of proportionality <span class="math inline">\(\phi\)</span> so that <span class="math inline">\(var(y) = \phi V(\mu)\)</span>. This <span class="math inline">\(\phi\)</span> is known as the <em>dispersion</em> parameter and does not alter the estimation of the regression coefficients <span class="math inline">\(\beta\)</span>. Wedderburn defines this quasi-likelihood function for an observation <span class="math inline">\(y\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(V(\mu)\)</span> as <span class="math display">\[Q(y; \mu) = \int^{\mu}\frac{y-\mu}{V(\mu)}d\mu\]</span> The likelihood of the sample of <span class="math inline">\(N\)</span> observations is <span class="math inline">\(\sum_{i=1}^NQ(y_i,\mu_i)\)</span>.</p>
<p>Nelder defines an extended likelihood function <span class="math display">\[Q^+(y; \mu) = -\frac{1}{2}log\{2\pi\phi V(y)\} - \frac{D(y;\mu)}{\phi}\]</span> where <span class="math inline">\(D\)</span> is the deviance function, and <span class="math inline">\(\phi\)</span> is the nuisance parameter. The estimates of <span class="math inline">\(\beta\)</span> obtainted by maximizing <span class="math inline">\(Q^+\)</span> are the same as those from maximizing <span class="math inline">\(Q\)</span> and the estimate of <span class="math inline">\(\phi\)</span> obtained from maximizing <span class="math inline">\(Q^+\)</span> is <span class="math inline">\(\hat{\phi} = D(y;\hat{\mu})\)</span>.</p>
<p>References:</p>
<ul>
<li><em>An Extended Quasi-Likelihood Function</em> Nelder, Pregibon (1987)</li>
<li><a href="https://en.wikipedia.org/wiki/Quasi-likelihood">Wikipedia: Quasi-Likelihood</a></li>
<li><a href="https://www.rdocumentation.org/packages/stats/versions/3.4.1/topics/power">R documentation: power function</a></li>
<li><a href="http://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html">R documentation: Family Objects for Models</a></li>
</ul>
</div>
</div>
<div id="applications-of-glms" class="section level1">
<h1><span class="header-section-number">4</span> Applications of GLMs</h1>
<div id="predicting-redshift-with-gamma-glm" class="section level2">
<h2><span class="header-section-number">4.1</span> Predicting Redshift with gamma GLM</h2>
<p><em>Overview of regression models</em></p>
<p>Suppose we take a physical measurement on a sample of galaxies to obtain the following training date: <span class="math display">\[\mathcal{D} = \{(x_1, y_1), \dots, (x_N, y_N)\}\]</span> where the <span class="math inline">\(x_i\)</span> represent the independent variable, the galaxy, and the the <span class="math inline">\(y_i\)</span> represents the response variable, e.g. a spectroscopic measurement. We can think of each measurement <span class="math inline">\(\{x_i, y_i\}\)</span> as a realization of a distinct pair of random variables: <span class="math inline">\(\{X_i, Y_i\}\)</span>, from a family of similar PDFs. We can use a generalized linear model to predict the expected value of the redshift <span class="math inline">\(\{Y_i\}\)</span> given that we know its galaxy <span class="math inline">\(\{X_i = x_i\}\)</span>. If the response variable is normally distributed conditioned on <span class="math inline">\(X\)</span>, we can assume theat the <span class="math inline">\(Y_i\)</span>’s have the following PDF: <span class="math display">\[f(y_i; \mu_i, \sigma_i) =  \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp\left[-\frac{1}{2}\frac{(y_i-\mu_i)^2}{\sigma_i^2}\right]\]</span></p>
<p>This is a special case of a GLM where the link function is identity: <span class="math display">\[g(E(Y_i)) = \mu_i = x_i^T\beta\]</span> and the inverse link is also the identity so <span class="math display">\[g^{-1}(x_i^T\beta) = E(Y_i)\]</span> If we can extend the general linear model to when the response variable is not normally distributed conitioned on the explanatory vaiable. In this case, we can use a generalized linear model for any random variable from the exponential family, which have the following PDF: <span class="math display">\[f(y; \theta, \phi) = exp\left\{\frac{y\theta - A(\theta)}{B(\phi)} + C(y; \phi)\right\}\]</span></p>
<p>The normal and gamma distributions are example of the exponential family. When rewritten in canonical form, the gamma distribution looks like: <span class="math display">\[f(y; \mu, \phi) = \exp\left\{\frac{y/\mu - (-\ln\mu)}{-\phi} + C(y;\phi)\right\}\]</span></p>
<p>In this case, you do not have to use the canonical, but can also use the log link: <span class="math inline">\(\mu^T = \log(\beta^TX)\)</span>. To avoid the problem of <em>multicollinearity</em>, we can use <em>Principle Component Analysis</em> to reduce the dimensions into uncorrelated variables. PCA allows for a more robust way to predict redshift.</p>
<p><em>Estimating Photometric Redshift in R</em></p>
<p>First load the CosmosPhotoz package and load the training and test data</p>
<pre class="r"><code>library(CosmoPhotoz)
data(PHAT0train)# Data for t raining
data(PHAT0test)# Data for estimation</code></pre>
<p>Then run the computeCombPCA functions which calculates principal components from the training and test set. It returns the PCA data to be stored in the object PC_comb</p>
<pre class="r"><code># Combine the t raining and t e s t data and
# c a l cul a te the pr inc ipal components
PC_comb &lt;- computeCombPCA( subset(PHAT0train,select=c(-redshift)),
                           subset(PHAT0test, select=c(-redshift)),
                           robust=TRUE)</code></pre>
<p>Create the training data by combining the response variable from the original training set with the PCA explanatory variables. The Testpc variable contains the response variable from the PCA set.</p>
<pre class="r"><code>Trainpc &lt;- cbind(PC_comb$x, redshift=PHAT0train$redshift)

Testpc &lt;- PC_comb$y</code></pre>
<p>Create a formula object that predicts the redshift variable using a polynomial formula of the first 6 components of PCA.</p>
<pre class="r"><code> # Formula based on the PCs
 formM &lt;- redshift~poly(Comp.1 ,2)*
poly(Comp.2 ,2)*Comp.3*Comp.4*
Comp.5*Comp.6</code></pre>
<p>Fit a Gamma regression to the Trainpc data using the previously defined formula.</p>
<pre class="r"><code># GLM f i t t i n g
Fit &lt;- glmTrainPhotoZ(Trainpc, formula=formM, method=&quot;Bayesian&quot; , family=&quot;gamma&quot;)</code></pre>
<p>Now make predictions on the test data using the glmfit attribute of the Fit object containing the gamma regression. Then print the predictions: this is long; so I will include only the head.</p>
<pre class="r"><code># Photo−z estimation
photoZtest &lt;- glmPredictPhotoZ(data=Testpc, train=Fit$glmfit)

# Pr int Photo−z estimation
head(photoZtest$photoz)</code></pre>
<pre><code>##            1            3            4            5            6 
## 5.526921e+04 2.935661e-02 9.399746e-01 2.839470e-02 5.345369e-01 
##            7 
## 1.637762e-02</code></pre>
<p>Then we can make predictions on the Testpc data and include the se.fit which gives the standard error of the predictions.</p>
<pre class="r"><code># Estimate confidence int e r v a l s er ror s
photoz_temp &lt;- predict(Fit$glmfit, newdata=Testpc, type=&quot;link&quot; , se.fit = TRUE)

photoz &lt;- photoz_temp$fit</code></pre>
<p>We can specify the lower and upper bounds of a 95% confidence intervals manually by setting a critical value of 1.96 and multiplying it to the standard error. Then we add and subtract them to the fitted values.</p>
<pre class="r"><code>critval &lt;- 1.96 ## approx 95% Confidence Int e rva l
upr &lt;- photoz_temp$fit + (critval * photoz_temp$se.fit)
lwr &lt;- photoz_temp$fit - (critval * photoz_temp$se.fit)
fit &lt;- photoz_temp$fit

fit2 &lt;- Fit$glmfit$family$linkinv(fit)
upr2 &lt;- Fit$glmfit$family$linkinv(upr)#upper l imi t
lwr2 &lt;- Fit$glmfit$family$linkinv(lwr)#lower l imi t</code></pre>
<p>The upper limit of the confidence interval for the first few predictions:</p>
<pre class="r"><code>head(upr2)# upper l imi t</code></pre>
<pre><code>##            1            3            4            5            6 
## 1.154015e+09 4.362539e-02 1.522563e+01 4.440402e-02 4.993972e+00 
##            7 
## 2.509132e-02</code></pre>
<p>The lower limit of the confidence interval for the first few predictions:</p>
<pre class="r"><code>head(lwr2)# lower l imi t</code></pre>
<pre><code>##          1          3          4          5          6          7 
## 2.64700721 0.01975479 0.05803058 0.01815735 0.05721493 0.01069001</code></pre>
<p>plotDiagPhotoZ returns diagnostic plots from the results of photometric redshifts</p>
<pre class="r"><code># Create a boxplot showing the r e sul t s
plotDiagPhotoZ(photoz = photoZtest$photoz, specz = PHAT0test$redshift, type = &quot;box&quot;)</code></pre>
<pre><code>## Warning: Removed 9 rows containing non-finite values (stat_boxplot).</code></pre>
<p><img src="GLM_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>reference:</p>
<p>J. Elliott, R.S. de Souza, A. Krone-Martins, E. Cameron, E.E.O. Ishida, J. Hilbe (2015). <a href="https://doi.org/10.1016/j.ascom.2015.01.002">The overlooked potential of generalized linear models in astronomy, II: gamma regression and photometric redshifts</a>. Astronomy and Computing, 10, 61-72.</p>
</div>
<div id="all-time-world-ranking-data" class="section level2">
<h2><span class="header-section-number">4.2</span> All time world ranking data</h2>
<p>Consider the data from All Time World Rankings. We use <a href="http://www.mastersathletics.net/fileadmin/html/Rankings/All_Time/100metresmen.htm">man’s 100 meter dash records</a>, and <a href="http://www.mastersathletics.net/fileadmin/html/Rankings/All_Time/100metreswomen.htm">woman’s 100 meter dash records</a></p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<pre class="r"><code>runnerData &lt;- read.csv(&quot;runnerData - Sheet1.csv&quot;)</code></pre>
<pre class="r"><code>runnerData %&gt;% head()</code></pre>
<pre><code>##   Gender Age  Time
## 1      M  35  9.97
## 2      M  40 10.29
## 3      M  45 10.72
## 4      M  50 10.88
## 5      M  55 11.39
## 6      M  60 11.70</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<pre class="r"><code>fit1 = lm(Time ~ Age, data = subset(runnerData, Gender == &quot;M&quot;))

fit2 = lm(Time ~ poly(Age, 2), data = subset(runnerData, Gender == &quot;M&quot;))</code></pre>
<pre class="r"><code>summary(fit1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Time ~ Age, data = subset(runnerData, Gender == 
##     &quot;M&quot;))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9355 -0.6100 -0.2255  0.5460  1.4746 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.78776    0.84503   5.666 0.000208 ***
## Age          0.12520    0.01303   9.606 2.29e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7792 on 10 degrees of freedom
## Multiple R-squared:  0.9022, Adjusted R-squared:  0.8925 
## F-statistic: 92.28 on 1 and 10 DF,  p-value: 2.294e-06</code></pre>
<pre class="r"><code>summary(fit2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Time ~ poly(Age, 2), data = subset(runnerData, Gender == 
##     &quot;M&quot;))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.47423 -0.23887  0.05148  0.20903  0.33025 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   12.61250    0.08985 140.372 2.40e-16 ***
## poly(Age, 2)1  7.48562    0.31125  24.050 1.78e-09 ***
## poly(Age, 2)2  2.28040    0.31125   7.327 4.44e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3113 on 9 degrees of freedom
## Multiple R-squared:  0.986,  Adjusted R-squared:  0.9828 
## F-statistic:   316 on 2 and 9 DF,  p-value: 4.602e-09</code></pre>
<p>Analyzing the summary of the formula calls, we can see that the quadtratic term is <em>highly significant</em>. Not only does it pass the t-test, but it increases the <span class="math inline">\(R^2\)</span> from <span class="math inline">\(.9\)</span> to <span class="math inline">\(.98\)</span>: a near pearfect fit.</p>
<pre class="r"><code>ggplot(data = runnerData, aes(x = Age, y = Time, color = Gender)) +
  geom_point() +
  stat_smooth(method = &#39;loess&#39;, se = F)</code></pre>
<p><img src="GLM_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>A quadratic fit is clearly more appropriate.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<pre class="r"><code>library(tidyverse)</code></pre>
<p>Create dummy variable for gender column.</p>
<pre class="r"><code>runnerData %&gt;% transmute(Gender = ifelse(Gender == &quot;W&quot;, 1, 0),
                         Age = Age,
                         Time = Time) -&gt; runnerData1</code></pre>
<p>Confirm dummy variable has been created successfully:</p>
<pre class="r"><code>runnerData1 %&gt;% sample_n(size = 6)</code></pre>
<pre><code>##    Gender Age  Time
## 11      0  85 16.13
## 3       0  45 10.72
## 1       0  35  9.97
## 23      1  85 19.83
## 9       0  75 13.49
## 17      1  55 13.30</code></pre>
<p>Refit lm</p>
<pre class="r"><code>fit3 &lt;- lm(Time ~ Gender + Age, data = runnerData1)

summary(fit3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Time ~ Gender + Age, data = runnerData1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.3850 -1.0230 -0.1954  0.4399  3.9327 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.41779    1.02474   2.359 0.028064 *  
## Gender       2.14917    0.52721   4.076 0.000541 ***
## Age          0.16312    0.01527  10.680 6.04e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.291 on 21 degrees of freedom
## Multiple R-squared:  0.8616, Adjusted R-squared:  0.8484 
## F-statistic: 65.34 on 2 and 21 DF,  p-value: 9.623e-10</code></pre>
<p>Compare this to fitting the model without using the gender variable:</p>
<pre class="r"><code>fit4 &lt;- lm(Time ~ Age, data = runnerData1)

summary(fit4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Time ~ Age, data = runnerData1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.2360 -1.1121  0.0135  0.7855  5.0072 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.49237    1.29489   2.697   0.0132 *  
## Age          0.16312    0.01997   8.168 4.18e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.689 on 22 degrees of freedom
## Multiple R-squared:  0.752,  Adjusted R-squared:  0.7407 
## F-statistic: 66.71 on 1 and 22 DF,  p-value: 4.176e-08</code></pre>
<p>Adding the gender variable raises the <span class="math inline">\(R^2\)</span> from <span class="math inline">\(.75\)</span> to <span class="math inline">\(.86\)</span>. It also passes the t-test and is highly significant. It makes sense that the gender of the runner helps predict their score; that is why men and women run in separate categories in the first place.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li></li>
</ol>
<pre class="r"><code>beta_30 &lt;- fit3$coefficients[1]
b_0_M &lt;- fit1$coefficients[1]
beta_32 &lt;- fit3$coefficients[2]</code></pre>
<p>Comparing the intercepts, we have <span class="math inline">\(\hat{\beta}_{30}\)</span> = 2.4177885 and b_0_M = 4.7877622. We see that <span class="math inline">\(\hat{\beta}_{30}\)</span> is nearly half the value. The coefficient of the gender variable <span class="math inline">\(\beta_{32}\)</span> is 2.1491667: it takes half the value of the intercept.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li></li>
</ol>
<pre class="r"><code>fit.gamma &lt;- glm(Time ~ Age, data = subset(runnerData, Gender == &quot;M&quot;), family = Gamma(link = &#39;inverse&#39;))</code></pre>
<p>I have used the inverse link, which is in fact the default link for gamma. I also tested the log link because of upward curve in the data, but it performed much worse.</p>
<pre class="r"><code>summary(fit.gamma)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Time ~ Age, family = Gamma(link = &quot;inverse&quot;), data = subset(runnerData, 
##     Gender == &quot;M&quot;))
## 
## Deviance Residuals: 
##       Min         1Q     Median         3Q        Max  
## -0.045707  -0.024232  -0.003373   0.024836   0.048323  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.311e-01  3.009e-03   43.58 9.73e-13 ***
## Age         -7.921e-04  4.273e-05  -18.54 4.50e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Gamma family taken to be 0.001018569)
## 
##     Null deviance: 0.363554  on 11  degrees of freedom
## Residual deviance: 0.010172  on 10  degrees of freedom
## AIC: 15.649
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>Now I will refit the same model from part (b), but call it from the glm function, so the metrics are the same.</p>
<pre class="r"><code>glm(Time ~ Age, data = subset(runnerData, Gender == &quot;M&quot;), family = gaussian(link = &quot;identity&quot;))</code></pre>
<pre><code>## 
## Call:  glm(formula = Time ~ Age, family = gaussian(link = &quot;identity&quot;), 
##     data = subset(runnerData, Gender == &quot;M&quot;))
## 
## Coefficients:
## (Intercept)          Age  
##      4.7878       0.1252  
## 
## Degrees of Freedom: 11 Total (i.e. Null);  10 Residual
## Null Deviance:       62.11 
## Residual Deviance: 6.072     AIC: 31.88</code></pre>
<p>Deviance is a measure of goodness-of-fit, and lower deviance indicates a better fit. The null deviance considers the deviance of the null model (just an intercept), and the residual deviance considers the models with the full parameters. We can see that the deviance of the gamma fit is substantially lower than that of the gaussian. Furthermore, the residual deviance of the gamma is about 36 times lower than its null deviance, whereas the the gaussian residual deviance is only 6 times lower. This indicates that the gamma model explain the variation better. Last, the AIC of the gamma model is half that of the gaussian model, indicating that it is a substantially better fit.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>Show that the inverse Gaussian distribution is in the exponential family:</p>
<p>pdf: <span class="math display">\[f(x) = \left(\frac{\lambda}{2\pi x^3}\right)^{\frac{1}{2}}\exp\left\{-\frac{\lambda (x-\mu)^2}{2\mu^2 x}\right\}\]</span></p>
<p>take logs and simplify:</p>
<p><span class="math display">\[\log f(x) = \frac{1}{2}\log\lambda - \frac{1}{2}\log(2\pi x^3) - \frac{x\lambda}{2\mu^2} + \frac{\lambda}{\mu}
 - \frac{\lambda}{2x}\]</span></p>
<p>exponentiate:</p>
<p><span class="math display">\[f(x) = \exp\left\{-\frac{\lambda}{2\mu^2}x + \frac{\lambda}{\mu}+  \frac{-\lambda + x\log(\lambda) - x\log(2\pi x^3)}{2x} \right\}\]</span></p>
<p>compare to</p>
<p><span class="math display">\[f(x) = \exp\left\{a(x)b(\mu) + c(\mu) + d(x)\right\}\]</span> consider this as a 1-parameter family, so <span class="math inline">\(\lambda\)</span> is a nuisance parameter absorbed in the other functions. Then <span class="math display">\[a(x) = x\]</span> So the pdf is in canonical form and <span class="math display">\[b(\mu) = -\frac{\lambda}{2\mu^2}\]</span> is the natural parameter. Further more <span class="math display">\[c(\mu) =  \frac{\lambda}{\mu}\]</span> and <span class="math display">\[d(x) =  \frac{-\lambda + x\log(\lambda) - x\log(2\pi x^3)}{2x}\]</span>.</p>
<pre class="r"><code>glm(Time ~ Age, data = subset(runnerData, Gender == &quot;M&quot;), family = gaussian(link = &quot;inverse&quot;))</code></pre>
<pre><code>## 
## Call:  glm(formula = Time ~ Age, family = gaussian(link = &quot;inverse&quot;), 
##     data = subset(runnerData, Gender == &quot;M&quot;))
## 
## Coefficients:
## (Intercept)          Age  
##   0.1340362   -0.0008335  
## 
## Degrees of Freedom: 11 Total (i.e. Null);  10 Residual
## Null Deviance:       62.11 
## Residual Deviance: 1.792     AIC: 17.24</code></pre>
<p>This is a much better fit than using the identity link. The AIC is almost as low as that of the inverse gamma model. Equation: <span class="math display">\[\mu = \frac{1}{.1340362 - .0008335x}\]</span></p>
</div>
<div id="weighted-items" class="section level2">
<h2><span class="header-section-number">4.3</span> Weighted items</h2>
<p><img src="q1mid.png"></p>
<p>Set up the model matrix:</p>
<p><span class="math display">\[\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp;1 \end{bmatrix} \begin{bmatrix} \alpha_A \\ \alpha_B \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p>Use the normal equations to solve the parameters:</p>
<p><span class="math display">\[ \begin{bmatrix} \alpha_A \\ \alpha_B \end{bmatrix} =\left(\begin{bmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\end{bmatrix}\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp;1 \end{bmatrix} \right)^{-1}\begin{bmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[  =\left(\begin{bmatrix}2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix} \right)^{-1}\begin{bmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[  =\begin{bmatrix}\dfrac{2}{3} &amp; -\dfrac{1}{3} \\ -\dfrac{1}{3} &amp; \dfrac{2}{3} \end{bmatrix} \begin{bmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span> <span class="math display">\[  = \begin{bmatrix}\dfrac{2}{3}&amp;-\dfrac{1}{3}&amp;\dfrac{1}{3}\\-\dfrac{1}{3}&amp;\dfrac{2}{3}&amp;\dfrac{1}{3}\end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[\alpha_A = \frac{2y_1 - y_2 + y_3}{3}\]</span> <span class="math display">\[\alpha_B = \frac{2y_2 - y_1 + y_3}{3}\]</span></p>
<p><img src="q2mid.png"></p>
<p><span class="math display">\[ \begin{bmatrix} \alpha_A \\ \alpha_B \end{bmatrix} =\left(\begin{bmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\end{bmatrix}\begin{bmatrix}\frac{1}{\sigma^2_{\epsilon}}&amp;0&amp;0\\0&amp;\frac{1}{\sigma^2_{\epsilon}}&amp;0\\0&amp;0&amp;\frac{1}{k^2\sigma^2_{\epsilon}}\end{bmatrix}\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp;1 \end{bmatrix} \right)^{-1}\begin{bmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\end{bmatrix}\begin{bmatrix}\frac{1}{\sigma^2_{\epsilon}}&amp;0&amp;0\\0&amp;\frac{1}{\sigma^2_{\epsilon}}&amp;0\\0&amp;0&amp;\frac{1}{k^2\sigma^2_{\epsilon}}\end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[ =\left(\begin{bmatrix} \frac{1}{\sigma^2_{\epsilon}} &amp; 0 &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \\ 0 &amp; \frac{1}{\sigma^2_{\epsilon}} &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp;1 \end{bmatrix} \right)^{-1}\begin{bmatrix} \frac{1}{\sigma^2_{\epsilon}} &amp; 0 &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \\ 0 &amp; \frac{1}{\sigma^2_{\epsilon}} &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[ =\left(\begin{bmatrix}  \frac{k^2+1}{k^2\sigma^2_{\epsilon}}&amp; \frac{1}{k^2\sigma^2_{\epsilon}}\\ \frac{1}{k^2\sigma^2_{\epsilon}}&amp; \frac{k^2+1}{k^2\sigma^2_{\epsilon}}\end{bmatrix}\right)^{-1}\begin{bmatrix} \frac{1}{\sigma^2_{\epsilon}} &amp; 0 &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \\ 0 &amp; \frac{1}{\sigma^2_{\epsilon}} &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[ =\frac{k^2\sigma^4_{\epsilon}}{k^2+2}\begin{bmatrix}  \frac{k^2+1}{k^2\sigma^2_{\epsilon}}&amp; -\frac{1}{k^2\sigma^2_{\epsilon}}\\ -\frac{1}{k^2\sigma^2_{\epsilon}}&amp; \frac{k^2+1}{k^2\sigma^2_{\epsilon}}\end{bmatrix}\begin{bmatrix} \frac{1}{\sigma^2_{\epsilon}} &amp; 0 &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \\ 0 &amp; \frac{1}{\sigma^2_{\epsilon}} &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span></p>
<p><span class="math display">\[ =\begin{bmatrix}  \sigma^2_{\epsilon}\frac{k^2+1}{k^2+2}&amp; -\sigma^2_{\epsilon}\frac{1}{k^2+2}\\  -\sigma^2_{\epsilon}\frac{1}{k^2+2}&amp;  \sigma^2_{\epsilon}\frac{k^2+1}{k^2+2}\end{bmatrix}\begin{bmatrix} \frac{1}{\sigma^2_{\epsilon}} &amp; 0 &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \\ 0 &amp; \frac{1}{\sigma^2_{\epsilon}} &amp; \frac{1}{k^2\sigma^2_{\epsilon}} \end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span> <span class="math display">\[=\begin{bmatrix}\frac{k^2+1}{k^2+2} &amp; -\frac{1}{k^2+2} &amp; \frac{1}{k^2+2} \\ -\frac{1}{k^2+2} &amp; \frac{k^2+1}{k^2+2} &amp; \frac{1}{k^2+2}\end{bmatrix}\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}\]</span> <span class="math display">\[ \alpha_A = \frac{(k^2+1)y_1 - y_2 + y_3}{k^2+2}\]</span> <span class="math display">\[\alpha_B = \frac{(k^2+1)y_2 - y_1 + y_3}{k^2+2}\]</span></p>
<p><img src="q3mid.png"></p>
<pre class="r"><code>dframe &lt;- data.frame(x1 = c(1, 0, 1),
                     x2 = c(0, 1, 1),
                     y = c(41, 53, 97))

fit.a = lm(y ~ x1 + x2 -1, data = dframe)

summary(fit.a)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2 - 1, data = dframe)
## 
## Residuals:
##  1  2  3 
## -1 -1  1 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)  
## x1   42.000      1.414   29.70   0.0214 *
## x2   54.000      1.414   38.18   0.0167 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.732 on 1 degrees of freedom
## Multiple R-squared:  0.9998, Adjusted R-squared:  0.9994 
## F-statistic:  2316 on 2 and 1 DF,  p-value: 0.01469</code></pre>
<p>compare to our derived formulas:</p>
<p><span class="math display">\[\alpha_A = \frac{2(41) - 53 + 97}{3} = 42\]</span> <span class="math display">\[\alpha_B = \frac{2(53) - 41 + 97}{3} = 54\]</span></p>
<p>the estimates match exactly.</p>
<pre class="r"><code>fit.b = lm(y ~  x1 + x2 -1, data = dframe, weights = c(1, 1, .694))

summary(fit.b)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2 - 1, data = dframe, weights = c(1, 1, 
##     0.694))
## 
## Weighted Residuals:
##       1       2       3 
## -0.8719 -0.8719  1.0466 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)  
## x1   41.872      1.362   30.74   0.0207 *
## x2   53.872      1.362   39.55   0.0161 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.617 on 1 degrees of freedom
## Multiple R-squared:  0.9998, Adjusted R-squared:  0.9993 
## F-statistic:  2106 on 2 and 1 DF,  p-value: 0.01541</code></pre>
<p>compare to our derived formulas:</p>
<p><span class="math display">\[\alpha_A = \frac{(1.2^2+1)41 - 53 + 97}{1.2^2+2} = 41.872  \]</span></p>
<p><span class="math display">\[\alpha_A = \frac{(1.2^2+1)53 - 41 + 97}{1.2^2+2} = 53.872  \]</span></p>
<p>The results match.</p>
<p>Note that the effect of adding the weight in part (b) decreaed the estimates on both parameters by a small amount. From the equations we can see that it is because the weight added to the denominator is greater than the contribution to the numerator.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
