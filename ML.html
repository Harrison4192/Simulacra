<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Harrison Tietze" />


<title>Statistical Learning</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="faded.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Harrison's website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About Me</a>
</li>
<li>
  <a href="simulations.html">Sims</a>
</li>
<li>
  <a href="ML.html">ML</a>
</li>
<li>
  <a href="GLM.html">GLM</a>
</li>
<li>
  <a href="certificates.html">certificates</a>
</li>
<li>
  <a href="topology.html">Baire Space</a>
</li>
<li>
  <a href="KDE.html">KDE</a>
</li>
<li>
  <a href="https://docs.google.com/document/d/e/2PACX-1vTnpiuuxd0-ia3SMAk1SGusAyLyvwPN-wsl_dQCIC6Nglj0WgyTne0nYS1JISZZI6H-ym631b9B0kr0/pub">Novella</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Learning</h1>
<h4 class="author"><em>Harrison Tietze</em></h4>
<h4 class="date"><em>Fall 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#regression-models"><span class="toc-section-number">1</span> Regression Models</a><ul>
<li><a href="#glmnet"><span class="toc-section-number">1.1</span> glmnet</a></li>
<li><a href="#ridge-regression"><span class="toc-section-number">1.2</span> Ridge Regression</a></li>
<li><a href="#the-lasso"><span class="toc-section-number">1.3</span> The Lasso</a></li>
<li><a href="#principle-components-regression"><span class="toc-section-number">1.4</span> Principle Components Regression</a></li>
<li><a href="#partial-least-squares"><span class="toc-section-number">1.5</span> Partial Least Squares</a></li>
</ul></li>
<li><a href="#simplifying-with-caret"><span class="toc-section-number">2</span> Simplifying with Caret</a></li>
<li><a href="#subset-selection"><span class="toc-section-number">3</span> Subset selection</a><ul>
<li><a href="#best-subset-selection"><span class="toc-section-number">3.1</span> Best Subset Selection</a></li>
<li><a href="#estimates-of-test-error"><span class="toc-section-number">3.2</span> Estimates of Test error</a></li>
<li><a href="#forward-selection"><span class="toc-section-number">3.3</span> Forward Selection</a></li>
</ul></li>
<li><a href="#decision-trees"><span class="toc-section-number">4</span> Decision Trees</a><ul>
<li><a href="#fitting-classification-trees"><span class="toc-section-number">4.1</span> Fitting Classification Trees</a></li>
<li><a href="#cost-complexity-pruning"><span class="toc-section-number">4.2</span> Cost-complexity pruning</a></li>
<li><a href="#fitting-regression-trees"><span class="toc-section-number">4.3</span> Fitting Regression Trees</a></li>
<li><a href="#bagging"><span class="toc-section-number">4.4</span> Bagging</a></li>
<li><a href="#random-forests"><span class="toc-section-number">4.5</span> Random Forests</a></li>
<li><a href="#boosting"><span class="toc-section-number">4.6</span> Boosting</a></li>
</ul></li>
<li><a href="#support-vector-machines"><span class="toc-section-number">5</span> Support Vector Machines</a></li>
</ul>
</div>

<p>reference: <a href="https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_1?ie=UTF8&amp;qid=1510705551&amp;sr=8-1&amp;keywords=introduction+to+statistical+learning">Introduction to Statistical learning</a> (2013) <em>Tibshirani, Hastie</em></p>
<div id="regression-models" class="section level1">
<h1><span class="header-section-number">1</span> Regression Models</h1>
<div id="glmnet" class="section level2">
<h2><span class="header-section-number">1.1</span> glmnet</h2>
<p>In this analysis we will work with the “College” dataset, which contains 777 observations with 18 variables. The goals is to predict how many applications a college will recieve “Apps” based off other predictors.</p>
<p>First examine the structure of the dataset:</p>
<pre class="r"><code>str(College)</code></pre>
<pre><code>## &#39;data.frame&#39;:    777 obs. of  18 variables:
##  $ Private    : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Apps       : num  1660 2186 1428 417 193 ...
##  $ Accept     : num  1232 1924 1097 349 146 ...
##  $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...
##  $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...
##  $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...
##  $ F.Undergrad: num  2885 2683 1036 510 249 ...
##  $ P.Undergrad: num  537 1227 99 63 869 ...
##  $ Outstate   : num  7440 12280 11250 12960 7560 ...
##  $ Room.Board : num  3300 6450 3750 5450 4120 ...
##  $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...
##  $ Personal   : num  2200 1500 1165 875 1500 ...
##  $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...
##  $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...
##  $ S.F.Ratio  : num  18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...
##  $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...
##  $ Expend     : num  7041 10527 8735 19016 10922 ...
##  $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...</code></pre>
<p><img src="collegedata.png"></p>
<p>The variables are numeric, except for the factor: Private. Let’s build a model matrix that automatically creates dummy variables, which is important because glmnet only accepts numeric variables.</p>
<pre class="r"><code>X = model.matrix(Apps ~ . - 1, data = College)[, -1]
Y = model.matrix(~ Apps + 0, data = College)

data_df &lt;- as.data.frame(cbind(X, Y))</code></pre>
<p>Since we will be testing the performance of multiple models, we need to split the data into a test and training set.</p>
<pre class="r"><code>set.seed(1)
train &lt;- sample(1:nrow(X), replace = FALSE, round(nrow(X) * .25))
X.train &lt;- X[train, ]
Y.train &lt;- Y[train]

X.test &lt;- X[-train, ]
Y.test &lt;- Y[-train]</code></pre>
<p>Let’s fit a least-squares regression, which can be done using the lm function. The 17 coefficients of the least-squares regression model are fit by minimizing the RSS, the sum of squared residuals.</p>
<pre class="r"><code>set.seed(1)
LM.mod &lt;- lm(Apps ~ . , data = data_df, subset = train)

LM.pred &lt;- predict(LM.mod, newdata = data_df[-train, ])

RMSE &lt;- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

Percent.Error &lt;- function(x){RMSE(x)/mean(Y) * 100}

LM.RMSE &lt;- RMSE(LM.pred)
OLS.Percent.Err &lt;- Percent.Error(LM.pred)

my_results &lt;- tibble(model = &quot;OLS&quot;,
                  RMSE = LM.RMSE,
                  )

knitr::kable(my_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">45.324</td>
</tr>
</tbody>
</table>
<p>The RMSE can be interpreted as an estimate for the standard deviation of predicted values. In the context of our College data, given a new set of observations, we could estimate the amount of applications a college will receive by the RMSE. The percent error, given by the RMSE over the average value of Y, gives an estimate of percent average deviation of our prediction. This helps interpret the error, given the average college receives 3002 and our percent error is 1.5099806%.</p>
</div>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">1.2</span> Ridge Regression</h2>
<p>Ridge regression fits a linear model by minimizing the quanitity <span class="math inline">\(RSS + \lambda \sum_{j=1}^p\beta_j^2\)</span>, where <span class="math inline">\(\lambda \sum_{j=1}^p\beta_j^2\)</span> also written <span class="math inline">\(\lambda||\beta||^2\)</span> is called a <em>shrinkage penalty</em> and <span class="math inline">\(\lambda \geq 0\)</span> is called a <em>tuning parameter</em>. At the extremes, <span class="math inline">\(\lambda = 0\)</span> returns the least-squares estimate, and <span class="math inline">\(\lambda \rightarrow \infty\)</span> returns the <em>null model</em>, where all the predictors are forced to <span class="math inline">\(0\)</span>. Note, that shrinkage is not applied to the intercept. Instead we first center the inputs and then estmiate <span class="math inline">\(\beta_0\)</span> by <span class="math inline">\(\overline{y}\)</span>. Centering is also necessary because unlike in OLS, the scale of the predictors can influence the estimate. Centering predictors is done by dividing by their standard deviation: <span class="math inline">\(\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^N(x_{ij}-\overline{x_j})^2}}\)</span>. The <span class="math inline">\(\tilde{x_{ij}}\)</span> are called <em>standardized predictors</em> and are used to compute <em>standardized coefficients</em>.</p>
<p>Unlike the least squares estimate which produces only one set of coefficients, ridge regression computes a <em>path</em> of coefficients <span class="math inline">\(\beta^R_{\lambda}\)</span> which is a <span class="math inline">\(p-\)</span>dimensional sequence indexed by <span class="math inline">\(\lambda\)</span>. Cross validation must be used the to select the <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(\beta^R_{\lambda}\)</span> minimizes the out-of-sample error.</p>
<p>Let’s fit a ridge regression. Also, we can manually specify a grid of values for <span class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>set.seed(1)
grid &lt;- 10^ seq (10,-2, length =100)
ridge.mod &lt;- glmnet(X.train, Y.train, alpha = 0, lambda = grid)  

# alpha = 0 specifies that we are using a Ridge Regression.
# glmnet automatically does centering first; Standardize = TRUE by default</code></pre>
<p>The glmnet object has several useful accessor methods that return information about the model. <code>coef</code> returns a matrix of coefficients. There are 19 rows, one for each coefficient, and 100 columns corresponding to different lambda values.</p>
<pre class="r"><code>dim(coef(ridge.mod))</code></pre>
<pre><code>## [1]  18 100</code></pre>
<p>The <code>lambda</code> attribute returns the lambda values we generated. Let’s looks at the coefficients calculated for the 10th, 50th, and 90th lambda.</p>
<pre class="r"><code>el2norm &lt;- function(x){sqrt(sum(x^2))}

el2norm.r &lt;- function(col){el2norm(coef(ridge.mod)[, col])}

coef(ridge.mod)[,c(10,50,90)] %&gt;% rbind( map_dbl(c(10,50,90), el2norm.r) ) %&gt;% rbind(ridge.mod$lambda[c(10,50,90)])</code></pre>
<pre><code>## 20 x 3 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s9           s49           s89
## (Intercept)  3.118028e+03 -2.415997e+03  231.81608434
## PrivateYes  -2.696165e-02 -4.964185e+02 -661.19247088
## Accept       9.715573e-06  3.134358e-01    1.77826472
## Enroll       2.662444e-05  6.947055e-01   -1.27533920
## Top10perc    6.794253e-04  1.488282e+01   86.92547784
## Top25perc    5.983285e-04  1.088166e+01  -34.94138769
## F.Undergrad  5.199477e-06  1.273835e-01    0.05419474
## P.Undergrad  1.213187e-05  2.309573e-01   -0.03056227
## Outstate     3.958645e-07  7.614102e-03   -0.12828208
## Room.Board   4.759186e-06  1.399616e-01    0.28313187
## Books        3.523183e-05  5.714569e-01    0.26976880
## Personal     8.369619e-06  8.288952e-02    0.11487261
## PhD          6.726713e-04  8.114758e+00  -12.36805850
## Terminal     7.243822e-04  7.233512e+00    2.82203511
## S.F.Ratio    5.516312e-04  1.154257e+01  -13.51179410
## perc.alumni -3.098273e-04 -8.365358e+00   -1.81119850
## Expend       1.634436e-06  3.524842e-02    0.02118322
## Grad.Rate    3.206191e-04  8.064134e+00    8.80381612
##              3.118028e+03  2.466617e+03  707.19190066
##              8.111308e+08  1.149757e+04    0.16297508</code></pre>
<p>The last two rows are the <em>l-2-norm</em> and the <span class="math inline">\(\lambda\)</span> value. We can see that for <span class="math inline">\(\lambda = .16\)</span> we have the highest shrinkage of the coefficients</p>
<p>We can also use the <code>predict</code> method to estimate coefficients at a new value of <span class="math inline">\(\lambda\)</span>, say <span class="math inline">\(\lambda = 80\)</span></p>
<pre class="r"><code>predict(ridge.mod, s=80, type=&quot;coefficients&quot;)</code></pre>
<pre><code>## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                         1
## (Intercept) -2.698107e+02
## PrivateYes  -6.121242e+02
## Accept       1.593882e+00
## Enroll      -5.005744e-01
## Top10perc    7.060799e+01
## Top25perc   -2.290605e+01
## F.Undergrad  3.085139e-03
## P.Undergrad -1.522212e-04
## Outstate    -1.151999e-01
## Room.Board   3.024099e-01
## Books        2.723027e-01
## Personal     6.011289e-02
## PhD         -1.064534e+01
## Terminal    -5.831304e-01
## S.F.Ratio   -9.814125e+00
## perc.alumni -3.127758e+00
## Expend       2.834260e-02
## Grad.Rate    1.017593e+01</code></pre>
<p>Let’s also look a the <code>%Dev</code> column when we print our model directly.</p>
<pre class="r"><code># print(ridge.mod)   output is too long</code></pre>
<p><code>%Dev</code> is the <em>Null Deviance</em> of the model. If the <em>Null Deviance</em> is very small, it signifies that the proposed model does not perform better than the null model. We can see that when lambda is very high, the <code>%Dev</code> is almost 0, but increases as lambda decreases, and the penalty on the coefficients is relaxed.</p>
<p>Rather than supplying a pre-defined grid, we can use cross-validation to select the best lambda. The function <code>cv.glmnet</code> not only fits 100 models, but it tells us which one is the best.</p>
<pre class="r"><code>set.seed(1)

ridge.cv &lt;- cv.glmnet(X.train, Y.train, alpha = 0)

ridge.pred &lt;- predict(ridge.cv, s=&quot;lambda.min&quot;, newx = X.test) 


ridge.RMSE &lt;- RMSE(ridge.pred)


bestlam &lt;- ridge.cv$lambda.min</code></pre>
<p>Here <code>cv.glmnet</code> uses 10-fold cross validation (by default) and returns a <code>cv.glmnet</code> object containing a sequence of models. The model that miminimized the MSE can be found using <code>lambda.min</code>. Here the best lambda is 523.658 and its corresponding estimate of the test error is 44.828. We could access the coefficients of this model by calling <code>coef(ridge.cv, s = &quot;lambda.min&quot;)</code> and calculate their <em>l-2-norm</em> 1428.74.</p>
</div>
<div id="the-lasso" class="section level2">
<h2><span class="header-section-number">1.3</span> The Lasso</h2>
<p>The Lasso is similar to ridge regression, except that the penalty is applied to the <em>l-1-norm</em> norm of the coeffiecients: <span class="math inline">\(||\beta||_1\)</span>. That is, the lasso selects the best model by minimizing the quantity <span class="math inline">\(RSS + \lambda\sum_{j=1}^p|\beta_j|\)</span>. Unlike Ridge regression, however, the lasso shrinks some of the coefficients to zero, and thus performs <em>variable selection</em>. The advantage of the lasso is that it yields <em>sparse</em> models with less coefficients are easier to interperet.</p>
<pre class="r"><code>set.seed(1)

lasso.cv &lt;- cv.glmnet(X.train, Y.train, alpha = 1)

lasso.lam &lt;- lasso.cv$lambda.min

coef(lasso.cv, s = &quot;lambda.min&quot;)</code></pre>
<pre><code>## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                         1
## (Intercept) -103.66573558
## PrivateYes  -580.85348775
## Accept         1.70406630
## Enroll        -0.78559016
## Top10perc     74.87113588
## Top25perc    -24.96415815
## F.Undergrad    .         
## P.Undergrad    .         
## Outstate      -0.11343325
## Room.Board     0.26498802
## Books          0.22454134
## Personal       0.07655156
## PhD           -9.13154723
## Terminal       .         
## S.F.Ratio     -7.85770983
## perc.alumni   -0.62191812
## Expend         0.01889981
## Grad.Rate      7.33869822</code></pre>
<pre class="r"><code>lasso.mod &lt;- glmnet(X.train, Y.train, alpha = 1, lambda = grid)

lasso.pred &lt;- predict(lasso.mod, s=lasso.lam, newx = X.test)</code></pre>
<p>We can see that some of the variables have been dropped.</p>
<pre class="r"><code>lasso.RMSE &lt;- RMSE(lasso.pred)

my_results &lt;- rbind(my_results, list(&quot;Ridge&quot;, ridge.RMSE), list(&quot;Lasso&quot;, lasso.RMSE))

knitr::kable(my_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">45.324</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.828</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">43.967</td>
</tr>
</tbody>
</table>
<p>For some reason, the ridge and lasso perform substantially worse than the ordinary least squares model.</p>
</div>
<div id="principle-components-regression" class="section level2">
<h2><span class="header-section-number">1.4</span> Principle Components Regression</h2>
<pre class="r"><code>library(pls)
set.seed(1)
PCR.mod &lt;- pcr(Apps ~ ., subset = train, data = data_df,  validation = &quot;CV&quot;)

validationplot(PCR.mod, val.type = &quot;MSEP&quot;)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The validation plot compares the performance of PCR based off the number of components that are chosen. The MSEP axis is actually the RSME used to measure the training error. The error stabilizes around 5 or 6 components, and overall decreases monotonically as we include more components. Using all components would defeat the purpose of PCR and actually return the OLS solutions, where each variable is its own component.</p>
<pre class="r"><code>summary(PCR.mod)</code></pre>
<pre><code>## Data:    X dimension: 194 17 
##  Y dimension: 194 1
## Fit method: svdpc
## Number of components considered: 17
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV            5009     4484     2545     2515     2144     1938     1914
## adjCV         5009     4649     2535     2504     2085     1901     1880
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV        1908     1988     1992      1915      1899      1906      1959
## adjCV     1875     1947     1950      1874      1857      1864      1914
##        14 comps  15 comps  16 comps  17 comps
## CV         1924      1818      1820      1819
## adjCV      1885      1775      1777      1775
## 
## TRAINING: % variance explained
##       1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X       46.99    87.98    95.51    97.64    98.71    99.53    99.93
## Apps    30.24    78.36    79.67    92.75    92.79    92.83    92.85
##       8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X       99.98   100.00    100.00    100.00    100.00    100.00    100.00
## Apps    93.06    93.06     93.78     94.18     94.18     94.19     94.24
##       15 comps  16 comps  17 comps
## X       100.00    100.00     100.0
## Apps     95.09     95.09      95.2</code></pre>
<p>The summary which reveals the training error and variance explained per number of components included, further shows that including more than 6 components gives no benefit. However it may be best to use even less components to create a better model. Three components should work, since it explains 93% of the variance.</p>
<pre class="r"><code>PCR.pred &lt;- predict(PCR.mod, X.test, ncomp = 3)
PCR.RMSE &lt;- RMSE(PCR.pred)
my_results &lt;- rbind(my_results,  list(&quot;PCR&quot;, PCR.RMSE))

knitr::kable(my_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">45.324</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.828</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">43.967</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">73.931</td>
</tr>
</tbody>
</table>
</div>
<div id="partial-least-squares" class="section level2">
<h2><span class="header-section-number">1.5</span> Partial Least Squares</h2>
<p>Partial least sqaures is a supervised alternative to principle components regression. PLS attempts to choose components that explain variance in both the predictors and response.</p>
<pre class="r"><code>set.seed(1)
PLS.mod &lt;- plsr(Apps ~ ., data = data_df, subset = train, validation = &quot;CV&quot;)
summary(PLS.mod)</code></pre>
<pre><code>## Data:    X dimension: 194 17 
##  Y dimension: 194 1
## Fit method: kernelpls
## Number of components considered: 17
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV            5009     2540     2604     2246     1901     1916     1921
## adjCV         5009     2518     2624     2206     1868     1882     1886
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV        1980     2007     1976      1895      1900      1875      1832
## adjCV     1941     1964     1912      1853      1856      1826      1788
##        14 comps  15 comps  16 comps  17 comps
## CV         1818      1822      1824      1819
## adjCV      1775      1778      1779      1775
## 
## TRAINING: % variance explained
##       1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X       43.67    82.78    92.34    97.64    98.49    99.02    99.56
## Apps    79.68    82.55    90.51    92.83    92.89    92.97    93.05
##       8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X       99.98    99.98     100.0    100.00    100.00    100.00    100.00
## Apps    93.09    94.17      94.2     94.54     95.09     95.09     95.09
##       15 comps  16 comps  17 comps
## X       100.00    100.00     100.0
## Apps     95.09     95.12      95.2</code></pre>
<pre class="r"><code>validationplot(PLS.mod)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We can see that 3 components explains about 93% of the variance and gives a low training RMSE. It’s better to keep the amount of components lowe to decrease the variance of the model, even if more components give slightly lower training RMSE.</p>
<pre class="r"><code>PLS.pred &lt;- predict(PLS.mod, X.test, ncomp = 3)
PLS.RMSE &lt;- RMSE(PLS.pred)
my_results &lt;- rbind(my_results, list(&quot;PLS&quot;, PLS.RMSE))
knitr::kable(my_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">45.324</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.828</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">43.967</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">73.931</td>
</tr>
<tr class="odd">
<td align="left">PLS</td>
<td align="right">53.542</td>
</tr>
</tbody>
</table>
<p>Since PLS supervises the choice of components, it can be more effective than PCR for a regression problem when using less components. Notice how the RMSE at 1 component is alsmost 3 times that for PCR than PLS, but it becomes about equal for the inclusion of more components.</p>
</div>
</div>
<div id="simplifying-with-caret" class="section level1">
<h1><span class="header-section-number">2</span> Simplifying with Caret</h1>
<p>In this section, I write a function using caret to train all the models at once. Each model’s parameters is optimized by cross validation automatically, as specified in the trainControl function.</p>
<pre class="r"><code>set.seed(1)
library(caret)

caret_RMSE &lt;- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

trainControl &lt;- trainControl(method=&quot;cv&quot;, number=5)


  my_caret &lt;- function(method_name){ 
    #this function accepts the name of the method and returns its RMSE from testing it on our specified College dataset
  
  method_fit &lt;- train(Apps~., data=data_df, method=method_name, metric=&quot;RMSE&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;), trControl=trainControl)
  
  method_predictions &lt;- predict(method_fit, X.test)
  
  method_RMSE &lt;- caret_RMSE(method_predictions)
  
  list(method_name, method_RMSE)
  }
  
caret_names &lt;- list(&quot;lm&quot;, &quot;lasso&quot;, &quot;ridge&quot;, &quot;glmnet&quot;, &quot;pcr&quot;, &quot;pls&quot;, &quot;lars&quot;)</code></pre>
<p>Now I write a short pipeline to run the <code>my_caret</code> function on the list of method names and display the my_results in a nice table.</p>
<pre class="r"><code>map(caret_names, my_caret) %&gt;%
  transpose() %&gt;% 
  map(unlist) %&gt;% 
  set_names(c(&quot;models&quot;,&quot;RMSE&quot;)) %&gt;% 
  as_tibble() %&gt;%
  arrange(RMSE)-&gt; 
  caret_table

knitr::kable(caret_table, digits = 3, booktabs = TRUE, caption = &quot;Result produced by Caret. Models arranged by RMSE.&quot;)</code></pre>
<table>
<caption>Result produced by Caret. Models arranged by RMSE.</caption>
<thead>
<tr class="header">
<th align="left">models</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">lasso</td>
<td align="right">40.485</td>
</tr>
<tr class="even">
<td align="left">glmnet</td>
<td align="right">40.490</td>
</tr>
<tr class="odd">
<td align="left">ridge</td>
<td align="right">40.574</td>
</tr>
<tr class="even">
<td align="left">lm</td>
<td align="right">40.579</td>
</tr>
<tr class="odd">
<td align="left">lars</td>
<td align="right">40.579</td>
</tr>
<tr class="even">
<td align="left">pls</td>
<td align="right">48.381</td>
</tr>
<tr class="odd">
<td align="left">pcr</td>
<td align="right">69.426</td>
</tr>
</tbody>
</table>
<p>The my_results are quite similar to running each model individually.The models seem to perform slightly better under caret, probably because of the preprocessing and optimization of parameters done by the caret algorithm.</p>
</div>
<div id="subset-selection" class="section level1">
<h1><span class="header-section-number">3</span> Subset selection</h1>
<div id="best-subset-selection" class="section level2">
<h2><span class="header-section-number">3.1</span> Best Subset Selection</h2>
<p>Subset selection is a technique for selecting a subset of your original varaible to be predictors in your model. For example, let’s say we want to fit a least squares model to the training data, but we suspect that it will generalize better to testing data if we keep fewer variables. Perhaps some are reduntant or lack predictive power. In the <em>Best Subset Selection</em> procedure, we consider all possible models. Since we are working with the College data which has 17 predictor variables, this will require fitting <span class="math display">\[\binom{17}{0} + \binom{17}{1} + \binom{17}{2} + \dots + \binom{17}{17} = \sum_{k = 0}^{17}\binom{17}{k} = 2^{17}\]</span> models. A quick aside regarding the combinatoric identity: <span class="math inline">\(\sum_{k = 0}^{p}\binom{p}{k} = 2^{p}\)</span>. It arises from the question, what is the cardinality of the power set of a set containing <span class="math inline">\(n\)</span> objects. That is, how many distinct subsets can be created from this set. We can apprach this two ways: iterate over each element and decide whether or not to include it in your subset: this gives <span class="math inline">\(2^p\)</span> choices. Or compute individually the amount of subsets of size <span class="math inline">\(k\)</span> from <span class="math inline">\(0\leq k \leq n\)</span> by taking combinations, and then summing the counts. These two approaches are identical, hence the formula. Let’s apply:</p>
<pre class="r"><code>library(leaps)
BEST.mod_9 = regsubsets(Apps ~ ., data_df)
summary(BEST.mod_9)</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Apps ~ ., data_df)
## 17 Variables  (and intercept)
##             Forced in Forced out
## PrivateYes      FALSE      FALSE
## Accept          FALSE      FALSE
## Enroll          FALSE      FALSE
## Top10perc       FALSE      FALSE
## Top25perc       FALSE      FALSE
## F.Undergrad     FALSE      FALSE
## P.Undergrad     FALSE      FALSE
## Outstate        FALSE      FALSE
## Room.Board      FALSE      FALSE
## Books           FALSE      FALSE
## Personal        FALSE      FALSE
## PhD             FALSE      FALSE
## Terminal        FALSE      FALSE
## S.F.Ratio       FALSE      FALSE
## perc.alumni     FALSE      FALSE
## Expend          FALSE      FALSE
## Grad.Rate       FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: exhaustive
##          PrivateYes Accept Enroll Top10perc Top25perc F.Undergrad
## 1  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot; &quot;       &quot; &quot;       &quot; &quot;        
## 2  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 3  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 4  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 5  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 6  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 7  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot;*&quot;       &quot; &quot;        
## 8  ( 1 ) &quot;*&quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
##          P.Undergrad Outstate Room.Board Books Personal PhD Terminal
## 1  ( 1 ) &quot; &quot;         &quot; &quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 2  ( 1 ) &quot; &quot;         &quot; &quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 3  ( 1 ) &quot; &quot;         &quot; &quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 4  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 5  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 6  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot;*&quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 7  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot;*&quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 8  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot;*&quot;        &quot; &quot;   &quot; &quot;      &quot;*&quot; &quot; &quot;     
##          S.F.Ratio perc.alumni Expend Grad.Rate
## 1  ( 1 ) &quot; &quot;       &quot; &quot;         &quot; &quot;    &quot; &quot;      
## 2  ( 1 ) &quot; &quot;       &quot; &quot;         &quot; &quot;    &quot; &quot;      
## 3  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 4  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 5  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 6  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 7  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 8  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;</code></pre>
<p>Th asterix indicated whether the variable is included in the model. The row indices indicate a model of that many variables, so each row is a separate model, each one containing one more variable than in the previous row. The algorithm calculates only up to 9-variable models by default. Lets try setting <code>nvmax = 17</code> to include all models, and then extract some statistics.</p>
<pre class="r"><code>BEST.mod_17 &lt;- regsubsets(Apps ~ ., data_df, nvmax = 17)
bmod.summary &lt;- summary(BEST.mod_17)</code></pre>
<pre class="r"><code>show_metrics &lt;- function(my_summary){

metrics &lt;- c(&quot;adjr2&quot;, &quot;rsq&quot;, &quot;bic&quot;, &quot;cp&quot;)

best_df &lt;- as.data.frame(`[`(my_summary, metrics)) 

best_df_melt &lt;- best_df %&gt;% gather(key = &quot;metric&quot;, value = &quot;value&quot;) %&gt;% mutate(model = rep(1:17, 4))



(ggplot(data = best_df_melt, aes(x = model, y = value, color = metric)) +
         geom_line() +
        facet_grid(metric ~ ., scales = &quot;free_y&quot;)) %&gt;% print()

c(map_dbl(best_df[c(&quot;adjr2&quot;, &quot;rsq&quot;)], which.max), map_dbl(best_df[c(&quot;bic&quot;, &quot;cp&quot;)], which.min))

}</code></pre>
<pre class="r"><code>show_metrics(bmod.summary)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre><code>## adjr2   rsq   bic    cp 
##    13    17    10    12</code></pre>
<p>The graphs show how the error metric changes as we introduce more variables into the model. They seem to drop off rapidly and then hit a max or min, as displayed in the table.</p>
</div>
<div id="estimates-of-test-error" class="section level2">
<h2><span class="header-section-number">3.2</span> Estimates of Test error</h2>
<p>These statistics are intended to estimate the test error of the model by making an adjustment to the training error to account for the bias due to overfitting in the training process.</p>
<p>Let’s interperet each of these statistics (exact formulas can be looked up):</p>
<ul>
<li><p><span class="math inline">\(R^2_{adj}\)</span> modifies the denominator of <span class="math inline">\(RSS\)</span> in <span class="math inline">\(R^2\)</span> to <span class="math inline">\(RSS/(N - p - 1)\)</span>, where <span class="math inline">\(p\)</span> is the number of predictor variables in our model, thus inflating the error as we add more predictors. Unlike <span class="math inline">\(R^2\)</span> which will increase monotincally with additionaly variables,<span class="math inline">\(R^2_{adj}\)</span> penalizes the addition of noise variables. Both these statistics measure the goodness-of-fit of the model on a scale of <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, so we want to choose the model that maximizes them.</p></li>
<li><p><span class="math inline">\(C_p\)</span> adds a penalty of <span class="math inline">\(p *\hat{\sigma}\)</span> to the RSS, where sigma estimates the variance in the response. Clearly <span class="math inline">\(C_p\)</span> increases with more predictors, so we want to minimize it.</p></li>
<li><p><span class="math inline">\(AIC\)</span> is proportional to <span class="math inline">\(C_p\)</span> so it should be minimized</p></li>
<li><p><span class="math inline">\(BIC\)</span> adds a term of <span class="math inline">\(log(N) * p * \hat{\sigma}^2\)</span> to the <span class="math inline">\(RSS\)</span> where <span class="math inline">\(N\)</span> is the number of obserations, so it penalizes models with many variables and observations. Thus, we want to mimize it. The more severe penalty given by the <span class="math inline">\(BIC\)</span> also explains why it chose the sparsest model.</p></li>
</ul>
</div>
<div id="forward-selection" class="section level2">
<h2><span class="header-section-number">3.3</span> Forward Selection</h2>
<p>Clearly Best Subset selection is very computationally intensive, so shorter alternatives have been developed. <em>Forward Selection</em> works by adding in the best variable at each stage. That is, it chooses to include the variable that minimizes the model’s RSS, and then keeps that variable while choosing a new one from the remaining variables, until all are used. This my_results in having to test <span class="math inline">\(1 + \sum_{k=0}^{p-1}p - k = 1 + \frac{p(p+1)}{2}\)</span> models. In our College dataset, <span class="math inline">\(p = 17\)</span>, so the total number of models we fit is <span class="math display">\[1 + 17 + 16 + \dots + 2 + 1 = 1 + \frac{(17)(18)}{2} = 154\]</span> This is substantially less computation. Note that the additional one is from fitting the null model. At each stage the best variable is chosen by comparing <span class="math inline">\(R^2\)</span> amongst the different possible models. At the end, we get <span class="math inline">\(p+1\)</span> models: <span class="math inline">\(M_0, \dots, M_{17}\)</span>. Since these models do not have the same number of variables, we cannot compare them directly using <span class="math inline">\(R^2\)</span> (it will increase monotionically). Instead we can use one of the alternative metrics listed earlier.</p>
<pre class="r"><code>FWD.mod &lt;- regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = &quot;forward&quot;)

sum.FWD &lt;- summary(FWD.mod)


show_metrics(sum.FWD)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## adjr2   rsq   bic    cp 
##    13    17    10    12</code></pre>
<p>The metrics are the same for both methods.</p>
<p>Let’s compare the speed of these algorithms.</p>
<pre class="r"><code>BEST_time</code></pre>
<pre><code>## Unit: milliseconds
##                                              expr      min       lq
##  regsubsets(Apps ~ ., data = data_df, nvmax = 17) 3.004913 3.500291
##     mean   median       uq      max neval
##  5.13265 4.310343 6.365104 15.36862   100</code></pre>
<pre class="r"><code>FWD_time</code></pre>
<pre><code>## Unit: milliseconds
##                                                                  expr
##  regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = &quot;forward&quot;)
##       min       lq     mean   median       uq      max neval
##  2.683556 3.071442 4.420644 3.354546 3.782163 78.56575   100</code></pre>
<p>We can see that forward selection is on average a few miliseconds faster than the full model selection.</p>
<p>Let’s compare the models they selected as determined by the BIC. Since the BIC was minimized by the model with 10 variables, we can see if they are the same variables for the Best subset and Forward selection.</p>
<pre class="r"><code>sum.FWD$which[10, ] == bmod.summary$which[10, ]</code></pre>
<pre><code>## (Intercept)  PrivateYes      Accept      Enroll   Top10perc   Top25perc 
##        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 
## F.Undergrad P.Undergrad    Outstate  Room.Board       Books    Personal 
##        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 
##         PhD    Terminal   S.F.Ratio perc.alumni      Expend   Grad.Rate 
##        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE</code></pre>
<p>It looks like both methods give the same my_results. Therefore we can opt for the faster, forward selection method. However, this was a small data set. my_results may vary for large high-dimensional data.</p>
</div>
</div>
<div id="decision-trees" class="section level1">
<h1><span class="header-section-number">4</span> Decision Trees</h1>
<div id="fitting-classification-trees" class="section level2">
<h2><span class="header-section-number">4.1</span> Fitting Classification Trees</h2>
<p>In this setion I we will fit a classification tree to the College data set to predict whether or not a college is private. In fact, we use the original dataset, not the <code>data_df</code> that encodes the private factor as a dummy variable.</p>
<pre class="r"><code>library(tree)
tree.College &lt;- tree(Private ~ ., data = College)
summary(tree.College)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = Private ~ ., data = College)
## Variables actually used in tree construction:
## [1] &quot;F.Undergrad&quot; &quot;Outstate&quot;    &quot;P.Undergrad&quot; &quot;Top10perc&quot;   &quot;perc.alumni&quot;
## [6] &quot;Enroll&quot;     
## Number of terminal nodes:  10 
## Residual mean deviance:  0.2563 = 196.6 / 767 
## Misclassification error rate: 0.04891 = 38 / 777</code></pre>
<p>The misclassification error rate, which is the training error rate, is about <span class="math inline">\(5%\)</span>, which is quite good. The deviance reported is given by <span class="math display">\[-2\sum_m\sum_kn_{nm}\log\hat{p}_{mk}\]</span> Where <span class="math inline">\(n_{mk}\)</span> is the number of observations in the <span class="math inline">\(m\)</span>th terminal node that belong to the <span class="math inline">\(k\)</span>th class. Where <span class="math inline">\(m\)</span> is the number of terminal nodes of the tree, 10 in this case, and <span class="math inline">\(k\)</span> can take <span class="math inline">\(2\)</span> values: one for each class label (public or private). <span class="math inline">\(\hat{p}_{mk}\)</span> is the class propotion for a specific node, so in this case it is <span class="math inline">\(\frac{n_{mk}}{n_{m1} + n_{m2}}\)</span> for <span class="math inline">\(k = 0\)</span> or <span class="math inline">\(k=1\)</span>. We can see that this formula for deviance looks like the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a>. It follows that we can interpret it as a cumulative measure of node purity. To elaborate let’s discuss how the decision trees work. They divide up the predictor space into <span class="math inline">\(m\)</span> regions which can be visualized as a cut-up rectangular box where each box is the region, or an upside-down tree where the terminal leaves are the regions. <img src="treeimage.png"></p>
<p>For a classification tree, the class label that is a assigned to a data point in a certain region of the predictor space is the <em>majority vote</em> of class labels in that region. Since we only have 2 class labels, it would be whichever label is dominant. Therefore, misclassification error occurs when there are training points in a region where the majority vote has a different label. That’s why we want to measure the region’s or node’s purity: the percentage of training points that belong to a single label. By maximizing the purity, we minimize the training error. An easily interpretable measure of node purity is the Gini index: <span class="math display">\[G = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\]</span> which gives the purity for the <span class="math inline">\(m\)</span>th node. If all the training labels belong to almost a single class <span class="math inline">\(k\)</span>, we can see that <span class="math inline">\(G\)</span> is almost 0 and the node is very pure. The <em>cross-entropy</em> and similarly the deviance have a similar interpretaion to the Gini index, and thus we want to minimize them to get the best classfier.</p>
<p>The <em>residual mean deviance</em> statistic reported is simply the deviance divided by <span class="math inline">\(n - |T|\)</span> where <span class="math inline">\(n = 777\)</span> is the number of observations, and <span class="math inline">\(|T| = 10\)</span> is the number of terminal nodes. A small RMD indicates that the tree provides a good fit to the data.</p>
<p>We can print out a visual representation of the tree.</p>
<pre class="r"><code>plot(tree.College)
text(tree.College)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>We can see the most important determining variable is the number of full time undergraduates. From the leftmost branch we can see that a second split of the <code>F.Undergrad</code> predictor was significant: smaller schools tend to be private. The out-of-state tuition is also an important fact: schools with smaller enrollments but higher tuition tend to be private, and if the tuition is just really high, it is probably private. Private schools also have a larger percentage of alumni who donate back.</p>
<p>To get an estimate of the test error we need to split the data into a training and validation set.</p>
<pre class="r"><code>College.test &lt;- College[-train, ]
Private.test &lt;- College$Private[-train]
tree.college &lt;- tree(Private ~ ., data = College, subset = train)
tree.pred &lt;- predict(tree.college, College.test, type = &quot;class&quot;)
tree.table &lt;- table(tree.pred, Private.test)</code></pre>
<p>The table function gives a confusion matrix to explain the performance of the classifier:</p>
<pre class="r"><code>tree.table</code></pre>
<pre><code>##          Private.test
## tree.pred  No Yes
##       No  133  31
##       Yes  31 388</code></pre>
<p>By dividing the counts in the off-diagonal by the total observations, we get the misclassification error:</p>
<pre class="r"><code>error_rate = function(myTable)  {(myTable[1,2] + myTable[2, 1]) / (sum(myTable))}

tree_error = error_rate(tree.table)

tree_error</code></pre>
<pre><code>## [1] 0.1063465</code></pre>
</div>
<div id="cost-complexity-pruning" class="section level2">
<h2><span class="header-section-number">4.2</span> Cost-complexity pruning</h2>
<p>Fitting a deep classification tree may produce good predictions on the training set but is likely to overfit the data, leading to poor performance on the test set. It may be better to build a less complex tree that has more bias but lower variance when we test it against new data. To accomplish this we can use a formula that penalizes the complexity, or number of terminal nodes of the tree: <span class="math display">\[\sum_{m = 1}^{|T|}\sum_{i:x_i\in R_m}(y_i - \hat{y_{R_m}})^2+\alpha |T|\]</span> Alpha is the tuning parameter that adds a penalty for more leaves. When alpha is 0, we get the original tree. As it increases, we get a sequence of subtrees, with less or equal node to the previous tree. Cross validation can be used to select the optimal value for alpha. This process is done on the training data set. Once we choose an alpha, we can test that tree’s performance on the test set.</p>
<pre class="r"><code>set.seed(1)
cv.college = cv.tree(tree.college, FUN = prune.misclass)
cv.college</code></pre>
<pre><code>## $size
## [1] 8 6 3 2 1
## 
## $dev
## [1] 19 19 21 32 51
## 
## $k
## [1]      -Inf  0.000000  1.666667 14.000000 22.000000
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<pre class="r"><code>best_size &lt;- function(x){x$dev %&gt;% which.min() %&gt;% `[`(x$size, .)}
best_size_1 &lt;- best_size(cv.college)</code></pre>
<p>Explanation of the output: <code>size</code> refers to the number of terminal nodes in the tree, <code>dev</code> is the CV error, <code>k</code> is the tuning parameter <span class="math inline">\(\alpha\)</span>. We set <code>FUN = prune.misclass</code>to indicate that classification error is what guides the tuning process, as opposed to the default metric: deviance.</p>
<pre class="r"><code>plot_cv &lt;- function(cv_object){  as.data.frame(cv_object[c(&#39;size&#39;, &#39;dev&#39;, &#39;k&#39;)]) %&gt;% gather(key = &quot;metric&quot;, value = &quot;value&quot;) %&gt;% mutate(index = rep(1:length(cv_object$size), 3)) %&gt;%
ggplot(data = ., aes(x = index, y = value, col = metric)) +
  geom_line()}

plot_cv(cv.college)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>It looks like the CV error is minimized for a tree of size 8.</p>
<p>Now we can apply the <code>prune.misclass</code> function to obtain this tree:</p>
<pre class="r"><code>set.seed(1)
prune.college &lt;- prune.misclass(tree.college, best = best_size_1)
plot(prune.college)
text(prune.college, pretty = 0)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>We can see it incorporates the 2 most important variables previously identified, out-of-state tuition and number of full-time undergraduates.</p>
<pre class="r"><code>prune.pred = predict(prune.college, College.test, type = &quot;class&quot;)
prune.table &lt;- table(prune.pred, Private.test)</code></pre>
<pre class="r"><code>prune.table</code></pre>
<pre><code>##           Private.test
## prune.pred  No Yes
##        No  133  30
##        Yes  31 389</code></pre>
<pre class="r"><code>error_rate(prune.table)</code></pre>
<pre><code>## [1] 0.1046312</code></pre>
<p>Interestingly we have the same exact test error! If you compare both tables, you can see that the numbers are different but the sums are the same, yielding the same fraction. Now we have a much more interpretable tree with the same exact performance.</p>
</div>
<div id="fitting-regression-trees" class="section level2">
<h2><span class="header-section-number">4.3</span> Fitting Regression Trees</h2>
<p>Regression trees work much like classification trees, except that we estimate points by taking the average of the training responses of the region that it falls into, whereas in classification we took the majority vote. Let’s see how a regression tree performs on our college dataset.</p>
<pre class="r"><code>set.seed(1)
rtree.college &lt;- tree(Apps ~ ., subset = train, data = College)
summary(rtree.college)</code></pre>
<pre><code>## 
## Regression tree:
## tree(formula = Apps ~ ., data = College, subset = train)
## Variables actually used in tree construction:
## [1] &quot;Accept&quot;    &quot;Top25perc&quot;
## Number of terminal nodes:  6 
## Residual mean deviance:  5284000 = 993400000 / 188 
## Distribution of residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -8766.0  -471.8  -106.9     0.0   266.9 23620.0</code></pre>
<pre class="r"><code>plot(rtree.college)
text(rtree.college, pretty = 0)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Notice that only two predictors were used in the construction of this tree, but they are split 6 times. The <em>residual mean deviance</em> is just the mean squared error for the tree. <code>Accept</code> refers to the number of applications accepted. It makes perfect sense that this quantity would be correlated with <code>Apps</code> which is the number of applications received. I did not expect that graduateion rate would be the other important variable; however, it is only used in one split.</p>
<pre class="r"><code>rtree.yhat &lt;- predict(rtree.college, College[-train, ])
TREE.RMSE &lt;- RMSE(rtree.yhat)
TREE.RMSE</code></pre>
<pre><code>## [1] 63.64753</code></pre>
<pre class="r"><code>set.seed(1)
rcv.college = cv.tree(rtree.college)
plot_cv(rcv.college)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<pre class="r"><code>rcv.college</code></pre>
<pre><code>## $size
## [1] 6 5 4 3 2 1
## 
## $dev
## [1] 2103518473 2137107171 2275506070 2892084499 3482195433 5160836679
## 
## $k
## [1]       -Inf   50248845   60525208  333091597  589316171 2790697793
## 
## $method
## [1] &quot;deviance&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<pre class="r"><code>best_size_2 &lt;- ifelse(best_size(rcv.college) != 1, best_size(rcv.college), 5)</code></pre>
<p>It looks like the best tree is of size 6. Note that the <code>predict</code> function throws an error for a single-node tree.</p>
<pre class="r"><code>set.seed(1)
rprune.college &lt;- prune.tree(rtree.college, best = best_size_2)

plot(rprune.college)
text(rprune.college, pretty = 0)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<pre class="r"><code>rcollege.pred = predict(rprune.college, newdata = College[-train, ])

RMSE(rcollege.pred)</code></pre>
<pre><code>## [1] 63.64753</code></pre>
<p>Noice that this performs exactly the same as the unpruned tree even though it uses <code>Terminal</code>: percent of faculty with terminal degrees, as the secondary predictor.</p>
<pre class="r"><code>my_results &lt;- rbind(my_results, list(&quot;Tree&quot;, TREE.RMSE)) %&gt;% arrange(RMSE)
knitr::kable(my_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">43.967</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.828</td>
</tr>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">45.324</td>
</tr>
<tr class="even">
<td align="left">PLS</td>
<td align="right">53.542</td>
</tr>
<tr class="odd">
<td align="left">Tree</td>
<td align="right">63.648</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">73.931</td>
</tr>
</tbody>
</table>
<p>We can see that the regression tree performs worse than the optimized linear regression models. Perhaps we can infer that a linear decision boundary is a better fit for this data set. The tree will perform better when the predictor space can be divided up into rectangular regions.</p>
<p><img src="treefit.png"></p>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">4.4</span> Bagging</h2>
<p>Bagging is a technique to reduce the variance of a statistical learning procedure. If we fit a deep tree to the data, we may have overfitting, so the model will not generalize well. However, if we fit many trees and average the results we will have lower variance when generalizing. Bagging stands for <em>bootstrap aggregation</em> and works as follows: Take <span class="math inline">\(B\)</span> bootstrap samples and fit a full tree to each sample. Our prediction for an observation <span class="math inline">\(x\)</span> is <span class="math display">\[\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\]</span></p>
<pre class="r"><code>library(randomForest)
set.seed(1)
bag.college &lt;- randomForest(Apps ~ ., data = College, subset = train, mtry = 17, importance = TRUE)
bag.college</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = Apps ~ ., data = College, mtry = 17, importance = TRUE,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 17
## 
##           Mean of squared residuals: 8663121
##                     % Var explained: 65.11</code></pre>
<pre class="r"><code>varImpPlot(bag.college)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>To discuss these error metrics, we need to discuss <span class="math inline">\(OOB\)</span>: out-of-bag observations. For each observation, it will be left out of about <span class="math inline">\(B/3\)</span> bootstrap sample, and is said to be an out-of-bag observation. TWe can estimate the error of a single observation by finding the residual each time it was out of bag, and averaging them. The root mean square of these residuals estimates the test error. The <code>%IncMSE</code> reports the mean decrease of accuracy in predictions on the out-of-bag samples when a given variable is excluded from the model, where the <span class="math inline">\(OOB\)</span> sample is the observations left out when we take a bootstrap sample. The <code>IncNodePurity</code> is a measure of the total decrease in node impurity that results from splits on that variable, averaged over all trees. In case of regression trees, node impurity is measured using RSS</p>
<p>Note that bagging is a special case of random forests where all the predictors are used, so we can call the <code>randomForest</code> function and specify <code>mtry = 17</code>.</p>
<pre class="r"><code>yhat.bag &lt;- predict(bag.college, newdata = College[-train, ])
RMSE(yhat.bag)</code></pre>
<pre><code>## [1] 57.13291</code></pre>
<p>This is an improvement over the plain regression tree.</p>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">4.5</span> Random Forests</h2>
<p>Random forest provide an improvement over bagging by <em>decorrelating</em> the trees. With a random foresst, you are only allowedd to consider a subset of predictors during each split. For example, in our bagging model, for each split consider which of the 17 predictors will yield a greatest decrease in residual error and we split that predictor. With random forests we choose a smaller number, usually <span class="math inline">\(\sqrt{p}\)</span> for classification and <span class="math inline">\(p/3\)</span> for regressoin, so in this case <span class="math inline">\(6\)</span> predictors. During each split we choose another random sample of <span class="math inline">\(6\)</span> predictors from our original 17 and only consider the best split amongst those predictors. This decorrelates the trees especially when you have a couple of very strong predictors driving the splits. Then if you fit 500 trees, they’ll have similar specifications even if the bootstrapped samples are different. Random forests ensure you are considering the contributions of each predictor in making the best decision.</p>
<pre class="r"><code>set.seed(1)
rf.college &lt;- randomForest(Apps ~ ., data = College, subset = train, mtry = 6, importance = TRUE)
yhat.rf = predict(rf.college, newdata = College[-train, ])
RF.RMSE &lt;- RMSE(yhat.rf)
RF.RMSE</code></pre>
<pre><code>## [1] 55.09523</code></pre>
<p>The random forest did not improve over bagging.</p>
<pre class="r"><code>importance(rf.college)</code></pre>
<pre><code>##                %IncMSE IncNodePurity
## Private      2.3399770      13631854
## Accept      18.7177910    1763876068
## Enroll      12.4804402    1118401360
## Top10perc    2.7414120     108871243
## Top25perc    1.4729585     139928982
## F.Undergrad  9.3128040     818290221
## P.Undergrad  2.6593772     226790494
## Outstate    -2.1680527      61126749
## Room.Board   0.3275687      28617912
## Books       -2.9301220      65301093
## Personal    -2.7154225      57527289
## PhD         -0.2005949      59326427
## Terminal    -1.9788116      48049791
## S.F.Ratio    1.1426781     126809177
## perc.alumni  0.7150447      34342168
## Expend       3.6025646      41975290
## Grad.Rate    3.6260142      82016293</code></pre>
<pre class="r"><code>varImpPlot(rf.college)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>The three clear most important variables are the number of accepted applicants, number of new students enrolled, and number of fulltime undergraduates. Even though the performance on this test set was not much better than bagging, I believe this model would generalize much better. The bagging model was dominated by the <code>Accept</code> predictor and did not properly weight the contributions of <code>Enroll</code> and <code>F.Undergrad</code>, even though they were still identified as the most important. Instead the mysterious <code>Top25perc</code> found its way to the top, just like in the original tree.</p>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">4.6</span> Boosting</h2>
<p>Boosting is a “slow learning” method creates an additive model after training trees on bootstrapped samples.. The algorithm for boosting begins by assuming that your prediction for yhat is 0, so your residual is yhat - 0 = yhat. It fits a tree on each bootstrapped sample, each time adding a small increment to yhat, and decreasing the residual. Each prediction is actually the sum of small predictions made on each bootstrapped set, where each new prediction is made in order to decrease the residual slightly more. The output is the boosted additive model. Here is the algorithm as shown in the ISLR book:</p>
<p><img src="boosting.png"></p>
<p>Lambda is the shrinkage parameter applied to each boostrap prediction. The parameter <span class="math inline">\(d\)</span>, called the ineraction depth, can be quite small, and results in fitting small trees. By fitting small trees to the residual, we slowly improve f.hat in areas where it does not perform well. Since each tree takes into account trees that have already been grown, small trees are sufficient, which makes this model much different from the bagging approach.</p>
<pre class="r"><code>library(gbm)
set.seed(1)
boost.college &lt;- gbm(Apps ~ ., data = College[-train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4)
summary(boost.college)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<pre><code>##                     var     rel.inf
## Accept           Accept 71.03106826
## Enroll           Enroll 13.47969809
## Top25perc     Top25perc  4.06415091
## F.Undergrad F.Undergrad  2.92793274
## Top10perc     Top10perc  2.26276392
## Outstate       Outstate  1.24563265
## Terminal       Terminal  1.19311365
## Grad.Rate     Grad.Rate  1.10654086
## Expend           Expend  0.85114060
## PhD                 PhD  0.64147680
## Room.Board   Room.Board  0.45554150
## S.F.Ratio     S.F.Ratio  0.21998081
## P.Undergrad P.Undergrad  0.16033008
## Books             Books  0.11381049
## perc.alumni perc.alumni  0.11320335
## Personal       Personal  0.10219142
## Private         Private  0.03142387</code></pre>
<p>Note that the distribution assumed on the response is gaussian, since this is a regression problem (bernoulli for classification), and the shrinkage parameter is by default set to <span class="math inline">\(.001\)</span>. The relative influence plot confirms that accept and enroll are by far the most important variables.</p>
<pre class="r"><code>yhat.boost &lt;- predict(boost.college, newdata = College[-train, ], n.trees = 5000)
BOOST.RMSE &lt;- RMSE(yhat.boost)
BOOST.RMSE</code></pre>
<pre><code>## [1] 25.14018</code></pre>
<p>Wow, boosting emerges as the clear winner amongst all the models tested so far.</p>
<pre class="r"><code>my_results &lt;- rbind(my_results, list(&quot;Boost&quot;, BOOST.RMSE), list(&quot;RF&quot;, RF.RMSE)) %&gt;% arrange(RMSE)
knitr::kable(my_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Boost</td>
<td align="right">25.140</td>
</tr>
<tr class="even">
<td align="left">Lasso</td>
<td align="right">43.967</td>
</tr>
<tr class="odd">
<td align="left">Ridge</td>
<td align="right">44.828</td>
</tr>
<tr class="even">
<td align="left">OLS</td>
<td align="right">45.324</td>
</tr>
<tr class="odd">
<td align="left">PLS</td>
<td align="right">53.542</td>
</tr>
<tr class="even">
<td align="left">RF</td>
<td align="right">55.095</td>
</tr>
<tr class="odd">
<td align="left">Tree</td>
<td align="right">63.648</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">73.931</td>
</tr>
</tbody>
</table>
<p>Actually this result was unexpected. I have run this same code without setting the seed on the train/test split to get different results, and boosting consistently performed similar to ridge and lasso. However, it seems to perform anomolously well when setting the seed to 1.</p>
</div>
</div>
<div id="support-vector-machines" class="section level1">
<h1><span class="header-section-number">5</span> Support Vector Machines</h1>
<p>Support vector machines can be used for binary classification problems. They partition the predictor space by a separating hyperplace (a flat, affine, n-1 dimensional subspace), which aims to classify all points on either side into a single label, if the data is indeed linearly seprable. Here we can apply it to predict whether or not a college is public or private.</p>
<pre class="r"><code>set.seed(1)
library(e1071)
svm.fit &lt;- svm(Private ~ ., data = College, kernel = &quot;linear&quot;, cost = 10, scale = FALSE)</code></pre>
<pre class="r"><code>summary(svm.fit
        )</code></pre>
<pre><code>## 
## Call:
## svm(formula = Private ~ ., data = College, kernel = &quot;linear&quot;, 
##     cost = 10, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  10 
##       gamma:  0.05882353 
## 
## Number of Support Vectors:  178
## 
##  ( 54 124 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  No Yes</code></pre>
<p>The summary tells us there are 178 support vectors: 54 for the class ‘No’ and 124 for the class ‘Yes’. Support vectors are vectors that lie on the margin of the hyperplane or on the wrong side of its label. The <em>margin</em> is the minimal distance of the training observations to the hyperplane, and is a quantity we want to maximize to determine the optimal separating hyperplane.</p>
<pre class="r"><code>svm.fit$index %&gt;% head(n = 15)</code></pre>
<pre><code>##  [1]   7  27  40  43  58  60  67  70  82 111 143 173 179 200 222</code></pre>
<pre class="r"><code>length(svm.fit$index)</code></pre>
<pre><code>## [1] 178</code></pre>
<p>The ‘index’ attribute gives the indices of the observations that were used as support vectors.</p>
<p>The cost parameter determine how tolerant an SVC is to misclassified points. It can be interpreted as an “error budget” so increasing it allows for additional points to be misclassified in exchange for more robust prediction of other points. Generally, increasing the cost increases the bias and decreases the variance by preventing overfitting.</p>
<p>Let’s try using a smaller value of the cost parameter:</p>
<pre class="r"><code>set.seed(1)
tune.out &lt;- tune(svm, Private ~ ., data = College[train, ], kernel = &quot;linear&quot;,
                 ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100, 200, 300, 400, 500 ,600, 1000)))</code></pre>
<pre class="r"><code>summary(tune.out)</code></pre>
<pre><code>## 
## Parameter tuning of &#39;svm&#39;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##   100
## 
## - best performance: 0.03105263 
## 
## - Detailed performance results:
##     cost      error dispersion
## 1  1e-03 0.24157895 0.07490427
## 2  1e-02 0.11815789 0.08518669
## 3  1e-01 0.09289474 0.08412871
## 4  1e+00 0.10342105 0.07427758
## 5  5e+00 0.07736842 0.05089534
## 6  1e+01 0.06710526 0.05510629
## 7  1e+02 0.03105263 0.03648111
## 8  2e+02 0.05657895 0.05702766
## 9  3e+02 0.05684211 0.05752407
## 10 4e+02 0.06184211 0.05901690
## 11 5e+02 0.06184211 0.05901690
## 12 6e+02 0.06184211 0.05901690
## 13 1e+03 0.06184211 0.05901690</code></pre>
<p>Cross validation shows that the classifier performs best when the cost is set to <code>100</code>.</p>
<p>The <code>tune.out</code> object stores the best model:</p>
<pre class="r"><code>svm.clf = tune.out$best.model

summary(svm.clf)</code></pre>
<pre><code>## 
## Call:
## best.tune(method = svm, train.x = Private ~ ., data = College[train, 
##     ], ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 
##     200, 300, 400, 500, 600, 1000)), kernel = &quot;linear&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  100 
##       gamma:  0.05882353 
## 
## Number of Support Vectors:  22
## 
##  ( 11 11 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  No Yes</code></pre>
<p>The best model has a drastically larger cost parameter value.</p>
<pre class="r"><code>yhat_svm = predict(svm.clf, College[-train, ])

svm.table = table(yhat_svm, College$Private[-train])

svm.table</code></pre>
<pre><code>##         
## yhat_svm  No Yes
##      No  131  15
##      Yes  33 404</code></pre>
<pre class="r"><code>svm_error = error_rate(svm.table)</code></pre>
<pre class="r"><code>clf_results = tibble(classifier = list(&#39;tree&#39;, &#39;svm&#39;),
                         error = c(tree_error, svm_error)) %&gt;% arrange(error)

knitr::kable(clf_results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">classifier</th>
<th align="right">error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">svm</td>
<td align="right">0.082</td>
</tr>
<tr class="even">
<td align="left">tree</td>
<td align="right">0.106</td>
</tr>
</tbody>
</table>
<p>The support vector machine classifier outperforms the decision tree.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
