<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Harrison Tietze" />


<title>ML</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="faded.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Harrison's website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About Me</a>
</li>
<li>
  <a href="simulations.html">Sims</a>
</li>
<li>
  <a href="ML.html">ML</a>
</li>
<li>
  <a href="GLM.html">GLM</a>
</li>
<li>
  <a href="certificates.html">certificates</a>
</li>
<li>
  <a href="topology.html">Baire Space</a>
</li>
<li>
  <a href="KDE.html">KDE</a>
</li>
<li>
  <a href="https://docs.google.com/document/d/e/2PACX-1vTnpiuuxd0-ia3SMAk1SGusAyLyvwPN-wsl_dQCIC6Nglj0WgyTne0nYS1JISZZI6H-ym631b9B0kr0/pub">Novella</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">ML</h1>
<h4 class="author"><em>Harrison Tietze</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#regression-models"><span class="toc-section-number">1</span> Regression Models</a><ul>
<li><a href="#glmnet"><span class="toc-section-number">1.1</span> glmnet</a></li>
<li><a href="#ridge-regression"><span class="toc-section-number">1.2</span> Ridge Regression</a></li>
<li><a href="#the-lasso"><span class="toc-section-number">1.3</span> The Lasso</a></li>
<li><a href="#principle-components-regression"><span class="toc-section-number">1.4</span> Principle Components Regression</a></li>
<li><a href="#partial-least-squares"><span class="toc-section-number">1.5</span> Partial Least Squares</a></li>
</ul></li>
<li><a href="#simplifying-with-caret"><span class="toc-section-number">2</span> Simplifying with Caret</a></li>
<li><a href="#subset-selection"><span class="toc-section-number">3</span> Subset selection</a><ul>
<li><a href="#best-subset-selection"><span class="toc-section-number">3.1</span> Best Subset Selection</a></li>
<li><a href="#estimates-of-test-error"><span class="toc-section-number">3.2</span> Estimates of Test error</a></li>
<li><a href="#forward-selection"><span class="toc-section-number">3.3</span> Forward Selection</a></li>
</ul></li>
</ul>
</div>

<div id="regression-models" class="section level1">
<h1><span class="header-section-number">1</span> Regression Models</h1>
<p>reference: <em>Introduction to Statistical learning</em> (2015) <em>Tibshirani, Hastie</em></p>
<div id="glmnet" class="section level2">
<h2><span class="header-section-number">1.1</span> glmnet</h2>
<p>In this analysis we will work with the “College” dataset, which contains 777 observations with 18 variables. The goals is to predict how many applications a college will recieve “Apps” based off other predictors.</p>
<p>First examine the structure of the dataset:</p>
<pre class="r"><code>str(College)</code></pre>
<pre><code>## &#39;data.frame&#39;:    777 obs. of  18 variables:
##  $ Private    : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Apps       : num  1660 2186 1428 417 193 ...
##  $ Accept     : num  1232 1924 1097 349 146 ...
##  $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...
##  $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...
##  $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...
##  $ F.Undergrad: num  2885 2683 1036 510 249 ...
##  $ P.Undergrad: num  537 1227 99 63 869 ...
##  $ Outstate   : num  7440 12280 11250 12960 7560 ...
##  $ Room.Board : num  3300 6450 3750 5450 4120 ...
##  $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...
##  $ Personal   : num  2200 1500 1165 875 1500 ...
##  $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...
##  $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...
##  $ S.F.Ratio  : num  18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...
##  $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...
##  $ Expend     : num  7041 10527 8735 19016 10922 ...
##  $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...</code></pre>
<p>The variables are numeric, except for the factor: Private. Let’s build a model matrix that automatically creates dummy variables, which is important because glmnet only accepts numeric variables.</p>
<pre class="r"><code>X = model.matrix(Apps ~ . - 1, data = College)[, -1]
Y = model.matrix(~ Apps + 0, data = College)

data_df &lt;- as.data.frame(cbind(X, Y))</code></pre>
<p>Since we will be testing the performance of multiple models, we need to split the data into a test and training set.</p>
<pre class="r"><code>train &lt;- sample(1:nrow(X), replace = FALSE, round(nrow(X) * .25))
X.train &lt;- X[train, ]
Y.train &lt;- Y[train]

X.test &lt;- X[-train, ]
Y.test &lt;- Y[-train]</code></pre>
<p>Let’s fit a least-squares regression, which can be done using glmnet and no further arguments. The 17 coefficients of the least-squares regression model are fit by minimizing the RSS, the sum of squared residuals.</p>
<pre class="r"><code>OLS.mod &lt;- glmnet(X.train, Y.train)

OLS.pred &lt;- predict(OLS.mod, newx = X.test)

RMSE &lt;- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

Percent.Error &lt;- function(x){RMSE(x)/mean(Y) * 100}

OLS.RMSE &lt;- RMSE(OLS.pred)
OLS.Percent.Err &lt;- Percent.Error(OLS.pred)

results &lt;- tibble(model = &quot;OLS&quot;,
                  RMSE = OLS.RMSE,
                  )

knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">7.852</td>
</tr>
</tbody>
</table>
<p>The RMSE can be interpreted as an estimate for the standard deviation of predicted values. In the context of our College data, given a new set of observations, we could estimate the amount of applications a college will receive, give or take 7.6 apps. The percent error, given by the RMSE over the average value of Y, gives an estimate of percent average deviation of our prediction. This helps interpret the error, given the average college receives 3002 and our percent error is 0.2615748%, we can infer our error is overall relatively small.</p>
</div>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">1.2</span> Ridge Regression</h2>
<p>Ridge regression fits a linear model by minimizing the quanitity <span class="math inline">\(RSS + \lambda \sum_{j=1}^p\beta_j^2\)</span>, where <span class="math inline">\(\lambda \sum_{j=1}^p\beta_j^2\)</span> also written <span class="math inline">\(\lambda||\beta||^2\)</span> is called a <em>shrinkage penalty</em> and <span class="math inline">\(\lambda \geq 0\)</span> is called a <em>tuning parameter</em>. At the extremes, <span class="math inline">\(\lambda = 0\)</span> returns the least-squares estimate, and <span class="math inline">\(\lambda \rightarrow \infty\)</span> returns the <em>null model</em>, where all the predictors are forced to <span class="math inline">\(0\)</span>. Note, that shrinkage is not applied to the intercept. Instead we first center the inputs and then estmiate <span class="math inline">\(\beta_0\)</span> by <span class="math inline">\(\overline{y}\)</span>. Centering is also necessary because unlike in OLS, the scale of the predictors can influence the estimate. Centering predictors is done by dividing by their standard deviation: <span class="math inline">\(\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^N(x_{ij}-\overline{x_j})^2}}\)</span>. The <span class="math inline">\(\tilde{x_{ij}}\)</span> are called <em>standardized predictors</em> and are used to compute <em>standardized coefficients</em>.</p>
<p>Unlike the least squares estimate which produces only one set of coefficients, ridge regression computes a <em>path</em> of coefficients <span class="math inline">\(\beta^R_{\lambda}\)</span> which is a <span class="math inline">\(p-\)</span>dimensional sequence indexed by <span class="math inline">\(\lambda\)</span>. Cross validation must be used the to select the <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(\beta^R_{\lambda}\)</span> minimizes the out-of-sample error.</p>
<p>Let’s fit a ridge regression. Also, we can manually specify a grid of values for <span class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>grid &lt;- 10^ seq (10,-2, length =100)
ridge.mod &lt;- glmnet(X.test, Y.test, alpha = 0, lambda = grid)  

# alpha = 0 specifies that we are using a Ridge Regression.
# glmnet automatically does centering first; Standardize = TRUE by default</code></pre>
<p>The glmnet object has several useful accessor methods that return information about the model. <code>coef</code> returns a matrix of coefficients. There are 19 rows, one for each coefficient, and 100 columns corresponding to different lambda values.</p>
<pre class="r"><code>dim(coef(ridge.mod))</code></pre>
<pre><code>## [1]  18 100</code></pre>
<p>The <code>lambda</code> attribute returns the lambda values we generated. Let’s looks at the coefficients calculated for the 10th, 50th, and 90th lambda.</p>
<pre class="r"><code>el2norm &lt;- function(x){sqrt(sum(x^2))}

el2norm.r &lt;- function(col){el2norm(coef(ridge.mod)[, col])}

coef(ridge.mod)[,c(10,50,90)] %&gt;% rbind( map_dbl(c(10,50,90), el2norm.r) ) %&gt;% rbind(ridge.mod$lambda[c(10,50,90)])</code></pre>
<pre><code>## 20 x 3 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s9           s49           s89
## (Intercept)  3.100552e+03 -1.642019e+03 -345.77043617
## PrivateYes  -1.907322e-02 -4.835283e+02 -504.99526940
## Accept       7.431517e-06  2.577556e-01    1.63373924
## Enroll       1.783060e-05  5.324137e-01   -1.43072121
## Top10perc    3.877902e-04  1.019843e+01   60.61954496
## Top25perc    3.475631e-04  7.940532e+00  -22.31523237
## F.Undergrad  3.315524e-06  9.564331e-02    0.14580765
## P.Undergrad  5.875664e-06  1.283981e-01    0.01142611
## Outstate     2.922683e-07  8.303856e-03   -0.10231102
## Room.Board   3.144912e-06  1.043023e-01    0.19815883
## Books        1.463976e-05  3.393483e-01   -0.07713976
## Personal     4.479713e-06  7.827989e-02    0.01818403
## PhD          4.806404e-04  8.419201e+00  -12.15935258
## Terminal     5.231937e-04  8.951213e+00   -1.68406036
## S.F.Ratio    4.930160e-04  1.071608e+01   14.84022619
## perc.alumni -1.259608e-04 -4.697841e+00    0.59594153
## Expend       1.031089e-06  3.171805e-02    0.08660075
## Grad.Rate    1.783698e-04  6.329675e+00   11.43949645
##              3.100552e+03  1.711876e+03  615.83849498
##              8.111308e+08  1.149757e+04    0.16297508</code></pre>
<p>The last two rows are the <em>l-2-norm</em> and the <span class="math inline">\(\lambda\)</span> value. We can see that for <span class="math inline">\(\lambda = .16\)</span> we have the highest shrinkage of the coefficients</p>
<p>We can also use the <code>predict</code> method to estimate coefficients at a new value of <span class="math inline">\(\lambda\)</span>, say <span class="math inline">\(\lambda = 80\)</span></p>
<pre class="r"><code>predict(ridge.mod, s=80, type=&quot;coefficients&quot;)</code></pre>
<pre><code>## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                         1
## (Intercept) -808.97174060
## PrivateYes  -555.34941640
## Accept         1.40738905
## Enroll        -0.48903318
## Top10perc     47.07621130
## Top25perc    -12.62976976
## F.Undergrad    0.06724295
## P.Undergrad    0.02816964
## Outstate      -0.07695085
## Room.Board     0.22405145
## Books         -0.01746563
## Personal       0.01383666
## PhD          -10.49745537
## Terminal      -3.21081249
## S.F.Ratio     14.57428063
## perc.alumni   -3.38249800
## Expend         0.08772614
## Grad.Rate     11.66425194</code></pre>
<p>Let’s also look a the <code>%Dev</code> column when we print our model directly.</p>
<pre class="r"><code># print(ridge.mod)   output is too long</code></pre>
<p><code>%Dev</code> is the <em>Null Deviance</em> of the model. If the <em>Null Deviance</em> is very small, it signifies that the proposed model does not perform better than the null model. We can see that when lambda is very high, the <code>%Dev</code> is almost 0, but increases as lambda decreases, and the penalty on the coefficients is relaxed.</p>
<p>Rather than supplying a pre-defined grid, we can use cross-validation to select the best lambda. The function <code>cv.glmnet</code> not only fits 100 models, but it tells us which one is the best.</p>
<pre class="r"><code>set.seed(10)

ridge.cv &lt;- cv.glmnet(X.train, Y.train, alpha = 0)

ridge.pred &lt;- predict(ridge.cv, s=&quot;lambda.min&quot;, newx = X.test) 


ridge.RMSE &lt;- RMSE(ridge.pred)


bestlam &lt;- ridge.cv$lambda.min</code></pre>
<p>Here <code>cv.glmnet</code> uses 10-fold cross validation (by default) and returns a <code>cv.glmnet</code> object containing a sequence of models. The model that miminimized the MSE can be found using <code>lambda.min</code>. Here the best lambda is 347.364 and its corresponding estimate of the test error is 59.102. We could access the coefficients of this model by calling <code>coef(ridge.cv, s = &quot;lambda.min&quot;)</code> and calculate their <em>l-2-norm</em> 1445.03.</p>
</div>
<div id="the-lasso" class="section level2">
<h2><span class="header-section-number">1.3</span> The Lasso</h2>
<p>The Lasso is similar to ridge regression, except that the penalty is applied to the <em>l-1-norm</em> norm of the coeffiecients: <span class="math inline">\(||\beta||_1\)</span>. That is, the lasso selects the best model by minimizing the quantity <span class="math inline">\(RSS + \lambda\sum_{j=1}^p|\beta_j|\)</span>. Unlike Ridge regression, however, the lasso shrinks some of the coefficients to zero, and thus performs <em>variable selection</em>. The advantage of the lasso is that it yields <em>sparse</em> models with less coefficients are easier to interperet.</p>
<pre class="r"><code>set.seed(1)

lasso.cv &lt;- cv.glmnet(X.train, Y.train, alpha = 1)

lasso.lam &lt;- lasso.cv$lambda.min

coef(lasso.cv, s = &quot;lambda.min&quot;)</code></pre>
<pre><code>## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                         1
## (Intercept) -404.32771419
## PrivateYes  -202.77471484
## Accept         1.37604966
## Enroll         0.02134891
## Top10perc     17.65476802
## Top25perc      .         
## F.Undergrad    .         
## P.Undergrad    0.03286638
## Outstate      -0.03642873
## Room.Board     .         
## Books          .         
## Personal       .         
## PhD            .         
## Terminal      -4.64803483
## S.F.Ratio      5.60167335
## perc.alumni   -1.55170529
## Expend         0.07676709
## Grad.Rate      2.11049080</code></pre>
<pre class="r"><code>lasso.mod &lt;- glmnet(X.train, Y.train, alpha = 1, lambda = grid)

lasso.pred &lt;- predict(lasso.mod, s=lasso.lam, newx = X.test)</code></pre>
<p>We can see that some of the variables have been dropped.</p>
<pre class="r"><code>lasso.RMSE &lt;- RMSE(lasso.pred)

results &lt;- rbind(results, list(&quot;Ridge&quot;, ridge.RMSE), list(&quot;Lasso&quot;, lasso.RMSE))

knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">7.852</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">59.102</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">50.975</td>
</tr>
</tbody>
</table>
<p>For some reason, the ridge and lasso perform substantially worse than the ordinary least squares model.</p>
</div>
<div id="principle-components-regression" class="section level2">
<h2><span class="header-section-number">1.4</span> Principle Components Regression</h2>
<pre class="r"><code>library(pls)
set.seed(2)
PCR.mod &lt;- pcr(Apps ~ ., subset = train, data = data_df,  validation = &quot;CV&quot;)

validationplot(PCR.mod, val.type = &quot;MSEP&quot;)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The validation plot compares the performance of PCR based off the number of components that are chosen. The MSEP axis is actually the RSME used to measure the training error. The error stabilizes around 5 or 6 components, and overall decreases monotonically as we include more components. Using all components would defeat the purpose of PCR and actually return the OLS solutions, where each variable is its own component.</p>
<pre class="r"><code>summary(PCR.mod)</code></pre>
<pre><code>## Data:    X dimension: 194 17 
##  Y dimension: 194 1
## Fit method: svdpc
## Number of components considered: 17
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV            3284     3255     1323     1280     1272     1051    689.9
## adjCV         3284     3248     1320     1278     1268     1044    684.8
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       692.4    704.4    709.1     701.8     666.8     669.5     671.3
## adjCV    687.5    699.0    703.4     696.1     661.7     664.2     665.8
##        14 comps  15 comps  16 comps  17 comps
## CV        671.1     680.3     686.7     688.4
## adjCV     665.6     674.0     680.0     681.5
## 
## TRAINING: % variance explained
##       1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X      52.609    87.55    94.87    97.96    98.89    99.58    99.93
## Apps    4.232    84.54    85.73    86.38    92.27    96.49    96.49
##       8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X       99.98   100.00    100.00    100.00    100.00    100.00    100.00
## Apps    96.49    96.49     96.56     96.86     96.86     96.88     96.91
##       15 comps  16 comps  17 comps
## X       100.00    100.00    100.00
## Apps     96.96     96.98     97.01</code></pre>
<p>The summary which reveals the training error and variance explained per number of components included, further shows that including more than 6 components gives no benefit. However it may be best to use even less components to create a better model. Three components should work, since it explains 93% of the variance.</p>
<pre class="r"><code>PCR.pred &lt;- predict(PCR.mod, X.test, ncomp = 3)
PCR.RMSE &lt;- RMSE(PCR.pred)
results &lt;- rbind(results,  list(&quot;PCR&quot;, PCR.RMSE))

knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">7.852</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">59.102</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">50.975</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">83.059</td>
</tr>
</tbody>
</table>
</div>
<div id="partial-least-squares" class="section level2">
<h2><span class="header-section-number">1.5</span> Partial Least Squares</h2>
<p>Partial least sqaures is a supervised alternative to principle components regression. PLS attempts to choose components that explain variance in both the predictors and response.</p>
<pre class="r"><code>PLS.mod &lt;- plsr(Apps ~ ., data = data_df, subset = train, validation = &quot;CV&quot;)
summary(PLS.mod)</code></pre>
<pre><code>## Data:    X dimension: 194 17 
##  Y dimension: 194 1
## Fit method: kernelpls
## Number of components considered: 17
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV            3284     1349     1251     1071    771.1    688.2    685.0
## adjCV         3284     1336     1251     1070    765.3    684.2    680.6
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       689.3    699.9    694.8     661.3     661.6     667.9     664.8
## adjCV    684.8    694.5    674.2     656.8     656.7     662.2     659.5
##        14 comps  15 comps  16 comps  17 comps
## CV        664.7     666.4     666.2     668.9
## adjCV     659.3     660.8     660.6     663.0
## 
## TRAINING: % variance explained
##       1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X       36.80    86.61    93.35    96.18    98.79    99.58    99.86
## Apps    84.58    86.22    90.46    95.49    96.41    96.49    96.50
##       8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X       99.98    99.98    100.00     100.0    100.00    100.00    100.00
## Apps    96.50    96.84     96.86      96.9     96.96     96.97     96.97
##       15 comps  16 comps  17 comps
## X       100.00    100.00    100.00
## Apps     96.98     96.98     97.01</code></pre>
<pre class="r"><code>validationplot(PLS.mod)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We can see that 3 components explains about 93% of the variance and gives a low training RMSE. It’s better to keep the amount of components lowe to decrease the variance of the model, even if more components give slightly lower training RMSE.</p>
<pre class="r"><code>PLS.pred &lt;- predict(PLS.mod, X.test, ncomp = 3)
PLS.RMSE &lt;- RMSE(PLS.pred)
results &lt;- rbind(results, list(&quot;PLS&quot;, PLS.RMSE))
knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">7.852</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">59.102</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">50.975</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">83.059</td>
</tr>
<tr class="odd">
<td align="left">PLS</td>
<td align="right">69.416</td>
</tr>
</tbody>
</table>
<p>Since PLS supervises the choice of components, it can be more effective than PCR for a regression problem when using less components. Notice how the RMSE at 1 component is alsmost 3 times that for PCR than PLS, but it becomes about equal for the inclusion of more components.</p>
</div>
</div>
<div id="simplifying-with-caret" class="section level1">
<h1><span class="header-section-number">2</span> Simplifying with Caret</h1>
<p>In this section, I write a function using caret to train all the models at once. Each model’s parameters is optimized by cross validation automatically, as specified in the trainControl function.</p>
<pre class="r"><code>library(caret)

caret_RMSE &lt;- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

trainControl &lt;- trainControl(method=&quot;cv&quot;, number=5)


  my_caret &lt;- function(method_name){ 
    #this function accepts the name of the method and returns its RMSE from testing it on our specified College dataset
  
  method_fit &lt;- train(Apps~., data=data_df, method=method_name, metric=&quot;RMSE&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;), trControl=trainControl)
  
  method_predictions &lt;- predict(method_fit, X.test)
  
  method_RMSE &lt;- caret_RMSE(method_predictions)
  
  list(method_name, method_RMSE)
  }
  
caret_names &lt;- list(&quot;lm&quot;, &quot;lasso&quot;, &quot;ridge&quot;, &quot;glmnet&quot;, &quot;pcr&quot;, &quot;pls&quot;, &quot;lars&quot;)</code></pre>
<p>Now I write a short pipeline to run the <code>my_caret</code> function on the list of method names and display the results in a nice table.</p>
<pre class="r"><code>map(caret_names, my_caret) %&gt;%
  transpose() %&gt;% 
  map(unlist) %&gt;% 
  set_names(c(&quot;models&quot;,&quot;RMSE&quot;)) %&gt;% 
  as_tibble() %&gt;%
  arrange(RMSE)-&gt; 
  caret_table

knitr::kable(caret_table, digits = 3, booktabs = TRUE, caption = &quot;Result produced by Caret. Models arranged by RMSE.&quot;)</code></pre>
<table>
<caption>Result produced by Caret. Models arranged by RMSE.</caption>
<thead>
<tr class="header">
<th align="left">models</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ridge</td>
<td align="right">46.492</td>
</tr>
<tr class="even">
<td align="left">lars</td>
<td align="right">46.492</td>
</tr>
<tr class="odd">
<td align="left">lm</td>
<td align="right">46.492</td>
</tr>
<tr class="even">
<td align="left">lasso</td>
<td align="right">46.852</td>
</tr>
<tr class="odd">
<td align="left">glmnet</td>
<td align="right">46.977</td>
</tr>
<tr class="even">
<td align="left">pls</td>
<td align="right">61.225</td>
</tr>
<tr class="odd">
<td align="left">pcr</td>
<td align="right">88.725</td>
</tr>
</tbody>
</table>
<p>The results are quite similar to running each model individually. The ridge and lasso are slightly better, probably because the caret algorithm picked a better lambda. However there is a noticable discrepancy between the original least squares call, which is noticably better than all other methods, and this least-squares call, which is comparable to other regression methods.</p>
</div>
<div id="subset-selection" class="section level1">
<h1><span class="header-section-number">3</span> Subset selection</h1>
<div id="best-subset-selection" class="section level2">
<h2><span class="header-section-number">3.1</span> Best Subset Selection</h2>
<p>Subset selection is a technique for selecting a subset of your original varaible to be predictors in your model. For example, let’s say we want to fit a least squares model to the training data, but we suspect that it will generalize better to testing data if we keep fewer variables. Perhaps some are reduntant or lack predictive power. In the <em>Best Subset Selection</em> procedure, we consider all possible models. Since we are working with the College data which has 17 predictor variables, this will require fitting <span class="math display">\[\binom{17}{0} + \binom{17}{1} + \binom{17}{2} + \dots + \binom{17}{17} = \sum_{k = 0}^{17}\binom{17}{k} = 2^{17}\]</span> models. A quick aside regarding the combinatoric identity: <span class="math inline">\(\sum_{k = 0}^{p}\binom{p}{k} = 2^{p}\)</span>. It arises from th question, what is the cardinality of the power set of a set containing <span class="math inline">\(n\)</span> objects. That is, how many distinct subsets can be created from this set. We can apprach this two ways: iterate over each element and decide whether or not to include it in your subset: this gives <span class="math inline">\(2^p\)</span> choices. Or compute individually the amount of subsets of size <span class="math inline">\(k\)</span> from <span class="math inline">\(0\leq k \leq n\)</span> by taking combinations, and then summing the counts. These two approaches are identical, hence the formula. Let’s apply:</p>
<pre class="r"><code>library(leaps)
BEST.mod_9 = regsubsets(Apps ~ ., data_df)
summary(BEST.mod_9)</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Apps ~ ., data_df)
## 17 Variables  (and intercept)
##             Forced in Forced out
## PrivateYes      FALSE      FALSE
## Accept          FALSE      FALSE
## Enroll          FALSE      FALSE
## Top10perc       FALSE      FALSE
## Top25perc       FALSE      FALSE
## F.Undergrad     FALSE      FALSE
## P.Undergrad     FALSE      FALSE
## Outstate        FALSE      FALSE
## Room.Board      FALSE      FALSE
## Books           FALSE      FALSE
## Personal        FALSE      FALSE
## PhD             FALSE      FALSE
## Terminal        FALSE      FALSE
## S.F.Ratio       FALSE      FALSE
## perc.alumni     FALSE      FALSE
## Expend          FALSE      FALSE
## Grad.Rate       FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: exhaustive
##          PrivateYes Accept Enroll Top10perc Top25perc F.Undergrad
## 1  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot; &quot;       &quot; &quot;       &quot; &quot;        
## 2  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 3  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 4  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot; &quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 5  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 6  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
## 7  ( 1 ) &quot; &quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot;*&quot;       &quot; &quot;        
## 8  ( 1 ) &quot;*&quot;        &quot;*&quot;    &quot;*&quot;    &quot;*&quot;       &quot; &quot;       &quot; &quot;        
##          P.Undergrad Outstate Room.Board Books Personal PhD Terminal
## 1  ( 1 ) &quot; &quot;         &quot; &quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 2  ( 1 ) &quot; &quot;         &quot; &quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 3  ( 1 ) &quot; &quot;         &quot; &quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 4  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 5  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot; &quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 6  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot;*&quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 7  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot;*&quot;        &quot; &quot;   &quot; &quot;      &quot; &quot; &quot; &quot;     
## 8  ( 1 ) &quot; &quot;         &quot;*&quot;      &quot;*&quot;        &quot; &quot;   &quot; &quot;      &quot;*&quot; &quot; &quot;     
##          S.F.Ratio perc.alumni Expend Grad.Rate
## 1  ( 1 ) &quot; &quot;       &quot; &quot;         &quot; &quot;    &quot; &quot;      
## 2  ( 1 ) &quot; &quot;       &quot; &quot;         &quot; &quot;    &quot; &quot;      
## 3  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 4  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 5  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 6  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 7  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;      
## 8  ( 1 ) &quot; &quot;       &quot; &quot;         &quot;*&quot;    &quot; &quot;</code></pre>
<p>Th asterix indicated whether the variable is included in the model. The row indices indicate a model of that many variables, so each row is a separate model, each one containing one more variable than in the previous row. The algorithm calculates only up to 9-variable models by default. Lets try setting <code>nvmax = 17</code> to include all models, and then extract some statistics.</p>
<pre class="r"><code>BEST.mod_17 &lt;- regsubsets(Apps ~ ., data_df, nvmax = 17)
bmod.summary &lt;- summary(BEST.mod_17)</code></pre>
<pre class="r"><code>show_metrics &lt;- function(my_summary){

metrics &lt;- c(&quot;adjr2&quot;, &quot;rsq&quot;, &quot;bic&quot;, &quot;cp&quot;)

best_df &lt;- as.data.frame(`[`(my_summary, metrics)) 

best_df_melt &lt;- best_df %&gt;% gather(key = &quot;metric&quot;, value = &quot;value&quot;) %&gt;% mutate(model = rep(1:17, 4))



(ggplot(data = best_df_melt, aes(x = model, y = value, color = metric)) +
         geom_line() +
        facet_grid(metric ~ ., scales = &quot;free_y&quot;)) %&gt;% print()

c(map_dbl(best_df[c(&quot;adjr2&quot;, &quot;rsq&quot;)], which.max), map_dbl(best_df[c(&quot;bic&quot;, &quot;cp&quot;)], which.min))

}</code></pre>
<pre class="r"><code>show_metrics(bmod.summary)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre><code>## adjr2   rsq   bic    cp 
##    13    17    10    12</code></pre>
<p>The graphs show how the error metric changes as we introduce more variables into the model. They seem to drop off rapidly and then hit a max or min, as displayed in the table.</p>
</div>
<div id="estimates-of-test-error" class="section level2">
<h2><span class="header-section-number">3.2</span> Estimates of Test error</h2>
<p>These statistics are intended to estimate the test error of the model by making an adjustment to the training error to account for the bias due to overfitting in the training process.</p>
<p>Let’s interperet each of these statistics (exact formulas can be looked up):</p>
<ul>
<li><span class="math inline">\(R^2_{adj}\)</span> modifies the denominator of <span class="math inline">\(RSS\)</span> in <span class="math inline">\(R^2\)</span> to RSS/(N - p - 1), where p is the number of predictor variables in our model. Unlike <span class="math inline">\(R^2\)</span> which will increase monotincally with additionaly variables,<span class="math inline">\(R^2_{adj}\)</span> penalizes the addition of noise variables. Both these statistics measure the goodness-of-fit of the model on a scale of <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, so we want to choose the model that maximizes them.</li>
</ul>
<p>-<span class="math inline">\(C_p\)</span> adds a penalty of <span class="math inline">\(p *\hat{\sigma}\)</span> to the RSS, where sigma estimates the variance in the response. Clearly <span class="math inline">\(C_p\)</span> <span class="math inline">\(p\)</span> increases with more predictors, so we want to minimize it.</p>
<p>-<span class="math inline">\(AIC\)</span> is proportional to <span class="math inline">\(C_p\)</span> so it should be minimized</p>
<p>-<span class="math inline">\(BIC\)</span> adds a term of <span class="math inline">\(log(N) * p * \hat{\sigma}^2\)</span> to the RSS where <span class="math inline">\(N\)</span> is the number of obserations, so it penalizes models with many variables and observations. Thus, we want to mimize it. The more severe penalty given by the <span class="math inline">\(BIC\)</span> also explains why it chose the sparsest model.</p>
</div>
<div id="forward-selection" class="section level2">
<h2><span class="header-section-number">3.3</span> Forward Selection</h2>
<p>Clearly Best Subset selection is very computationally intensive, so shorter alternatives have been developed. <em>Forward Selection</em> works by adding in the best variable at each stage. That is, it chooses to include the variable that minimizes the model’s RSS, and then keeps that variable while choosing a new one from the remaining variables, until all are used. This results in having to test <span class="math inline">\(1 + \sum_{k=0}^{p-1}p - k = 1 + \frac{p(p+1)}{2}\)</span> models. In our College dataset, <span class="math inline">\(p = 17\)</span>, so we fit <span class="math display">\[1 + 17 + 16 + \dots + 2 + 1 = 1 + \frac{(17)(18)}{2} = 154\]</span>. This is substantially less computation. Note that the additional one is from fitting the null model. At each stage the best variable is chosen by comparing <span class="math inline">\(R^2\)</span> amongst the different possible models. At the end, we get <span class="math inline">\(p+1\)</span> models: <span class="math inline">\(M_0, \dots, M_17\)</span>. Since these models do not have the number of variables, we cannot compare them directly using <span class="math inline">\(R^2\)</span> (it will increase monotionically). Instead we can use on the alternative criteria listed earlier.</p>
<pre class="r"><code>FWD.mod &lt;- regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = &quot;forward&quot;)

sum.FWD &lt;- summary(FWD.mod)


show_metrics(sum.FWD)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## adjr2   rsq   bic    cp 
##    13    17    10    12</code></pre>
<p>The metrics are the same for both methods.</p>
<p>Let’s compare the speed of these algorithms.</p>
<pre class="r"><code>BEST_time</code></pre>
<pre><code>## Unit: milliseconds
##                                              expr      min       lq
##  regsubsets(Apps ~ ., data = data_df, nvmax = 17) 3.093272 3.288136
##      mean   median       uq      max neval
##  4.127863 3.598927 4.352843 14.51823   100</code></pre>
<pre class="r"><code>FWD_time</code></pre>
<pre><code>## Unit: milliseconds
##                                                                  expr
##  regsubsets(Apps ~ ., data = data_df, nvmax = 17, method = &quot;forward&quot;)
##       min      lq    mean   median       uq      max neval
##  2.744229 2.87404 3.66638 3.182119 3.778638 14.34198   100</code></pre>
<p>We can see that forward selection is on average a fe miliseconds faster than the full model selection.</p>
<p>Let’s compare the models they selected as determined by the BIC. Since the BIC was minimized by the model with 10 variables, we can see if they are the same variables for the Best subset and Forward selection.</p>
<pre class="r"><code>sum.FWD$which[10, ] == bmod.summary$which[10, ]</code></pre>
<pre><code>## (Intercept)  PrivateYes      Accept      Enroll   Top10perc   Top25perc 
##        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 
## F.Undergrad P.Undergrad    Outstate  Room.Board       Books    Personal 
##        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 
##         PhD    Terminal   S.F.Ratio perc.alumni      Expend   Grad.Rate 
##        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE</code></pre>
<p>It looks like both methods give the same results. Therefore we can opt for the faster, forward selection method. However, this was a small data set. Results may vary for large high-dimesnional data.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
