<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Harrison Tietze" />


<title>ML</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="faded.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Harrison's website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About Me</a>
</li>
<li>
  <a href="simulations.html">Sims</a>
</li>
<li>
  <a href="ML.html">ML</a>
</li>
<li>
  <a href="GLM.html">GLM</a>
</li>
<li>
  <a href="certificates.html">certificates</a>
</li>
<li>
  <a href="topology.html">Baire Space</a>
</li>
<li>
  <a href="KDE.html">KDE</a>
</li>
<li>
  <a href="https://docs.google.com/document/d/e/2PACX-1vTnpiuuxd0-ia3SMAk1SGusAyLyvwPN-wsl_dQCIC6Nglj0WgyTne0nYS1JISZZI6H-ym631b9B0kr0/pub">Novella</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">ML</h1>
<h4 class="author"><em>Harrison Tietze</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#regression-models"><span class="toc-section-number">1</span> Regression Models</a><ul>
<li><a href="#glmnet"><span class="toc-section-number">1.1</span> glmnet</a></li>
<li><a href="#ridge-regression"><span class="toc-section-number">1.2</span> Ridge Regression</a></li>
<li><a href="#the-lasso"><span class="toc-section-number">1.3</span> The Lasso</a></li>
<li><a href="#principle-components-regression"><span class="toc-section-number">1.4</span> Principle Components Regression</a></li>
<li><a href="#partial-least-squares"><span class="toc-section-number">1.5</span> Partial Least Squares</a></li>
</ul></li>
<li><a href="#simplifying-with-caret"><span class="toc-section-number">2</span> Simplifying with Caret</a></li>
</ul>
</div>

<div id="regression-models" class="section level1">
<h1><span class="header-section-number">1</span> Regression Models</h1>
<div id="glmnet" class="section level2">
<h2><span class="header-section-number">1.1</span> glmnet</h2>
<p>In this analysis we will work with the “College” dataset, which contains 777 observations with 18 variables. The goals is to predict how many applications a college will recieve “Apps” based off other predictors.</p>
<p>First examine the structure of the dataset:</p>
<pre class="r"><code>str(College)</code></pre>
<pre><code>## &#39;data.frame&#39;:    777 obs. of  18 variables:
##  $ Private    : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Apps       : num  1660 2186 1428 417 193 ...
##  $ Accept     : num  1232 1924 1097 349 146 ...
##  $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...
##  $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...
##  $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...
##  $ F.Undergrad: num  2885 2683 1036 510 249 ...
##  $ P.Undergrad: num  537 1227 99 63 869 ...
##  $ Outstate   : num  7440 12280 11250 12960 7560 ...
##  $ Room.Board : num  3300 6450 3750 5450 4120 ...
##  $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...
##  $ Personal   : num  2200 1500 1165 875 1500 ...
##  $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...
##  $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...
##  $ S.F.Ratio  : num  18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...
##  $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...
##  $ Expend     : num  7041 10527 8735 19016 10922 ...
##  $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...</code></pre>
<p>The variables are numeric, except for the factor: Private. Let’s build a model matrix that automatically creates dummy variables, which is important because glmnet only accepts numeric variables.</p>
<pre class="r"><code>X = model.matrix(Apps ~ ., data = College)
Y = College$Apps

data_df &lt;- cbind(as.data.frame(X), College[, &quot;Apps&quot;]) %&gt;% rename(Apps = `College[, &quot;Apps&quot;]`)</code></pre>
<p>Since we will be testing the performance of multiple models, we need to split the data into a test and training set.</p>
<pre class="r"><code>train &lt;- sample(1:nrow(X), replace = FALSE, round(nrow(X) * .25))
X.train &lt;- X[train, ]
Y.train &lt;- Y[train]

X.test &lt;- X[-train, ]
Y.test &lt;- Y[-train]</code></pre>
<p>Let’s fit a least-squares regression, which can be done using glmnet and no further arguments. The 17 coefficients of the least-squares regression model are fit by minimizing the RSS, the sum of squared residuals.</p>
<pre class="r"><code>OLS.mod &lt;- glmnet(X.train, Y.train)

OLS.pred &lt;- predict(OLS.mod, newx = X.test)

RMSE &lt;- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

Percent.Error &lt;- function(x){RMSE(x)/mean(Y) * 100}

OLS.RMSE &lt;- RMSE(OLS.pred)
OLS.Percent.Err &lt;- Percent.Error(OLS.pred)

results &lt;- tibble(model = &quot;OLS&quot;,
                  RMSE = OLS.RMSE,
                  )

knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">6.595</td>
</tr>
</tbody>
</table>
<p>The RMSE can be interpreted as an estimate for the standard deviation of predicted values. In the context of our College data, given a new set of observations, we could estimate the amount of applications a college will receive, give or take 7.6 apps. The percent error, given by the RMSE over the average value of Y, gives an estimate of percent average deviation of our prediction. This helps interpret the error, given the average college receives 3002 and our percent error is 0.2197004%, we can infer our error is overall relatively small.</p>
</div>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">1.2</span> Ridge Regression</h2>
<p>Ridge regression fits a linear model by minimizing the quanitity <span class="math inline">\(RSS + \lambda \sum_{j=1}^p\beta_j^2\)</span>, where <span class="math inline">\(\lambda \sum_{j=1}^p\beta_j^2\)</span> also written <span class="math inline">\(\lambda||\beta||^2\)</span> is called a <em>shrinkage penalty</em> and <span class="math inline">\(\lambda \geq 0\)</span> is called a <em>tuning parameter</em>. At the extremes, <span class="math inline">\(\lambda = 0\)</span> returns the least-squares estimate, and <span class="math inline">\(\lambda \rightarrow \infty\)</span> returns the <em>null model</em>, where all the predictors are forced to <span class="math inline">\(0\)</span>. Note, that shrinkage is not applied to the intercept. Instead we first center the inputs and then estmiate <span class="math inline">\(\beta_0\)</span> by <span class="math inline">\(\overline{y}\)</span>. Centering is also necessary because unlike in OLS, the scale of the predictors can influence the estimate. Centering predictors is done by dividing by their standard deviation: <span class="math inline">\(\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^N(x_{ij}-\overline{x_j})^2}}\)</span>. The <span class="math inline">\(\tilde{x_{ij}}\)</span> are called <em>standardized predictors</em> and are used to compute <em>standardized coefficients</em>.</p>
<p>Unlike the least squares estimate which produces only one set of coefficients, ridge regression computes a <em>path</em> of coefficients <span class="math inline">\(\beta^R_{\lambda}\)</span> which is a <span class="math inline">\(p-\)</span>dimensional sequence indexed by <span class="math inline">\(\lambda\)</span>. Cross validation must be used the to select the <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(\beta^R_{\lambda}\)</span> minimizes the out-of-sample error.</p>
<p>Let’s fit a ridge regression. Also, we can manually specify a grid of values for <span class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>grid &lt;- 10^ seq (10,-2, length =100)
ridge.mod &lt;- glmnet(X.test, Y.test, alpha = 0, lambda = grid)  

# alpha = 0 specifies that we are using a Ridge Regression.
# glmnet automatically does centering first; Standardize = TRUE by default</code></pre>
<p>The glmnet object has several useful accessor methods that return information about the model. <code>coef</code> returns a matrix of coefficients. There are 19 rows, one for each coefficient, and 100 columns corresponding to different lambda values.</p>
<pre class="r"><code>dim(coef(ridge.mod))</code></pre>
<pre><code>## [1]  19 100</code></pre>
<p>The <code>lambda</code> attribute returns the lambda values we generated. Let’s looks at the coefficients calculated for the 10th, 50th, and 90th lambda.</p>
<pre class="r"><code>el2norm &lt;- function(x){sqrt(sum(x^2))}

el2norm.r &lt;- function(col){el2norm(coef(ridge.mod)[, col])}

coef(ridge.mod)[,c(10,50,90)] %&gt;% rbind( map_dbl(c(10,50,90), el2norm.r) ) %&gt;% rbind(ridge.mod$lambda[c(10,50,90)])</code></pre>
<pre><code>## 21 x 3 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s9           s49           s89
## (Intercept)  3.001460e+03 -1.458263e+03 -450.12509864
## (Intercept)  .             .               .         
## PrivateYes  -1.789027e-02 -4.648071e+02 -494.38698125
## Accept       7.103069e-06  2.489066e-01    1.58314568
## Enroll       1.681834e-05  5.134601e-01   -0.86900691
## Top10perc    3.544599e-04  9.530257e+00   49.79963953
## Top25perc    3.276538e-04  7.841425e+00  -14.15596655
## F.Undergrad  3.098810e-06  9.162011e-02    0.05640780
## P.Undergrad  4.827473e-06  1.103401e-01    0.04442850
## Outstate     2.300851e-07  7.305875e-03   -0.08570968
## Room.Board   2.775406e-06  9.438862e-02    0.15145333
## Books        1.481607e-05  3.409711e-01    0.02115933
## Personal     4.871357e-06  9.181531e-02    0.03090023
## PhD          4.415587e-04  8.323631e+00   -8.66418687
## Terminal     4.631391e-04  8.360791e+00   -3.33962422
## S.F.Ratio    4.458416e-04  1.112056e+01   15.39869024
## perc.alumni -1.343654e-04 -5.211799e+00    0.13310284
## Expend       9.174057e-07  2.860713e-02    0.07795514
## Grad.Rate    1.576579e-04  5.769915e+00    8.68071987
##              3.001460e+03  1.530703e+03  670.90498961
##              8.111308e+08  1.149757e+04    0.16297508</code></pre>
<p>The last two rows are the <em>l-2-norm</em> and the <span class="math inline">\(\lambda\)</span> value. We can see that for <span class="math inline">\(\lambda = .16\)</span> we have the highest shrinkage of the coefficients</p>
<p>We can also use the <code>predict</code> method to estimate coefficients at a new value of <span class="math inline">\(\lambda\)</span>, say <span class="math inline">\(\lambda = 80\)</span></p>
<pre class="r"><code>predict(ridge.mod, s=80, type=&quot;coefficients&quot;)</code></pre>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                         1
## (Intercept) -842.54145642
## (Intercept)    .         
## PrivateYes  -511.92401570
## Accept         1.36629641
## Enroll        -0.22883111
## Top10perc     39.62013832
## Top25perc     -7.76407057
## F.Undergrad    0.03611710
## P.Undergrad    0.03576975
## Outstate      -0.06314725
## Room.Board     0.17405745
## Books          0.05510074
## Personal       0.01487869
## PhD           -7.06756018
## Terminal      -4.38993870
## S.F.Ratio     15.11353972
## perc.alumni   -3.52229120
## Expend         0.07919428
## Grad.Rate      9.62239114</code></pre>
<p>Let’s also look a the <code>%Dev</code> column when we print our model directly.</p>
<pre class="r"><code># print(ridge.mod)   output is too long</code></pre>
<p><code>%Dev</code> is the <em>Null Deviance</em> of the model. If the <em>Null Deviance</em> is very small, it signifies that the proposed model does not perform better than the null model. We can see that when lambda is very high, the <code>%Dev</code> is almost 0, but increases as lambda decreases, and the penalty on the coefficients is relaxed.</p>
<p>Rather than supplying a pre-defined grid, we can use cross-validation to select the best lambda. The function <code>cv.glmnet</code> not only fits 100 models, but it tells us which one is the best.</p>
<pre class="r"><code>set.seed(10)

ridge.cv &lt;- cv.glmnet(X.train, Y.train, alpha = 0)

ridge.pred &lt;- predict(ridge.cv, s=&quot;lambda.min&quot;, newx = X.test) 


ridge.RMSE &lt;- RMSE(ridge.pred)


bestlam &lt;- ridge.cv$lambda.min</code></pre>
<p>Here <code>cv.glmnet</code> uses 10-fold cross validation (by default) and returns a <code>cv.glmnet</code> object containing a sequence of models. The model that miminimized the MSE can be found using <code>lambda.min</code>. Here the best lambda is 485.847 and its corresponding estimate of the test error is 44.633. We could access the coefficients of this model by calling <code>coef(ridge.cv, s = &quot;lambda.min&quot;)</code> and calculate their <em>l-2-norm</em> 1550.27.</p>
</div>
<div id="the-lasso" class="section level2">
<h2><span class="header-section-number">1.3</span> The Lasso</h2>
<p>The Lasso is similar to ridge regression, except that the penalty is applied to the <em>l-1-norm</em> norm of the coeffiecients: <span class="math inline">\(||\beta||_1\)</span>. That is, the lasso selects the best model by minimizing the quantity <span class="math inline">\(RSS + \lambda\sum_{j=1}^p|\beta_j|\)</span>. Unlike Ridge regression, however, the lasso shrinks some of the coefficients to zero, and thus performs <em>variable selection</em>. The advantage of the lasso is that it yields <em>sparse</em> models with less coefficients are easier to interperet.</p>
<pre class="r"><code>set.seed(1)

lasso.cv &lt;- cv.glmnet(X.train, Y.train, alpha = 1)

lasso.lam &lt;- lasso.cv$lambda.min

coef(lasso.cv, s = &quot;lambda.min&quot;)</code></pre>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                         1
## (Intercept)   69.34902528
## (Intercept)    .         
## PrivateYes  -595.64358292
## Accept         1.86593908
## Enroll        -0.34992361
## Top10perc     57.22930932
## Top25perc    -18.46259680
## F.Undergrad   -0.18922923
## P.Undergrad    0.06772948
## Outstate      -0.15971823
## Room.Board     0.05447433
## Books         -0.45883492
## Personal       0.13779185
## PhD          -13.57517691
## Terminal       0.79909250
## S.F.Ratio      5.10482889
## perc.alumni    4.09874698
## Expend         0.14168984
## Grad.Rate     12.80833938</code></pre>
<pre class="r"><code>lasso.mod &lt;- glmnet(X.train, Y.train, alpha = 1, lambda = grid)

lasso.pred &lt;- predict(lasso.mod, s=lasso.lam, newx = X.test)</code></pre>
<p>We can see that some of the variables have been dropped.</p>
<pre class="r"><code>lasso.RMSE &lt;- RMSE(lasso.pred)

results &lt;- rbind(results, list(&quot;Ridge&quot;, ridge.RMSE), list(&quot;Lasso&quot;, lasso.RMSE))

knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">6.595</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.633</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">49.003</td>
</tr>
</tbody>
</table>
<p>For some reason, the ridge and lasso perform substantially worse than the ordinary least squares model.</p>
</div>
<div id="principle-components-regression" class="section level2">
<h2><span class="header-section-number">1.4</span> Principle Components Regression</h2>
<pre class="r"><code>library(pls)
set.seed(2)
PCR.mod &lt;- pcr(Apps ~ ., subset = train, data = data_df,  validation = &quot;CV&quot;)

validationplot(PCR.mod, val.type = &quot;MSEP&quot;)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The validation plot compares the performance of PCR based off the number of components that are chosen. The MSEP axis is actually the RSME used to measure the training error. The error stabilizes around 5 or 6 components, and overall decreases monotonically as we include more components. Using all components would defeat the purpose of PCR and actually return the OLS solutions, where each variable is its own component.</p>
<pre class="r"><code>summary(PCR.mod)</code></pre>
<pre><code>## Data:    X dimension: 194 18 
##  Y dimension: 194 1
## Fit method: svdpc
## Number of components considered: 18
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV            4686     4699     2764     2763     2798     2260     1541
## adjCV         4686     4732     2747     2747     2807     2171     1515
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV        1511     1524     1558      1456      1438      1440      1468
## adjCV     1489     1501     1532      1433      1415      1417      1443
##        14 comps  15 comps  16 comps  17 comps  18 comps
## CV         1475      1433      1440      1396       NaN
## adjCV      1450      1408      1414      1372       NaN
## 
## TRAINING: % variance explained
##       1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X     47.7637    85.35    93.30    97.18    98.83    99.59    99.93
## Apps   0.1278    73.09    73.84    79.71    93.85    93.97    93.99
##       8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X       99.97      100    100.00    100.00     100.0     100.0     100.0
## Apps    93.99       94     94.59     94.78      94.8      94.8      94.8
##       15 comps  16 comps  17 comps  18 comps
## X       100.00    100.00    100.00       100
## Apps     95.23     95.25     95.35       100</code></pre>
<p>The summary which reveals the training error and variance explained per number of components included, further shows that including more than 6 components gives no benefit. However it may be best to use even less components to create a better model. Three components should work, since it explains 93% of the variance.</p>
<pre class="r"><code>PCR.pred &lt;- predict(PCR.mod, X.test, ncomp = 3)
PCR.RMSE &lt;- RMSE(PCR.pred)
results &lt;- rbind(results,  list(&quot;PCR&quot;, PCR.RMSE))

knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">6.595</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.633</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">49.003</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">66.753</td>
</tr>
</tbody>
</table>
</div>
<div id="partial-least-squares" class="section level2">
<h2><span class="header-section-number">1.5</span> Partial Least Squares</h2>
<p>Partial least sqaures is a supervised alternative to principle components regression. PLS attempts to choose components that explain variance in both the predictors and response.</p>
<pre class="r"><code>PLS.mod &lt;- plsr(Apps ~ ., data = data_df, subset = train, validation = &quot;CV&quot;)
summary(PLS.mod)</code></pre>
<pre><code>## Data:    X dimension: 194 18 
##  Y dimension: 194 1
## Fit method: kernelpls
## Number of components considered: 18
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV            4686     2684     2550     2166     1748     1602     1581
## adjCV         4686     2649     2365     2128     1719     1573     1554
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV        1604     1680     1693      1566      1611      1593      1555
## adjCV     1575     1646     1651      1534      1574      1555      1520
##        14 comps  15 comps  16 comps  17 comps  18 comps
## CV         1547      1555      1564      1556      1556
## adjCV      1513      1521      1529      1521      1521
## 
## TRAINING: % variance explained
##       1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X       37.56    47.72    88.66    95.05    98.82    99.56    99.92
## Apps    75.75    87.97    90.26    93.04    93.97    93.99    94.00
##       8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X       99.97    99.97    100.00    100.00    100.00    100.00    100.00
## Apps    94.04    94.63     94.81     94.97     95.23     95.24     95.24
##       15 comps  16 comps  17 comps  18 comps
## X       100.00    100.00    100.00    102.90
## Apps     95.25     95.25     95.35     95.35</code></pre>
<pre class="r"><code>validationplot(PLS.mod)</code></pre>
<p><img src="ML_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We can see that 3 components explains about 93% of the variance and gives a low training RMSE. It’s better to keep the amount of components lowe to decrease the variance of the model, even if more components give slightly lower training RMSE.</p>
<pre class="r"><code>PLS.pred &lt;- predict(PLS.mod, X.test, ncomp = 3)
PLS.RMSE &lt;- RMSE(PLS.pred)
results &lt;- rbind(results, list(&quot;PLS&quot;, PLS.RMSE))
knitr::kable(results, digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">6.595</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">44.633</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">49.003</td>
</tr>
<tr class="even">
<td align="left">PCR</td>
<td align="right">66.753</td>
</tr>
<tr class="odd">
<td align="left">PLS</td>
<td align="right">52.771</td>
</tr>
</tbody>
</table>
<p>Since PLS supervises the choice of components, it can be more effective than PCR for a regression problem when using less components. Notice how the RMSE at 1 component is alsmost 3 times that for PCR than PLS, but it becomes about equal for the inclusion of more components.</p>
</div>
</div>
<div id="simplifying-with-caret" class="section level1">
<h1><span class="header-section-number">2</span> Simplifying with Caret</h1>
<p>In this section, I write a function using caret to train all the models at once. Each model’s parameters is optimized by cross validation automatically, as specified in the trainControl function.</p>
<pre class="r"><code>library(caret)

caret_RMSE &lt;- function(x){sqrt(sum((x-Y.test)^2))/length(x)}

trainControl &lt;- trainControl(method=&quot;cv&quot;, number=5)


  my_caret &lt;- function(method_name){ 
    #this function accepts the name of the method and returns its RMSE from testing it on our specified College dataset
  
  method_fit &lt;- train(Apps~., data=data_df, method=method_name, metric=&quot;RMSE&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;), trControl=trainControl)
  
  method_predictions &lt;- predict(method_fit, X.test)
  
  method_RMSE &lt;- caret_RMSE(method_predictions)
  
  list(method_name, method_RMSE)
  }
  
caret_names &lt;- list(&quot;lm&quot;, &quot;lasso&quot;, &quot;ridge&quot;, &quot;glmnet&quot;, &quot;pcr&quot;, &quot;pls&quot;)</code></pre>
<p>Now I write a short pipeline to run the <code>my_caret</code> function on the list of method names and display the results in a nice table.</p>
<pre class="r"><code>map(caret_names, my_caret) %&gt;%
  transpose() %&gt;% 
  map(unlist) %&gt;% 
  set_names(c(&quot;models&quot;,&quot;RMSE&quot;)) %&gt;% 
  as_tibble() %&gt;%
  arrange(RMSE)-&gt; 
  caret_table

knitr::kable(caret_table, digits = 3, booktabs = TRUE, caption = &quot;Result produced by Caret. Models arranged by RMSE.&quot;)</code></pre>
<table>
<caption>Result produced by Caret. Models arranged by RMSE.</caption>
<thead>
<tr class="header">
<th align="left">models</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">lasso</td>
<td align="right">40.884</td>
</tr>
<tr class="even">
<td align="left">glmnet</td>
<td align="right">40.895</td>
</tr>
<tr class="odd">
<td align="left">lm</td>
<td align="right">40.956</td>
</tr>
<tr class="even">
<td align="left">ridge</td>
<td align="right">40.956</td>
</tr>
<tr class="odd">
<td align="left">pls</td>
<td align="right">47.444</td>
</tr>
<tr class="even">
<td align="left">pcr</td>
<td align="right">70.510</td>
</tr>
</tbody>
</table>
<p>The results are quite similar to running each model individually. The ridge and lasso are slightly better, probably because the caret algorithm picked a better lambda. However there is a noticable discrepancy between the original least squares call, which is noticably better than all other methods, and this least-squares call, which is comparable to other regression methods.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
